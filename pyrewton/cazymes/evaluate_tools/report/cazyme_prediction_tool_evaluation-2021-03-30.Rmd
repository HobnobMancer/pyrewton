---
title: "Evaluation of the CAZyme classifiers: dbCAN, CUPP and eCAMI"
author: "Emma E. M. Hobbs"
date: "2021 March"
output: 
  bookdown::html_document2:
    toc: true
    toc_float:
      toc_collapsed: false
    number_sections: true
    css: "css/rmd_style.css"
    theme: lumen
---

```{r setup, include=FALSE}
#
# Import required libraries
#
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library('kableExtra')
library(magrittr) 
library('ggplot2')
library("DT")
library("datasets")
library("dplyr")
library("GGally")
library("ggridges")
library("readr")
library(knitr)
library(tidyverse)
library("MLmetrics")
library('mclust')  # adjusted rand index
library('Metrics')  # Fbeta score
library("devtools")
library(ggpubr)
library(RColorBrewer)
library(cowplot)
library(scales)
library("MASS")
library("interp")
library('reshape2')

#
# define global constants
#

# BG color for plots - should match .figure and .caption classes in rmd_style.css
figbg = "whitesmoke"

#
# Colour schemes for data
#

# define small colour set, with one colour per CAZyme classifier
colour_set <- c("#0a7a6d", "#d4af37", "#7844b8", "#acbf1b", "#c22176", "#3888e0")
# colour gradient from red-orange-yellow-blue-gree
colour_grad <- c(
  "#1c0a00", "#620021", "#940113",
  "#D73027", "#F38345", "#FDBA67",
  "#FEE168", "#fff966", "#BCF9FC",
  "#98D0E4", "#66A5CC", "#3D7A99",
  "#2e5a6b", "#2e663b", "#228537",
  "#1ba841", "#3fcc3f", "#6ddb1a",
  "#a5f200")
# define large colour set when many colours are needed
n <- 60
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
qual_col = qual_col_pals[c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE), ]
large_colour_set = unlist(mapply(brewer.pal, qual_col$maxcolors, rownames(qual_col)))
```

```{r importData, include=FALSE}
#
# Import data files
#

# import file with test set coverage of genomes and CAZomes
test_set_cov_df = read.table("data/cazome_coverage_2021_04_06-11_11_04.txt", sep="\t",header=TRUE)

# import "binary_classification_evaluation_<date>.csv"
# Written in long form, with the columns: Statistic_parameter, Genomic_assembly, Prediction_tool, Statistic_value
binary_stat_df <- read.csv("data/binary_classification_evaluation_2021_04_08.csv")

# import "binary_bootstrap_accuracy_evaluation_<date>.csv"
# Contains the output from the bootstrap resampling of the binary classification of CAZymes/non-CAZymes
bootstrap_results_df <- read.csv("data/binary_bootstrap_accuracy_evaluation_2021_04_08.csv")

# import binary false positive and negative predictions
binary_fp_predictions = read.csv("data/binary_false_positive_classifications_2021_11_09.csv")
binary_fn_predictions = read.csv("data/binary_false_negative_classifications_2021_11_09.csv")

# import "class_predicted_classifications_<date>.csv"
# Contains the columns: Genomic_accession, Protein_accession, Prediciton_tool, one column per CAZy class, Rand_index and Adjusted_rand_index
# Contains the clasifications (0/1) for each prediction tool for every protein across all test sets
# Includes the calculated rand index and adjusted rand index
class_ri_ari_raw_df <- read.csv("data/class_predicted_classifications_2021_04_08.csv")

# import "class_stats_per_test_set_<date>.csv"
# Contains the calculated performance statistics when evaluating performance of CAZy class prediction per test set.
# Written in long form with the columns: Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
cazy_class_df <- read.csv("data/class_stats_per_test_set_2021_11_09.csv")

# import "family_long_form_stats_df_<date>.csv"
# Contains the calculated performance satistics when evaluating performance of CAZy family prediction per test set.
# Written in long form with the columns: CAZy_family, Prediction_tool, Statistical_parameter, Statistic_value
cazy_family_long_df <- read.csv("data/family_long_form_stats_df_2021_11_09.csv")

# import "family_predicted_classifications_<date>.csv
# Contains the CAZy family epredictions for every protein for every CAZy family, and includes the 
# Rand indx and Adjusted Rand Index for every protein
fam_classification_df <- read.csv ("data/family_predicted_classifications_2021_11_09.csv")
```

<div id="summary">
The pCAZyme classifiers dbCAN, CUPP and eCAMI were independently evaluated against a high quality benchmark test set. The performances were evaluated upon the CAZyme/non-CAZyme differentiation and multilabel classification of CAZy family annotations. This notebook contains that statistical evaluation of the CAZyme classifiers.  
Results summary:    
- dbCAN and DIAMOND showed the strongest performances in CAZyme/non-CAZyme differentiation
- dbCAN was the strongest performing tool across all categories, Hotpep (a tool invoked by dbCAN) was the weakest
- The performances between CUPP and eCAMI were similar, although CUPP should a marginally better performance when comparing the multilabel classification of CAZy family annotations
- The performance of dbCAN may be optimised by substituting Hotpep with CUPP and/or eCAMI
</div>


# Introduction

The CAZyme classifiers dbCAN ([Zhange et al. 2018](https://academic.oup.com/nar/article/46/W1/W95/4996582)), CUPP ([Barrett and Lange, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6489277/)) and eCAMI ([Xu et al. 2019](https://academic.oup.com/bioinformatics/article/36/7/2068/5651014)) use different methods to predict if a protein is a CAZyme or non-CAZyme, and predict the CAZy family annotations for predicted CAZymes. These classifiers have not been independently evaluated against a high quality benchmark test set.

This notebook layouts out the independent evaluation of dbCAN, CUPP and eCAMI against a high quality benchmark test set. The tools were evaluated upon their ability to differentiate between CAZymes and non-CAZymes, and their performance of predicting the CAZy family annotations of predicted CAZymes.

dbCAN incorporates the three protein function classifiers HMMER ([Potter et al. 2018](https://pubmed.ncbi.nlm.nih.gov/29905871/)), Hotpep ([Busk et al. 2017](https://pubmed.ncbi.nlm.nih.gov/28403817/)), and DIAMOND ([Buchfink et al. 2015](https://www.nature.com/articles/nmeth.3176)). In order to comprehensively evaluate the preformance of dbCAN, the predictions from HMMER, Hotpep and DIAMOND were evaluated independently of each other, and the consensus prediction (a prediction which at least two of the tools agree upon) was defined as the dbCAN result.


# Test sets

A single test set of 100 CAZymes and 100 non-CAZymes with the highest sequence similarity (rated by bit-score ratio) was created per genomic assembly selected to be included in the benchmark test set. Choosing the 100 non-CAZymes with the highest sequence similarity was devised to increase increase the probability of causing confusion and evaulate the tools against a difficult test set to gather a better idea of the expected performance when using the classifiers. An equal number of CAZymes to non-CAZymes was selected to prevent over representation of one population over the other.

For inclusion of a genomic assembly for the creation of a test set, the assembly had to meet of all the following criteria:

- Contains at least 100 CAZymes
- Contains at least 100 non-CAZymes
- Has an 'Assembly level' of 'Complete Genome' in the NCBI Assembly database
- Protein records are still present in NCBI
- Not listed as an 'Anomalous assembly' in the NCBI Assembly database

The genomic assemblies were also chosen from a range of taxonomies to provide as informative image of the performance of the classifiers over a range of datasets that users may wish to analyse.

Table \@ref(tab:gassembly) contains the genomic assemblies used to create the test sets for the evaluation. In total 81 assemblies were chose, 1 from an Oomycete species (more Oomycete species with greater than 100 CAZymes in CAZy could not be found), 25 fungal Ascomycetes species were selected, 13 Yeast, 2 Eukaryote microorganisms, 20 Gram positive bacteria, and 20 Gram negative bacteria.

```{r cazomeCovStats, echo=FALSE}
mean_genome_cov = mean(test_set_cov_df$Genome_CAZome_percentage)
print("Mean percentage of genome incorporated in the CAZome across all test sets:")
print(mean_genome_cov)
sd_genome_cov = sd(test_set_cov_df$Genome_CAZome_percentage)
print("Standard deviation of the percentage of genome incorporated in the CAZome across all test sets:")
print(sd_genome_cov)

mean_cazome_cov = mean(test_set_cov_df$CAZome_coverage_percentage)
print("Mean percentage of CAZomes incorporated in the test set across all genomes:")
print(mean_cazome_cov)
sd_cazome_cov = sd(test_set_cov_df$CAZome_coverage_percentage)
print("Standard deviation of the percentage of CAZome incorporated in the test set across all genomes:")
print(sd_cazome_cov)
```

```{r cazomeCovHisto, echo=FALSE, results='asis' ,fig.cap="Histogram of CAZome coverage of the test sets for each respective source genomic assembly, overlayed by a box and whisker plot of the percentage of the CAZome incorproated in the test set."}
p.cazome.c <- ggplot(test_set_cov_df, aes(x=CAZome_coverage_percentage)) +
  geom_histogram(fill="#138d91", alpha=0.7, color="#0d4a4d") +
  geom_boxplot(
    outlier.shape=NA,
    width = 0.5,
    data=test_set_cov_df,
    aes(x=CAZome_coverage_percentage, y=5),
    alpha=0.5) +
  labs(x="CAZome coverage (%)", y="Number of test sets")
p.cazome.c
```

```{r saveCov. include=FALSE}
pdf(file = "cazomeCovHis.pdf", width = 8.58, height = 5.5)
p.cazome.c
dev.off()
```

# Evaluation of binary CAZyme/non-CAZyme classification

The assignment of CAZy family annotations to proteins that meet specific criteria by a CAZyme classifier identifies the protein as a CAZyme. If no CAZy family annotations are assigned to a protein by a CAZyme classifier, the tool has identified the protein as a non-CAZyme. This notebook evaluates the performance of the CAZyme classifiers dbCAN (which incorporates HMMER, Hotpep and DIAMOND), CUPP and eCAMI for this binary CAZyme/non-CAZyme classification.


## Summary statistics

For every classifier-test set pair, the specificity, sensitivity, prevision, F1-score and accuracy were calculated.

The mean of each statistical parameter was calculated for each classifier across all tests, to represent the overall performance of each CAZyme classifier.

These results are presented in table \@ref(tab:sumstats).

```{r sumstats, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. Data collected is the mean of call calculated statistical parameters across all test sets, plus and minus the standard devliation. All figures are rounded to 4 decimal places."}
# Calculate statistics
subset_spec <- binary_stat_df[which(binary_stat_df$Statistic_parameter == "Specificity"), ]
binary_specificity <- subset_spec %>% group_by(Prediction_tool) %>% summarise("Mean Specificity"=mean(Statistic_value), "Specificity Standard Deviation"=sd(Statistic_value))

subset_sens <- binary_stat_df[which(binary_stat_df$Statistic_parameter == "Sensitivity"), ]
binary_sensitivity <- subset_sens %>% group_by(Prediction_tool) %>% summarise("Mean Sensitivity"=mean(Statistic_value), "Sensitivity Standard Deviation"=sd(Statistic_value))

subset_prec <- binary_stat_df[which(binary_stat_df$Statistic_parameter == "Precision"), ]
binary_precision <- subset_prec %>% group_by(Prediction_tool) %>% summarise("Mean Precision"=mean(Statistic_value), "Precision Standard Deviation"=sd(Statistic_value))

subset_f1 <- binary_stat_df[which(binary_stat_df$Statistic_parameter == "FBeta-score"), ]
binary_f1_score <- subset_f1 %>% group_by(Prediction_tool) %>% summarise("Mean F1-score"=mean(Statistic_value), "F1-score Standard Deviation"=sd(Statistic_value))

subset_acc <- binary_stat_df[which(binary_stat_df$Statistic_parameter == "Accuracy"), ]
binary_accuracy <- subset_acc %>% group_by(Prediction_tool) %>% summarise("Mean Accuracy"=mean(Statistic_value), "Accuracy Standard Deviation"=sd(Statistic_value))

# combine data and build a single dataframe
binary_summary_df <- merge(binary_specificity, binary_sensitivity)
binary_summary_df <- merge(binary_summary_df, binary_precision)
binary_summary_df <- merge(binary_summary_df, binary_f1_score)
binary_summary_df <- merge(binary_summary_df, binary_accuracy)

# define factors
binary_summary_df$Prediction_tool <- factor(binary_summary_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

names(binary_summary_df)[names(binary_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

binary_summary_df <- binary_summary_df[c(2,5,6,3,1,4), ]
row.names(binary_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(binary_summary_df, caption="Overall performance of CAZyme classifiers differentiation between CAZymes and non-CAZymes", align='c', digits = 4) %>% kable_styling(full_width = F)

```


## Specificity

Specificity is the proportion of known negatives (known non-CAZymes) which are correctly classified as negatives (non-CAZymes).

Figure \@ref(fig:spec) is a graphical representation of the results calculated in table \@ref(tab:sumstats).

```{r spec, echo=FALSE, fig.cap="One-dimensional scatter plot of specificity scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_spec$Prediction_tool <- factor(subset_spec$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

p.binary.spec = ggplot(subset_spec %>% group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.binary.spec
```

```{r savebinarySpec, include=FALSE}
pdf(file = "binarySpec.pdf", width = 8.58, height = 5.5)
p.binary.spec
dev.off()
```

## Sensitivity

Sensitivity (also known as recall) is the proportion of known positives (CAZymes) that are correctly identified as positives (CAZymes).

Figure \@ref(fig:recallbc) graphically represents of the results calculated in table \@ref(tab:sumstats).


```{r recallbc, echo=FALSE, fig.cap="One-dimensional scatter plot of recall (sensitivity) scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_sens$Prediction_tool <- factor(subset_sens$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

p.binary.sens = ggplot(subset_sens %>% group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.binary.sens
```

```{r saveBinSens, include=FALSE}
pdf(file = "binarySensitivity.pdf", width = 8.58, height = 5.5)
p.binary.sens
dev.off()
```

## Precision

Precision is the proportion of positive predictions by the classifiers that are correct.

In this case, precision represents the fraction of CAZyme predictions by the classifiers that are correct, specifically the proportion of predicted CAZymes that are known CAZymes.

Figure \@ref(fig:precbc) is a visual representation of the results calculated in table \@ref(tab:sumstats).

```{r precbc, echo=FALSE, fig.cap="One-dimensional scatter plot of precision scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_prec$Prediction_tool <- factor(subset_prec$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

p.binary.prec = ggplot(subset_prec %>% group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.binary.prec
```

```{r saveBinPrec, include=FALSE}
pdf(file = "binaryPrec.pdf", width = 8.58, height = 5.5)
p.binary.prec
dev.off()
```

## F1-score

The F1-score is a harmonic (or weighted) average of recall and precision and provides an idea of the overall performance of the tool, 0 being the lowest and 1 being the best performance. Figure \@ref(fig:f1bc) shows the F1-score from each test set, for each classifier.

```{r f1bc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}

subset_f1$Prediction_tool <- factor(subset_f1$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

p.binary.f1 = ggplot(subset_f1 %>% group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.binary.f1
```

```{r saveBinF1, include=FALSE}
pdf(file = "binaryF1.pdf", width = 8.58, height = 5.5)
p.binary.f1
dev.off()
```

## Accuracy

Accuarcy (calculated using (TP + TN) / (TP + TN + FP + FN) ) provides an idea of the overall performance of the classifiers as a measure of the degree to which their CAZyme/non-CAZyme predictions conforms to the correct result. Figure \@ref(fig:accbc) is a plot of respective data from table \@ref(tab:sumstats).


```{r accbc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}
subset_acc$Prediction_tool <- factor(subset_acc$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

p.binary.acc = ggplot(subset_acc %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy") 
p.binary.acc
```

```{r saveBinAcc, include=FALSE}
pdf(file = "binaryAcc.pdf", width = 8.58, height = 5.5)
p.binary.acc
dev.off()
```

Below is a combination (3x2) plot of the above plots for evaluating the binary CAZyme/non-CAZyme classification performance of the CAZyme classifiers.

```{r binaryCombo, include=FALSE}
axis_values = seq(0.3,1, by = 0.05)
mp.binary.spec = ggplot(subset_spec %>% group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold"),
        axis.text.x=element_text(angle=-90)) +
  xlab("Classifier") +
  ylab("Specificity") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.sens = ggplot(subset_sens %>% group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Sensitivity") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.prec = ggplot(subset_prec %>% group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Precision") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.f1 = ggplot(subset_f1 %>% group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("F1-score") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.acc = ggplot(subset_acc %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Accuracy") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
plot_grid(mp.binary.spec, mp.binary.sens, mp.binary.prec, mp.binary.f1, mp.binary.acc, labes='AUTO', ncol=3)
```

```{r saveBinMuliPlot, indluce=FALSE}
png("binary_multiplot.png", units='in', width=8, height=6,res=900)
plot_grid(mp.binary.spec, mp.binary.sens, mp.binary.prec, mp.binary.f1, mp.binary.acc, labes='AUTO', ncol=3)
dev.off()
```

## Expected Range of Accruacy

The statistics evaluated above provide an idea of the general performance of the tools, but they do not provide an idea of the expect range of performance. Specifically, the data does not provide a clear image of the best and worse performance a user can expect when using these tools.

To compare the expected typical range in accuracies for each classifier, 6 test sets (identified by the source genomic assemblies) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times each, and for each bootstrap sample the accuracy calculated. The accuracies of the bootstrap samples for each classifier were plotted on stacked histograms, shown in figure \@ref(fig:bsacc).

```{r bsacc, echo=FALSE, fig.cap="Stacked histograms of bootstrap sample accuracies of CAZyme classifiers' differentiation between CAZymes and non-CAZymes. 6 test sets (identified by their source genomic assembly) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times. The accuracy of each of the 600 bootstrap samples per test set were plotted as a stacked histogram."}
bootstrap_results_df$Prediction_tool <- factor(bootstrap_results_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

# plot generated to see the number of items in each bin for the figure above, and additional boxplots to represent the distribution of the data
p.bs.annotated = ggplot(bootstrap_results_df %>% group_by(Genomic_accession), aes(x=accuracy)) +
  geom_histogram(alpha=0.7, color="black", binwidth = 0.01, aes(fill=Genomic_accession), position = ) +
  geom_boxplot(outlier.shape=NA, width = 10, data=bootstrap_results_df, aes(x=accuracy, y=100), alpha=0.5) +
  scale_fill_brewer(palette="Set1") +
  # stat_bin(aes(y=..count.. + 5, label=..count..), geom="text", binwidth = 0.01, size=3.5, angle=90) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12),
        legend.position="bottom") +
  labs(x = "Accuracy", y = "Number of bootstrap samples", fill="Test set key") +
  # scale_y_continuous(breaks = seq(0,125, by = 25)) +
  # scale_x_continuous(breaks = seq(0.75,1, by = 0.05)) +
  facet_wrap(~ Prediction_tool, ncol=2) 
  # remove the comment if you want to see the histogram with the total number per column printed
p.bs.annotated
```

```{r saveBootstrap, include=FALSE}
pdf(file = "binaryBootstrapAccAnnotated.pdf", width = 8.25, height = 11)
p.bs.annotated
dev.off()

svg(file = "binaryBootstrapAccAnnotated.svg", width = 8.25, height = 11)
p.bs.annotated
dev.off()
```

### Investigation of Non-CAZymes classified as CAZymes (False positives)

Few of the known non-CAZymes were classified as CAZymes by the CAZyme classifiers. The cause of the non-CAZymes being classified as CAZymes may be because of a very high sequence similarity between the non-CAZyme and known CAZymes, and/or CAZy incorrectly classifying the non-CAZyme as a CAZyme and not a CAZyme. The latter case maybe true if all 6 classifiers classify the non-CAZyme as a CAZyme.

First the equation to calcualte the correlation must be determined. Either the Pearson correlation or Spearman's correlation coefficients could be calculated. Pearson's correlation assumes the data is normally distributed, Spearman's correlation is a nonparametric statistic becuase it does not presume a normal distribution. Figure \@ref(fig:fp.cor)[A] shows a histogram of the number of prediciton tools that generated false positives and figure \@ref(fig:fp.cor)[B] a boxplot of the highest BLAST score ratios of false positives against CAZymes in the same test set. Both plots demonstrate the data is not normally distributed. Additionally, figure \@ref(fig:fp.cor)[B] shows the data contains outliers, which Pearson's correlation is also sensitive to. Therefore, the Spearman's correlation coefficient was calculated, which is shown in \@ref(fig:fp.cor)[C].

```{r fp.cor, echo=FALSE, fig.cap="Correlation between BLAST score ratio between the number of CAZyme classifiers that incorrectly identified a known non-CAZymes as a CAZyme and the highest BLAST score ratio of the known non-CAZyme and a known CAZyme fromt the same test set."}
binary_fp_predictions <- false_positive[!duplicated(false_positive), ]
binary_fp_predictions
p.total.fp <- ggplot(binary_fp_predictions, )
# plot bar charts to show wish tools produced the most fp and fn predictions
# find correlation between number of tools and BLAST score ratio
# Calculate Spearman rank correlation not Pearson correlation becuase do not want to apply assumption that the data is normally distributed
nt_bsr_cor = cor(false_positive_dfs$Number_of_tools, false_positive_dfs$BLAST_score_ratio, method = "spearman")  

p.nt = ggplot(false_positive_dfs, aes(x=Number_of_tools, fill=Number_of_tools)) +
  geom_histogram(alpha=0.7, color="black") +
  scale_fill_manual(values=col_vector) +
  theme(legend.position = "none",
        legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12)) +
  xlab("Number of Classifiers") +
  ylab("Number of Proteins")

p.bsr = ggplot(false_positive_dfs, aes(x="", y=BLAST_score_ratio)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12)) +
  ylab("BLAST Score Ratio") +
  xlab("All BLAST Score Ratios")

# pdf(file = "binaryNonCAZymes.pdf", width = 8.58, height = 5.5)
p.fp.cor = ggplot(false_positive_dfs, aes(x=Number_of_tools, y=BLAST_score_ratio, colour=BLAST_score_ratio)) +
  geom_point() +
  geom_smooth(method="lm", color="#cc1b70") +
  annotate("text", x=5.5, y=2.1,
            label=paste("rs = ", format(nt_bsr_cor, digits=3)), color="#cc1b70") + 
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        legend.position="bottom",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12)) +
  labs(x = "Number of Classifiers", y = "BLAST Score Ratio", color="BLAST Score Ratio")
# p.fp.cor
# dev.off()

# pdf(file = "binaryNonCAZymesFull.pdf", width = 8.58, height = 5.5)
left_panel <- plot_grid(p.nt, p.bsr, nrow=2, labels=c("A", "B"))
plot_grid(left_panel, p.fp.cor, ncol=2, labels = c("", "C"), rel_widths = c(0.75,2))
# dev.off()
```

## Conclusions on the Binary CAZyme/non-CAZyme Prediction Performance

Overall, all tools show a low probability of producing false positives (misclassifying a non-CAZyme as a CAZyme), and few of the positive predictions are false positives. Therefore, we can be confident in that the CAZyme predictions made by each of these tools are most likely (typically 90-100%) correct. However, all the classifiers demonstrated a consistent behaviour to not identify all CAZymes within a CAZome. Therefore, we can be confident in the CAZyme predictions, but should not presume all non-CAZyme predictions are correct; these classifiers are unlikely to identify the complete CAZome although a near-complete CAZome will be accuracetly identified.

dbCAN consistently demonstrated the strongest performance in all catagories, inferring that eCAMI and CUPP are not suitable replacements of the CAZyme classifier. Hotpep consistently demonstrated the weakest performance, and is incorporated within dbCAN. Therefore, substituting eCAMI and/or CUPP into dbCAN instead of Hotpep may futher improve the performance of dbCAN. The new k-mer based methods, eCAMI and CUPP demonstrated similar performances. CUPP showed a more consistent performance and eCAMI demonstrating a greater range in performance although its mean performance was fractionally greater than that of CUPP. However, more bootstrap calculated accuracy scores feel within the range of 0.9-1.0 for CUPP than eCAMI. This infers that a CUPP may typically provide a better performance than eCAMI, although eCAMI does have the potential on some occations to out perform CUPP, depending on the test set.


# Evaluation of CAZy Class classification performance

CAZy groups CAZymes into CAZy families by sequence similarity, and CAZy families are grouped into one of 6 functional classes. The CAZyme classifiers predict the CAZy family annotations of predicted CAZymes, but it is of interest to see what the level of performance of the classiferis is at the CAZy class level. Specifically, a classifier may struggle to predict the correct CAZy class for a CAZyme but consistently predict the correct CAZy class. Therefore, the aim of this part of the evaluation is to evaluate the performance of the classifiers to predict the correct CAZy class of predict CAZymes.


## General trends in CAZy class classification performance

Below is a table summary all statistical parameters calculated in order to evaluate the performance of the CAZy class classification for each prediction tool across all CAZy classes.

```{r cazyClassStasTable, echo=FALSE}
# Calculate statistics
class_subset_spec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
class_subset_spec  <- class_subset_spec[complete.cases(class_subset_spec), ]
class_specificity <- class_subset_spec %>% group_by(Prediction_tool) %>% summarise("Mean Specificity"=mean(Statistic_value), "Specificity Standard Deviation"=sd(Statistic_value))

class_subset_sens <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
class_subset_sens  <- class_subset_sens[complete.cases(class_subset_sens), ]
class_sensitivity <- class_subset_sens %>% group_by(Prediction_tool) %>% summarise("Mean Sensitivity"=mean(Statistic_value), "Sensitivity Standard Deviation"=sd(Statistic_value))

class_subset_prec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Precision"), ]
class_subset_prec  <- class_subset_prec[complete.cases(class_subset_prec), ]
class_precision <- class_subset_prec %>% group_by(Prediction_tool) %>% summarise("Mean Precision"=mean(Statistic_value), "Precision Standard Deviation"=sd(Statistic_value))

class_subset_f1 <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
class_subset_f1  <- class_subset_f1[complete.cases(class_subset_f1), ]
class_f1_score <- class_subset_f1 %>% group_by(Prediction_tool) %>% summarise("Mean F1-score"=mean(Statistic_value), "F1-score Standard Deviation"=sd(Statistic_value))

class_subset_acc <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Accuracy"), ]
class_subset_acc  <- class_subset_acc[complete.cases(class_subset_acc), ]
class_accuracy <- class_subset_acc %>% group_by(Prediction_tool) %>% summarise("Mean Accuracy"=mean(Statistic_value), "Accuracy Standard Deviation"=sd(Statistic_value))

# combine data and build a single dataframe
class_summary_df <- merge(class_specificity, class_sensitivity)
class_summary_df <- merge(class_summary_df, class_precision)
class_summary_df <- merge(class_summary_df, class_f1_score)
class_summary_df <- merge(class_summary_df, class_accuracy)

# define factors
class_summary_df$Prediction_tool <- factor(class_summary_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

names(class_summary_df)[names(class_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

class_summary_df <- class_summary_df[c(2,5,6,3,1,4), ]
row.names(class_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  class_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy class classification performance",
  align='c',
  digits = 4) %>% kable_styling(full_width = F)

```

Below a proportional area plot representing the F-beta score for each CAZyme classifier for each test set is generated. each square is sized proportional to the relative sample size. Every class was not included in every sample, resulting in different sample sizes between CAZy classes, the same between classifiers.

```{r statsFbeta echo=FALSE}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

# subset Fbeta-scores
class_fbeta_subset <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]

# Not every CAZy class is present in every test set, where it was not present a value of NA was given.
# these rows need to be dropped so that the proportional area plot represents the number of test sets containing
# that CAZy class
class_fbeta_subset  <- class_fbeta_subset[complete.cases(class_fbeta_subset), ]

# classify the Fbeta-scores into bins
val.class <- vector()

for(i in 1:nrow(class_fbeta_subset)) {
  if(class_fbeta_subset[i, 6] == 1){val.class <- append(val.class, '[1.00]')} 
  else if (class_fbeta_subset[i, 6] == 0){val.class <- append(val.class, '[0.00]')}
  else if (class_fbeta_subset[i, 6] < 1 && class_fbeta_subset[i, 6] >= 0.95){val.class <- append(val.class, '(0.95, 1.00]')}
  else if (class_fbeta_subset[i, 6] < 0.95 && class_fbeta_subset[i, 6] >= 0.9){val.class <- append(val.class, '(0.90, 0.95]')}
  else if (class_fbeta_subset[i, 6] < 0.90 && class_fbeta_subset[i, 6] >= 0.85){val.class <- append(val.class, '(0.85, 0.90]')}
  else if (class_fbeta_subset[i, 6] < 0.85 && class_fbeta_subset[i, 6] >= 0.80){val.class <- append(val.class, '(0.80, 0.85]')}
  else if (class_fbeta_subset[i, 6] < 0.80 && class_fbeta_subset[i, 6] >= 0.75){val.class <- append(val.class, '(0.75, 0.80]')}
  else if (class_fbeta_subset[i, 6] < 0.75 && class_fbeta_subset[i, 6] >= 0.70){val.class <- append(val.class, '(0.70, 0.75]')}
  else if (class_fbeta_subset[i, 6] < 0.70 && class_fbeta_subset[i, 6] >= 0.65){val.class <- append(val.class, '(0.65, 0.70]')}
  else if (class_fbeta_subset[i, 6] < 0.65 && class_fbeta_subset[i, 6] >= 0.60){val.class <- append(val.class, '(0.60, 0.65]')}
  else if (class_fbeta_subset[i, 6] < 0.60 && class_fbeta_subset[i, 6] >= 0.55){val.class <- append(val.class, '(0.55, 0.60]')}
  else if (class_fbeta_subset[i, 6] < 0.55 && class_fbeta_subset[i, 6] >= 0.50){val.class <- append(val.class, '(0.50, 0.55]')}
  else if (class_fbeta_subset[i, 6] < 0.50 && class_fbeta_subset[i, 6] >= 0.45){val.class <- append(val.class, '(0.45, 0.50]')}
  else if (class_fbeta_subset[i, 6] < 0.45 && class_fbeta_subset[i, 6] >= 0.40){val.class <- append(val.class, '(0.40, 0.45]')}
  else if (class_fbeta_subset[i, 6] < 0.40 && class_fbeta_subset[i, 6] >= 0.35){val.class <- append(val.class, '(0.35, 0.40]')}
  else if (class_fbeta_subset[i, 6] < 0.35 && class_fbeta_subset[i, 6] >= 0.30){val.class <- append(val.class, '(0.30, 0.35]')}
  else if (class_fbeta_subset[i, 6] < 0.30 && class_fbeta_subset[i, 6] >= 0.25){val.class <- append(val.class, '(0.25, 0.30]')}
  else if (class_fbeta_subset[i, 6] < 0.25 && class_fbeta_subset[i, 6] >= 0.20){val.class <- append(val.class, '(0.20, 0.25]')}
  else if (class_fbeta_subset[i, 6] < 0.20 && class_fbeta_subset[i, 6] >= 0.15){val.class <- append(val.class, '(0.15, 0.20]')}
  else if (class_fbeta_subset[i, 6] < 0.15 && class_fbeta_subset[i, 6] >= 0.10){val.class <- append(val.class, '(0.10, 0.15]')}
  else if (class_fbeta_subset[i, 6] < 0.10 && class_fbeta_subset[i, 6] >= 0.05){val.class <- append(val.class, '(0.05, 0.10]')}
  else if (class_fbeta_subset[i, 6] < 0.05 && class_fbeta_subset[i, 6] >= 0){val.class <- append(val.class, '(0.00, 0.05]')}
  else {val.class <- append(val.class, '< 0')}
}
class_fbeta_subset$val.class <- val.class

# set order data is presented
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) 
class_fbeta_subset$CAZy_class <- factor(class_fbeta_subset$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
class_fbeta_subset$val.class <- factor(class_fbeta_subset$val.class, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.classF1 = ggally_count(class_fbeta_subset, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.class)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.classF1
```

```{r saveCazyClassFbeta, include=FALSE}
svg(file = "mlcClassF1.svg",  width = 8.25, height = 11)
p.classF1
dev.off()
```

A dataframe of the number of test sets containing each CAZy class is generated.

```{r calcCazyClassSamples, echo=FALSE}
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

calc_class_sample_size <- function(class_df, tool) {
  tool_data <- c(tool)
  tool_df = class_df[which(class_df$Prediction_tool == tool), ]
  cazy_classes = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
  for (cazy_class in cazy_classes) {
    sample_size <- sum(tool_df$CAZy_class == cazy_class)
    tool_data = c(tool_data, sample_size)
  }
  tool_data_m = matrix(tool_data,nrow=1)
  return(tool_data_m)
}

dbcan_row = calc_class_sample_size(class_fbeta_subset, 'dbCAN')
hmmer_row = calc_class_sample_size(class_fbeta_subset, 'HMMER')
diamond_row = calc_class_sample_size(class_fbeta_subset, 'DIAMOND')
hotpep_row = calc_class_sample_size(class_fbeta_subset, 'Hotpep')
cupp_row = calc_class_sample_size(class_fbeta_subset, 'CUPP')
ecami_row = calc_class_sample_size(class_fbeta_subset, 'eCAMI')

cazy_class_sample_size <- rbind(dbcan_row, hmmer_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, diamond_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, hotpep_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, cupp_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, ecami_row)
cazy_class_sample_size_df <- as.data.frame(cazy_class_sample_size)
names(cazy_class_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
cazy_class_sample_size_df
```

The sensitivity of each CAZyme classifier can be plotted against the specificity for each CAZy class, however plotting all CAZy classes in a single plot produces an overally cramped plot, unless very few test sets were used.

```{r cazyClassSpecVsSens, echo=FALSE}
# subset specificity scores
cazy_class_specificity_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
# rename columns
names(cazy_class_specificity_df)[names(cazy_class_specificity_df) == "Statistic_value"] <- "Specificity"
cazy_class_specificity_df <- cazy_class_specificity_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Specificity")]

# subset sensitivity scores
cazy_class_sens_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
names(cazy_class_sens_df)[names(cazy_class_sens_df) == "Statistic_value"] <- "Sensitivity"
cazy_class_sens_df <- cazy_class_sens_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Sensitivity")]

# merge dataframes
cazy_class_spec_sense_df <- merge(cazy_class_specificity_df, cazy_class_sens_df, by=c("Genomic_accession", "Prediction_tool", "CAZy_class"))

# specificity was not predicted for every CAZy class in eveyr test set, this is the result of the CAZy class not being present in the test set and the classifier not predicting the presence of the CAZy class in the test set
# In these cases specificity is NA
# NA values are removed
cazy_class_spec_sense_df  <- cazy_class_spec_sense_df[complete.cases(cazy_class_spec_sense_df), ]

# one plot per class with be made using facet_wrap by Prediciton_tool.
p.class.spec.sense = ggplot(cazy_class_spec_sense_df %>% group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity", color="Classifer") +
  facet_wrap(~CAZy_class)
p.class.spec.sense
```


## Performance per CAZy class

Below the prediction sensitivity is plotted against the specificity for each classifier, and a separate plot is generated for each CAZy class.

The scatter plots of sensitivity against specificity overlay a coloured contour to highlight the distribution of the points. When too many points have the same value a contour cannot be generated. In order to plot a contour noise is added to the data. The original data is used to plot the scatter plot and the data with added noise is used to plot the contour.

The percentage of the data points which need noise to be added to them in order to generate a contour varies from data set to data set. To change the percentage of the data points with noise added to them, change the third value of call to the function `plot.class.sens.vs.spec()`, which is used to generate the plots. The third value is the percentage of data points to add noise to, written in decimal form.

```{r addNoise, echo=FALSE}
# When plotting scatter plots overlaying a coloured contour, contours cannot be ploted if too many data points have the same value
# To sort this add noise to the data

add_noise <- function(data, corrupt_percent, min.val, max.val){
  corrupt <- rbinom(length(data), 1, corrupt_percent)    # choose an average of <corrupt_percent>% to corrupt at random
  corrupt <- as.logical(corrupt)
  noise <- rnorm(sum(corrupt), min.val, max.val) # generate the noise to add
  data[corrupt] <- data[corrupt] - noise      # about 10% of x has been corrupted
  return(data)
}

```

```{r buildClassSensVsSpecPlot, echo=FALSE}
plot.class.sens.vs.spec <- function(class.data.df, cazy_class, corruption_size, min.val, max.val){
  # subset rows containing data for the given CAZy class
  class.subset.df <- class.data.df[which(class.data.df$CAZy_class == cazy_class), ]
  
  # set the order prediction tools are plotted in the plot
  class.subset.df$Prediction_tool <- factor(class.subset.df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI'))

  # subset and add noise to the data to enable plotting contours, contours cannot be plotted if too many data points have the same value
  class.sens.data <- class.subset.df$Sensitivity
  class.sens.data.noise <- add_noise(class.sens.data, corruption_size, min.val, max.val)

  class.spec.data <- class.subset.df$Specificity
  class.spec.data.noise <- add_noise(class.spec.data, corruption_size, min.val, max.val)
  
  # add the data with noise as new columns to the df
  class.subset.df$Sens_noise <- class.sens.data.noise
  class.subset.df$Spec_noise <- class.spec.data.noise
  
  # generate the plot
  p.class.sens.spec = ggplot(class.subset.df %>% group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity)) +
    geom_density_2d_filled(alpha = 0.5, aes(x=Sens_noise, y=Spec_noise)) +
    geom_point() +
    scale_color_manual(values=colour_set) +
    theme(plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11)) +
    labs(x = "Sensitivity", y = "Specificity", color="Classifer", fill="Density") +
    xlim(0.4, 1.0) +
    ylim(0.9, 1.0) +
    facet_wrap(~ Prediction_tool)
  p.class.sens.spec
}
```

#### CAZy class classification for GH

```{r classGhSpecSens, echo=FALSE}
p.class.gh.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'GH', 0.3, 0.001, 0.01)
p.class.gh.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classGhSpecVsSens", width = 11.25, height = 7)
p.class.gh.spec.sens
dev.off()
```

#### CAZy class classification for GT

```{r classGhSpecSens, echo=FALSE}
p.class.gt.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'GT', 0.5, 0.001, 0.01)
p.class.gt.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classGtSpecVsSens", width = 11.25, height = 7)
p.class.gt.spec.sens
dev.off()
```

#### CAZy class classification for PL

```{r classGhSpecSens, echo=FALSE}
p.class.pl.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'PL', 0.5, 0.001, 0.01)
p.class.pl.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classPlSpecVsSens", width = 11.25, height = 7)
p.class.pl.spec.sens
dev.off()
```

#### CAZy class classification for CE

```{r classGhSpecSens, echo=FALSE}
p.class.ce.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'CE', 0.4, 0.001, 0.01)
p.class.ce.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classCeSpecVsSens", width = 11.25, height = 7)
p.class.ce.spec.sens
dev.off()
```

#### CAZy class classification for AA

```{r classGhSpecSens, echo=FALSE}
p.class.aa.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'AA', 0.3, 0.001, 0.01)
p.class.aa.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classAaSpecVsSens", width = 11.25, height = 7)
p.class.aa.spec.sens
dev.off()
```

#### CAZy class classification for CBM

```{r classGhSpecSens, echo=FALSE}
p.class.cbm.spec.sens <- plot.class.sens.vs.spec(cazy_class_spec_sense_df, 'CBM', 0.3, 0.001, 0.01)
p.class.cbm.spec.sens
```

```{r saveclassGhSpecSens, echo=FALSE}
pdf(file = "classCbmSpecVsSens", width = 11.25, height = 7)
p.class.cbm.spec.sens
dev.off()
```



## Rand Index and Adjusted Rand Index of CAZy Class Prediction

A single CAZyme can be included in multiple CAZy classes leading to the multilabel classification of CAZymes. To address this and evaluate the multilabel classification of CAZy classes the Rand Index (RI) and Adjusted Rand Index (ARI) were calculated.

The RI is the measure of accuracy across all potential classifications of a protein. The RI ranges from 0 (no correct annotations) to 1 (all annotations correct). The ARI is the RI adjusted for chance, where 0 is the equivalent to assigning the CAZy class annotations randomly, -1 where the annotations are systematically handed out incorrectly and 1 where the annotations are all correct.

```{r riCalc, echoFALSE}
class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

class_ri_stats_df <- class_ri_ari_raw_df %>% group_by(Prediction_tool) %>% summarise("Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index))

kable(class_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ariCalc, echoFALSE}
class_ari_stats_df <- class_ri_ari_raw_df %>% group_by(Prediction_tool) %>% summarise("Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index))

kable(class_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

Plot are violin plots underlying scatter plots, presenting the RI and ARI for every protein across all test sets.

```{r classRI, echo=FALSE, fig.cap="Violin plot of Rand Index (RI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

# plot RI
p.ri.class = ggplot(class_ri_ari_raw_df %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  scale_y_continuous(breaks = seq(0,1, by = 0.1))
p.ri.class
```

```{r classRI, echo=FALSE, fig.cap="Violin plot of Adjusted Rand Index (ARI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

p.ari.class = ggplot(class_ri_ari_raw_df %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Adjusted_Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  scale_y_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))
p.ari.class
```


# Evaluation CAZy family classification performance

The following section evaluates the performance of the CAZyme classifiers to predict CAZy family classifications.

## General trends in performance across all CAZy families

Below is a table summarising the overall CAZy family classifications for each test set across all CAZy families.

```{r cazyClassStasTable, echo=FALSE}
# Calculate statistics
fam_subset_spec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Specificity"), ]
fam_subset_spec  <- fam_subset_spec[complete.cases(fam_subset_spec), ]
fam_specificity <- fam_subset_spec %>% group_by(Prediction_tool) %>% summarise("Mean Specificity"=mean(Statistic_value), "Specificity Standard Deviation"=sd(Statistic_value))

fam_subset_sens <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Sensitivity"), ]
fam_subset_sens  <- fam_subset_sens[complete.cases(fam_subset_sens), ]
fam_sensitivity <- fam_subset_sens %>% group_by(Prediction_tool) %>% summarise("Mean Sensitivity"=mean(Statistic_value), "Sensitivity Standard Deviation"=sd(Statistic_value))

fam_subset_prec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Precision"), ]
fam_subset_prec  <- fam_subset_prec[complete.cases(fam_subset_prec), ]
fam_precision <- fam_subset_prec %>% group_by(Prediction_tool) %>% summarise("Mean Precision"=mean(Statistic_value), "Precision Standard Deviation"=sd(Statistic_value))

fam_subset_f1 <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Fbeta_score"), ]
fam_subset_f1  <- fam_subset_f1[complete.cases(fam_subset_f1), ]
fam_f1_score <- fam_subset_f1 %>% group_by(Prediction_tool) %>% summarise("Mean F1-score"=mean(Statistic_value), "F1-score Standard Deviation"=sd(Statistic_value))

fam_subset_acc <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Accuracy"), ]
fam_subset_acc  <- fam_subset_acc[complete.cases(fam_subset_acc), ]
fam_accuracy <- fam_subset_acc %>% group_by(Prediction_tool) %>% summarise("Mean Accuracy"=mean(Statistic_value), "Accuracy Standard Deviation"=sd(Statistic_value))

# combine data and build a single dataframe
fam_summary_df <- merge(fam_specificity, fam_sensitivity)
fam_summary_df <- merge(fam_summary_df, fam_precision)
fam_summary_df <- merge(fam_summary_df, fam_f1_score)
fam_summary_df <- merge(fam_summary_df, fam_accuracy)

# define factors
fam_summary_df$Prediction_tool <- factor(fam_summary_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

names(fam_summary_df)[names(fam_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

fam_summary_df <- fam_summary_df[c(2,5,6,3,1,4), ]
row.names(fam_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  fam_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy family classification performance across all CAZy families",
  align='c',
  digits = 4) %>% kable_styling(full_width = F)

```

The evaluate the overall performance of each classifier, for each CAZy family, the F1-score was calculated for every family. Families were grouped by their parent CAZy class and the distribution of the F1-scores is shown in figure \@ref(fig:fbetaclass).

```{r fbetaclass, echo=FALSE, fig.cap="Proportaional area plot of F1-score per CAZy distribution per CAZy class."}
# F-beta scores already subset previously, and are stored in the var fam_subset_f1

# classify the Fbeta-scores into bins
val.fam <- vector()

for(i in 1:nrow(fam_subset_f1)){
  if(fam_subset_f1[i, 5] == 1){val.fam <- append(val.fam, '[1.00]')} 
  else if (fam_subset_f1[i, 5] == 0){val.fam <- append(val.fam, '[0.00]')}
  else if (fam_subset_f1[i, 5] < 1 && fam_subset_f1[i, 5] >= 0.95){val.fam <- append(val.fam, '(0.95, 1.00]')}
  else if (fam_subset_f1[i, 5] < 0.95 && fam_subset_f1[i, 5] >= 0.9){val.fam <- append(val.fam, '(0.90, 0.95]')}
  else if (fam_subset_f1[i, 5] < 0.90 && fam_subset_f1[i, 5] >= 0.85){val.fam <- append(val.fam, '(0.85, 0.90]')}
  else if (fam_subset_f1[i, 5] < 0.85 && fam_subset_f1[i, 5] >= 0.80){val.fam <- append(val.fam, '(0.80, 0.85]')}
  else if (fam_subset_f1[i, 5] < 0.80 && fam_subset_f1[i, 5] >= 0.75){val.fam <- append(val.fam, '(0.75, 0.80]')}
  else if (fam_subset_f1[i, 5] < 0.75 && fam_subset_f1[i, 5] >= 0.70){val.fam <- append(val.fam, '(0.70, 0.75]')}
  else if (fam_subset_f1[i, 5] < 0.70 && fam_subset_f1[i, 5] >= 0.65){val.fam <- append(val.fam, '(0.65, 0.70]')}
  else if (fam_subset_f1[i, 5] < 0.65 && fam_subset_f1[i, 5] >= 0.60){val.fam <- append(val.fam, '(0.60, 0.65]')}
  else if (fam_subset_f1[i, 5] < 0.60 && fam_subset_f1[i, 5] >= 0.55){val.fam <- append(val.fam, '(0.55, 0.60]')}
  else if (fam_subset_f1[i, 5] < 0.55 && fam_subset_f1[i, 5] >= 0.50){val.fam <- append(val.fam, '(0.50, 0.55]')}
  else if (fam_subset_f1[i, 5] < 0.50 && fam_subset_f1[i, 5] >= 0.45){val.fam <- append(val.fam, '(0.45, 0.50]')}
  else if (fam_subset_f1[i, 5] < 0.45 && fam_subset_f1[i, 5] >= 0.40){val.fam <- append(val.fam, '(0.40, 0.45]')}
  else if (fam_subset_f1[i, 5] < 0.40 && fam_subset_f1[i, 5] >= 0.35){val.fam <- append(val.fam, '(0.35, 0.40]')}
  else if (fam_subset_f1[i, 5] < 0.35 && fam_subset_f1[i, 5] >= 0.30){val.fam <- append(val.fam, '(0.30, 0.35]')}
  else if (fam_subset_f1[i, 5] < 0.30 && fam_subset_f1[i, 5] >= 0.25){val.fam <- append(val.fam, '(0.25, 0.30]')}
  else if (fam_subset_f1[i, 5] < 0.25 && fam_subset_f1[i, 5] >= 0.20){val.fam <- append(val.fam, '(0.20, 0.25]')}
  else if (fam_subset_f1[i, 5] < 0.20 && fam_subset_f1[i, 5] >= 0.15){val.fam <- append(val.fam, '(0.15, 0.20]')}
  else if (fam_subset_f1[i, 5] < 0.15 && fam_subset_f1[i, 5] >= 0.10){val.fam <- append(val.fam, '(0.10, 0.15]')}
  else if (fam_subset_f1[i, 5] < 0.10 && fam_subset_f1[i, 5] >= 0.05){val.fam <- append(val.fam, '(0.05, 0.10]')}
  else if (fam_subset_f1[i, 5] < 0.05 && fam_subset_f1[i, 5] >= 0){val.fam <- append(val.fam, '(0.00, 0.05]')}
  else {val.fam <- append(val.fam, '< 0')}
}
# add the bins to the df
fam_subset_f1$val.fam <- val.fam

# add the parent CAZy class to the df for facet wrapping

# retrieve all the family names
dbcan.fam.subset <- cazy_family_long_df[which(cazy_family_long_df$Prediction_tool == "dbCAN"), ]
tn.dbcan.fam.subset <- dbcan.fam.subset[which(dbcan.fam.subset$Statistical_parameter == "TNs"), ]

# separate names into one vector per CAZy class
gh.names = tn.dbcan.fam.subset[1:172,]$CAZy_family
gt.names = tn.dbcan.fam.subset[173:287,]$CAZy_family
pl.names = tn.dbcan.fam.subset[288:329,]$CAZy_family
ce.names = tn.dbcan.fam.subset[330:348,]$CAZy_family
aa.names = tn.dbcan.fam.subset[349:365,]$CAZy_family
cbm.names = tn.dbcan.fam.subset[366:454,]$CAZy_family

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class
  
pl.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class
  
ce.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class
  
aa.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class
  
cbm.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
fam_fbeta_df <- rbind(gh.subset, gt.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, pl.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, ce.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, aa.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, cbm.subset)

# set order data is presented
fam_fbeta_df$Prediction_tool <- factor(fam_fbeta_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) 
fam_fbeta_df$CAZy_class <- factor(fam_fbeta_df$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
fam_fbeta_df$val.fam <- factor(fam_fbeta_df$val.fam, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.famF1 = ggally_count(fam_fbeta_df, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.fam)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.famF1
```

```{r saveFambeta, include=FALSE}
svg(file = "mlcFamF1.svg",  width = 8.25, height = 11)
p.famF1
dev.off()
```

\@ref(fig:fbetaclass)
Below is a table displaying the number of test sets in which each CAZy class was present, and were used to draw the proporitonal areas for each class in figure\@ref(fig:fbetaclass).

```{r calcFamFbetaSampleSize, echo=FALSE}
fam_dbcan_row = calc_class_sample_size(fam_fbeta_df, 'dbCAN')
fam_hmmer_row = calc_class_sample_size(fam_fbeta_df, 'HMMER')
fam_diamond_row = calc_class_sample_size(fam_fbeta_df, 'DIAMOND')
fam_hotpep_row = calc_class_sample_size(fam_fbeta_df, 'Hotpep')
fam_cupp_row = calc_class_sample_size(fam_fbeta_df, 'CUPP')
fam_ecami_row = calc_class_sample_size(fam_fbeta_df, 'eCAMI')

cazy_class_fam_sample_size_df <- rbind(fam_dbcan_row, fam_hmmer_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_diamond_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_hotpep_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_cupp_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_ecami_row)
cazy_class_fam_sample_size_df <- as.data.frame(cazy_class_fam_sample_size_df)
names(cazy_class_fam_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
cazy_class_fam_sample_size_df
```


## Performance per CAZy family

To evaluate the performance of predicting each CAZy family independent of all other CAZy families, the sensitivity and precision for each CAZy family, for each CAZyme classifier was calculated and plotted against each other (Fig.\@ref(fig:famrllvspc)). Whereas sensitivity was plotted against sensitivity for CAZy classes, owing to the extremely small variation in specificity scores, sensitivity was plotted as a percentage against log10 of the specificity percentage.

```{r prepPerformancePerFam, echo=FALSE}
# func for adding CAZy class column for a given CAZy class
add_parent_class <- function(fam.df, fam.names, cazy.class){
  fam.df.subset <- fam.df[which(fam.df$CAZy_family %in% fam.names), ]
  class.names <- rep(cazy.class, nrow(fam.df.subset))
  fam.df.subset$CAZy_class <- class.names
  return(fam.df.subset)
}

# add parent CAZy classes to the sensitivity and precision data
fam_subset_sens.gh <- add_parent_class(fam_subset_sens, gh.names, 'GH')
fam_subset_sens.gt <- add_parent_class(fam_subset_sens, gt.names, 'GT')
fam_subset_sens.pl <- add_parent_class(fam_subset_sens, pl.names, 'PL')
fam_subset_sens.ce <- add_parent_class(fam_subset_sens, ce.names, 'CE')
fam_subset_sens.aa <- add_parent_class(fam_subset_sens, aa.names, 'AA')
fam_subset_sens.cbm <- add_parent_class(fam_subset_sens, cbm.names, 'CBM')

fam_subset_spec.gh <- add_parent_class(fam_subset_spec, gh.names, 'GH')
fam_subset_spec.gt <- add_parent_class(fam_subset_spec, gt.names, 'GT')
fam_subset_spec.pl <- add_parent_class(fam_subset_spec, pl.names, 'PL')
fam_subset_spec.ce <- add_parent_class(fam_subset_spec, ce.names, 'CE')
fam_subset_spec.aa <- add_parent_class(fam_subset_spec, aa.names, 'AA')
fam_subset_spec.cbm <- add_parent_class(fam_subset_spec, cbm.names, 'CBM')

# recombine the dataframes
fam_subset_sens_all <- rbind(fam_subset_sens.gh, fam_subset_sens.gt)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.pl)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.ce)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.aa)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.cbm)

fam_subset_spec_all <- rbind(fam_subset_spec.gh, fam_subset_spec.gt)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.pl)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.ce)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.aa)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.cbm)

# rename columns to faciltate merging
names(fam_subset_sens_all)[names(fam_subset_sens_all) == "Statistic_value"] <- "Sensitivity"
names(fam_subset_spec_all)[names(fam_subset_spec_all) == "Statistic_value"] <- "Specificity"

# drop the column called X - from the index number created by pandas in Python
fam_subset_sens_all <- fam_subset_sens_all[c("CAZy_family", "Prediction_tool", "Sensitivity", "CAZy_class")]
fam_subset_spec_all <- fam_subset_spec_all[c("CAZy_family", "Prediction_tool", "Specificity", "CAZy_class")]

# merge the dataframes
cazy_fam_spec_sense_df <- merge(
  fam_subset_sens_all,
  fam_subset_spec_all,
  by=c("Prediction_tool", "CAZy_class", "CAZy_family")
)

# set order data is presented
cazy_fam_spec_sense_df$Prediction_tool <- factor(cazy_fam_spec_sense_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) 
cazy_fam_spec_sense_df$CAZy_class <- factor(cazy_fam_spec_sense_df$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

# calculate sensitivity and specificity as a percentage
cazy_fam_spec_sense_df$Sens_percent <- cazy_fam_spec_sense_df$Sensitivity * 100
cazy_fam_spec_sense_df$Spec_percent <- cazy_fam_spec_sense_df$Specificity * 100

# log10 of specificity is calculated when plotting the chart
# define func for plotting sens % vs spec % log10
plot_fam_sens_vs_spec <- function(
  fam.class.df,
  cazy.class,
  noise.sample.size,
  noise.min.val,
  noise.max.value,
  lower.limit,
  upper.limit){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  # add noise to the data for plotting the contour
  sens_perc_data <- fam.class.df.class.subset$Sens_percent
  class.sens.data.noise <- add_noise(sens_perc_data, noise.sample.size, noise.min.val, noise.max.value)
  spec_perc_data <- fam.class.df.class.subset$Spec_percent
  class.spec.data.noise <- add_noise(spec_perc_data, noise.sample.size, noise.min.val, noise.max.value)
  
  # add the data with noise as new columns to the df
  fam.class.df.class.subset$Sens_noise <- class.sens.data.noise
  fam.class.df.class.subset$Spec_noise <- class.spec.data.noise
  
  # generate the plot
  p.fam = ggplot(fam.class.df.class.subset %>% group_by(Prediction_tool), aes(
      x=Sens_percent,
      y=Spec_percent
    )) +
    geom_density_2d_filled(alpha = 0.5, aes(x=Sens_noise, y=Spec_noise)) +
    geom_point() +
    scale_color_manual(values=colour_set) +
    theme(plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11)) +
    labs(x = "Sensitivity (%)", y = "Specificity (log10)", color="Classifer", fill="Density") +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x), labels = trans_format("log10", math_format(10^.x)), limits=c(lower.limit,upper.limit)) +
    coord_trans(y="log10") +
    facet_wrap(~ Prediction_tool)
  
  return(p.fam)
}

plot.fam.sens.jitter <- function(fam.class.df, cazy.class){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  p.fam.sens <- ggplot(fam.class.df.class.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Sensitivity, fill=Prediction_tool)) +
	  geom_boxplot(outlier.shape=NA) +
	  geom_jitter(width=0.1, height=0) +
	  scale_fill_manual(values = colour_set) +
	  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold")) +
	  xlab("Classifier") + 
	  ylab("Sensitivity") 
  
  return(p.fam.sens)
}
```

Later on in this report the sensitivity for each CAZy family is plotted against specificity, as was done with CAZy class. However, owing to extremely small different in specificity, with no tool producing a specificity less than 0.995 it is extremely difficult to separate performance by specificity, so a boxplot and scatter plot for each is plotted. Each point represents one test set, and test sets are grouped by CAZyme classifier and facet wrapped by the parent CAZy class.

```{r famsSense, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of sensitivity for each CAZy family for each CAZyme classifier."}
# set order data is presented
cazy_fam_spec_sense_df$Prediction_tool <- factor(cazy_fam_spec_sense_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) 
cazy_fam_spec_sense_df$CAZy_class <- factor(cazy_fam_spec_sense_df$CAZy_class, levels = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM'))

p.fam.sens <- ggplot(cazy_fam_spec_sense_df %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Sensitivity, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  facet_wrap(~ CAZy_class)
p.fam.sens
```

```{r saveFamSensBoxplot, echo=FALSE}
pdf(file = "famSensBoxplots.pdf", width = 11.25, height = 7)
p.fam.sens
dev.off()
```

For better resolution we can group the CAZy families by their parent CAzy classes, and compare the performances of the tools CAZy class, by CAZy class. Owing to the minimal variation in specificity scores, specificity was plotted as the percentage specificity log10.

### Glycoside Hydrolases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Glycoside Hydrolase CAZy family.

```{r ghfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycoside Hydrolases."}
p.gh.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GH', 0.8, 0.1, 0.8, 99.7, 100)
p.gh.sens.spec.fam
```

```{r saveGhSensSpec, echo=FALSE}
pdf(file = "famSpecSensGh.pdf", width = 11.25, height = 7)
p.gh.sens.spec.fam
dev.off()
```

### Glycosyltransferases

Figure \@ref(fig:gtfamrllvspc) shows the plotting of sensitivity against specificity for each Glycosyltransferases CAZy family.

```{r gtfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycosyltransferases"}
p.gt.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GT', 0.8, 0.1, 0.8, 99.8, 100)
p.gt.sens.spec.fam
```

```{r saveGtSensSpec, echo=FALSE}
pdf(file = "famSpecSensGt.pdf", width = 11.25, height = 7)
p.gt.sens.spec.fam
dev.off()
```

### Polysaccharide Lyases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Polysaccharide Lyases CAZy family.

```{r plfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Polysaccharide Lyases"}
p.pl.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'PL', 0.9, 0.1, 0.4, 99.9, 100)
p.pl.sens.spec.fam
```

```{r savePlSensSpec, echo=FALSE}
pdf(file = "famSpecSensPl.pdf", width = 11.25, height = 7)
p.pl.sens.spec.fam
dev.off()
```

### Carbohydrate Esterases

Figure \@ref(fig:cefamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Esterases CAZy family.

```{r cefamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Esterases"}
p.ce.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CE', 0.8, 0.1, 0.5, 99.7, 100)
p.ce.sens.spec.fam
```

```{r saveCeSensSpec, echo=FALSE}
pdf(file = "famSpecSensCe.pdf", width = 11.25, height = 7)
p.ce.sens.spec.fam
dev.off()
```

### Auxillary Activities

Figure \@ref(fig:afamrllvspc) shows the plotting of sensitivity against specificity for each Auxillary Activities CAZy family.

```{r aafamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Auxillary Activities"}
p.aa.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'AA', 0.8, 0.1, 0.7, 99.7, 100)
p.aa.sens.spec.fam
```

```{r saveAaSensSpec, echo=FALSE}
pdf(file = "famSpecSensAa.pdf", width = 11.25, height = 7)
p.aa.sens.spec.fam
dev.off()
```


### Carbohydate Binding Modules

Figure \@ref(fig:cbmfamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Binding Module CAZy family.

```{r cbmfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Binding Modules"}
p.cbm.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CBM', 0.8, 0.1, 1, 99.4, 100)
p.cbm.sens.spec.fam
```

```{r saveCbmSensSpec, echo=FALSE}
pdf(file = "famSpecSensCbm.pdf", width = 11.25, height = 7)
p.cbm.sens.spec.fam
dev.off()
```

## Consistently poor performing CAZy families

We then pulled out the CAZy families with a sensitivity score less than 0.75, which there were only 10.

```{r ghfamPoorFam, echo=FALSE, fig.cap="GH families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}
# pdf(file = "mlcGHpoorFams.pdf", width = 8.58, height = 4.8)
gh.poor.fam.names <- gh.poor.fams$CAZy.family

gh.poor.fam.subset <- gh.fam.subset[which(gh.fam.subset$CAZy_family %in% gh.poor.fam.names), ]

gh.poor.fam.subset$Prediction_tool <- factor(gh.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
gh.poor.fam.subset$CAZy_family <- factor(gh.poor.fam.subset$CAZy_family, levels = c("GH170", "GH166", "GH163", "GH138", "GH135", "GH123", "GH50", "GH45", "GH24", "GH0"))

poor.fam.populations <- rep(c(19436, 17656, 463, 1080, 471, 640, 112, 22, 973, 4879), 6)

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorGHSupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.gh.poor.pops <- ggplot(gh.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=poor.fam.populations, x="Family Population")) +
  geom_text(data=gh.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
# p.gh.poor.pops
# dev.off()

# tiff(file = "mlcPoorGHFams.tiff", width = 7, height = 5.5, units="in", res=700)
p.gh.poor.fam <- ggplot(gh.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=gh.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score")))
p.gh.poor.fam
# dev.off()
```

### GlycosylTransferases

....

```{r gtfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class GlycosylTransferases."}
gt.fam.subset <- fam.stat.df[which(fam.stat.df$CAZy_family %in% gt.names), ]
gt.fam.subset <- gt.fam.subset[complete.cases(gt.fam.subset), ]

gt.fam.subset$Prediction_tool <- factor(gt.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
gt.fam.subset$Specificity_percentage <- gt.fam.subset$Specificity * 100
gt.fam.subset$Recall_percentage <- gt.fam.subset$Recall * 100

# pdf(file = "mlcGTfamsSpecSens.pdf", width = 11.25, height = 7)
p.gt.fam = ggplot(gt.fam.subset, aes(x=Recall, y=Specificity_percentage, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity (log10)") +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
          labels = trans_format("log10", math_format(10^.x))) +
  coord_trans(y="log10") +
  facet_wrap(~ Prediction_tool)
p.gt.fam
# dev.off()
```

.....


```{r gtfamPoorFam, echo=FALSE, fig.cap="GT families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}

gt.poor.fam.names <- gt.poor.fams$CAZy.family

gt.poor.fam.subset <- gt.fam.subset[which(gt.fam.subset$CAZy_family %in% gt.poor.fam.names), ]

gt.poor.fam.subset$Prediction_tool <- factor(gt.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
gt.poor.fam.subset$CAZy_family <- factor(gt.poor.fam.subset$CAZy_family, levels=c("GT113", "GT111", "GT109", "GT80", "GT61", "GT60", "GT52", "GT47", "GT31", "GT29", "GT23", "GT10", "GT0"))

gt.poor.fam.populations <- rep(c(1091, 1735, 99, 196, 6385, 388, 889, 1206, 2171, 959, 839, 1365, 20258), 6)

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorGTSupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.gt.poor.pops <- ggplot(gt.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=gt.poor.fam.populations, x="Family Population")) +
  geom_text(data=gt.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
p.gt.poor.pops
# dev.off()

# tiff(file = "mlcPoorGTFams.tiff", width = 7, height = 5.5, units="in", res=700)
p.gt.poor.fam <- ggplot(gt.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=gt.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        legend.position = "bottom") +
  labs(fill = expression(paste("F", beta, "-score")))
p.gt.poor.fam
# dev.off()
```

...

### Polysaccharide Lyases

....

```{r plfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Polysaccharide Lyases."}
pl.fam.subset <- fam.stat.df[which(fam.stat.df$CAZy_family %in% pl.names), ]
pl.fam.subset <- pl.fam.subset[complete.cases(pl.fam.subset), ]

pl.fam.subset$Prediction_tool <- factor(pl.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
pl.fam.subset$Specificity_percentage <- pl.fam.subset$Specificity * 100
pl.fam.subset$Recall_percentage <- pl.fam.subset$Recall * 100

# pdf(file = "mlcPLfamsSpecSens.pdf", width = 11.25, height = 7)
p.pl.fam = ggplot(pl.fam.subset, aes(x=Recall, y=Specificity_percentage, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity (log10)") +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
          labels = trans_format("log10", math_format(10^.x))) +
  coord_trans(y="log10") +
  facet_wrap(~ Prediction_tool)
p.pl.fam
# dev.off()
```

.....


```{r plfamPoorFam, echo=FALSE, fig.cap="PL families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}
pdf(file = "mlcPLpoorFams.pdf", width = 8.58, height = 4.8)
pl.poor.fam.names <- pl.poor.fams$CAZy.family

pl.poor.fam.subset <- pl.fam.subset[which(pl.fam.subset$CAZy_family %in% pl.poor.fam.names), ]

pl.poor.fam.subset$Prediction_tool <- factor(pl.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
pl.poor.fam.subset$CAZy_family <- factor(pl.poor.fam.subset$CAZy_family, levels=c("PL38", "PL33", "PL31", "PL29", "PL0"))

pl.poor.fam.populations <- rep(c(1108, 521, 270, 91, 1967), (length(pl.poor.fam.subset$Prediction_tool)/6))

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorPLSupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.pl.poor.pops <- ggplot(pl.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=pl.poor.fam.populations, x="Family Population")) +
  geom_text(data=pl.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
# p.pl.poor.pops
# dev.off()

# tiff(file = "mlcPoorPLFams.tiff", width = 7, height = 5.5, units="in", res=700)
p.pl.poor.fam <- ggplot(pl.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=pl.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score")))
p.pl.poor.fam
# dev.off()
```

...

### Carbohydrate Esterases

....

```{r cefamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Esterases."}
ce.fam.subset <- fam.stat.df[which(fam.stat.df$CAZy_family %in% ce.names), ]
ce.fam.subset <- ce.fam.subset[complete.cases(ce.fam.subset), ]

ce.fam.subset$Prediction_tool <- factor(ce.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
ce.fam.subset$Specificity_percentage <- ce.fam.subset$Specificity * 100
ce.fam.subset$Recall_percentage <- ce.fam.subset$Recall * 100

# pdf(file = "mlcCEfamsSpecSens.pdf", width = 11.25, height = 7)
p.ce.fam = ggplot(ce.fam.subset, aes(x=Recall, y=Specificity_percentage, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity (log10)") +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
          labels = trans_format("log10", math_format(10^.x))) +
  coord_trans(y="log10") +
  facet_wrap(~ Prediction_tool)
p.ce.fam
# dev.off()
```

.....


```{r cefamPoorFam, echo=FALSE, fig.cap="CE families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}

ce.poor.fam.names <- ce.poor.fams$CAZy.family

ce.poor.fam.subset <- ce.fam.subset[which(ce.fam.subset$CAZy_family %in% ce.poor.fam.names), ]

ce.poor.fam.subset$Prediction_tool <- factor(ce.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
ce.poor.fam.subset$CAZy_family <- factor(ce.poor.fam.subset$CAZy_family, levels=c("CE18", "CE16", "CE1", "CE0"))

ce.poor.fam.populations <- rep(c(33, 182, 5102, 2688), (length(ce.poor.fam.subset$Prediction_tool)/6))

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorCESupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.ce.poor.pops <- ggplot(ce.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=ce.poor.fam.populations, x="Family Population")) +
  geom_text(data=ce.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
# p.ce.poor.pops
# dev.off()

# tiff(file = "mlcPoorCEFams.tiff", width = 7, height = 4, units="in", res=700)
p.ce.poor.fam <- ggplot(ce.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=ce.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score")))
p.ce.poor.fam
# dev.off()
```

...

### Auxiliary Activities

....

```{r aafamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Auxiliary Activities."}
aa.fam.subset <- fam.stat.df[which(fam.stat.df$CAZy_family %in% aa.names), ]
aa.fam.subset <- aa.fam.subset[complete.cases(aa.fam.subset), ]

aa.fam.subset$Prediction_tool <- factor(aa.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
aa.fam.subset$Specificity_percentage <- aa.fam.subset$Specificity * 100
aa.fam.subset$Recall_percentage <- aa.fam.subset$Recall * 100

# pdf(file = "mlcAAfamsSpecSens.pdf", width = 11.25, height = 7)
p.aa.fam = ggplot(aa.fam.subset, aes(x=Recall, y=Specificity_percentage, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity (log10)") +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
          labels = trans_format("log10", math_format(10^.x))) +
  coord_trans(y="log10") +
  facet_wrap(~ Prediction_tool)
p.aa.fam
# dev.off()
```

.....


```{r aafamPoorFam, echo=FALSE, fig.cap="AA families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}

aa.poor.fam.names <- aa.poor.fams$CAZy.family

aa.poor.fam.subset <- aa.fam.subset[which(aa.fam.subset$CAZy_family %in% aa.poor.fam.names), ]

aa.poor.fam.subset$Prediction_tool <- factor(aa.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
aa.poor.fam.subset$CAZy_family <- factor(aa.poor.fam.subset$CAZy_family, levels=c("AA14", "AA8", "AA7", "AA2", "AA0"))

aa.poor.fam.populations <- rep(c(38, 176, 702, 77), (length(aa.poor.fam.subset$Prediction_tool)/6))

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorAASupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.aa.poor.pops <- ggplot(aa.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=aa.poor.fam.populations, x="Family Population")) +
  geom_text(data=aa.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
# p.aa.poor.pops
# dev.off()

 #tiff(file = "mlcPoorAAFams.tiff", width = 7, height = 4, units="in", res=700)
p.aa.poor.fam <- ggplot(aa.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=aa.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score")))
p.aa.poor.fam
# dev.off()
```

...

### Carbohydrate-Binding Module

....

```{r cbmfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate-Binding Module."}
cbm.fam.subset <- fam.stat.df[which(fam.stat.df$CAZy_family %in% cbm.names), ]
cbm.fam.subset <- cbm.fam.subset[complete.cases(cbm.fam.subset), ]

cbm.fam.subset$Prediction_tool <- factor(cbm.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
cbm.fam.subset$Specificity_percentage <- cbm.fam.subset$Specificity * 100
cbm.fam.subset$Recall_percentage <- cbm.fam.subset$Recall * 100

# pdf(file = "mlcCBMfamsSpecSens.pdf", width = 11.25, height = 7)
p.cbm.fam = ggplot(cbm.fam.subset, aes(x=Recall, y=Specificity_percentage, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity (log10)") +
  scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x),
          labels = trans_format("log10", math_format(10^.x))) +
  coord_trans(y="log10") +
  facet_wrap(~ Prediction_tool)
p.cbm.fam
# dev.off()
```

.....


```{r cbmfamPoorFam, echo=FALSE, fig.cap="CBM families which at least 3 tools scored and Fbeta-score of less than 0.75. The nubmer in bold next to each CAZy family is the number of known CAZymes from the CAZy family included across all test sets."}

cbm.poor.fam.names <- cbm.poor.fams$CAZy.family

cbm.poor.fam.subset <- cbm.fam.subset[which(cbm.fam.subset$CAZy_family %in% cbm.poor.fam.names), ]

cbm.poor.fam.subset$Prediction_tool <- factor(cbm.poor.fam.subset$Prediction_tool, levels = c('dbCAN', 'HMMER', 'Hotpep', 'DIAMOND', 'CUPP', 'eCAMI'))
cbm.poor.fam.subset$CAZy_family <- factor(cbm.poor.fam.subset$CAZy_family, levels=c(
  "CBM87", "CBM67", "CBM61", "CBM57", "CBM56", "CBM54", "CBM51", "CBM50", "CBM47", "CBM45", "CBM44", "CBM42", "CBM41", "CBM38", "CBM36", "CBM35", "CBM32", "CBM30",
  "CBM26", "CBM25", "CBM24", "CBM22", "CBM20", "CBM19", "CBM16", "CBM14", "CBM13", "CBM12", "CBM11", "CBM10", "CBM9", "CBM6", "CBM5", "CBM4", "CBM3", "CBM2",
  "CBM1",    "CBM0"
))

cbm.poor.fam.populations <- rep(c(33, 713, 776, 946, 241, 251, 113191, 1117, 189, 26, 903, 597, 135, 4359, 10217, 38, 889, 712, 202, 1418, 2376, 160, 994, 3570, 10811, 2675, 169, 333, 627, 4491, 12516, 1195, 2046, 7150, 1671, 1394), (length(cbm.poor.fam.subset$Prediction_tool)/6))

# This table is manual cropped to add data to the plot below
# tiff(file = "mlcPoorCBMSupplement.tiff", width = 8, height = 5.5, units="in", res=700)
p.cbm.poor.pops <- ggplot(cbm.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family)) +
  xlab("Classifier") +
  ylab("CAZy family") +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  coord_cartesian(clip="off") +
  geom_text(aes(label=cbm.poor.fam.populations, x="Family Population")) +
  geom_text(data=cbm.poor.fam.subset, aes(label=Recall_sample_size, x="CAZymes"))
# p.cbm.poor.pops
# dev.off()

# tiff(file = "mlcPoorCBMFams.tiff", width = 7, height = 6, units="in", res=700)
p.cbm.poor.fam <- ggplot(cbm.poor.fam.subset %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=CAZy_family, fill=Fbeta_score)) +
  geom_tile() +
  scale_fill_viridis_c() +
  xlab("Classifier") +
  ylab("CAZy family") +
  geom_text(data=cbm.poor.fam.subset, aes(label=round(Fbeta_score, digits=3))) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        text = element_text(size=10),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score")))
p.cbm.poor.fam
# dev.off()
```


## Evaluation of multi-label CAZy family classification performance

CAZy annotates proteins in a domain-wise manner. Consequently, a single protein may be assigned to multiple CAZy families. The ability of a classifier to assign all the correct CAZy family annotations for a given protein when only evaluating the CAZy family classification performance per CAZy family, independently of all other CAZy classes.

The CAZy family multi-label classification performance is represented by the Rand Index (RI) and Adjusted Rand Index (ARI). The RI is a quantitive measure of similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. In this case the two clusters are the predicted and groud truth CAZy family annotations. The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:  
`ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)`
This produces a score between 1 and -1. A score of 1 is produced if all predicted and known CAZy family annotations are identical, 0 if completely random clustering of -1 if systematically incorrect clustering and the number of incorrect classifications of proteins is greater than would be expected from randomly annotating proteins with CAZy families.

```{r famRi, echo=FALSE}
fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

fam_ri_stats_df <- fam_classification_df %>% group_by(Prediction_tool) %>% summarise("Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index))

kable(fam_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famAriCalc, echoFALSE}
fam_ari_stats_df <- fam_classification_df %>% group_by(Prediction_tool) %>% summarise("Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index))

kable(fam_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

Multilabel classification raises when a single instance can be assinged to multiple classes. In this evaluation a single instance is a protein and the classes are CAZy families, a single CAZyme can be assigned to multiple CAZy families. This is important to take into consideration because the same approaches for statistical evaluation of binary classification provided a limited view of the performance of the classifiers when applied to multilabel classification.

Plot are violin plots overlayed by scatter plots of the Rand Index and Adjusted Rand Index for every protein in every test set, excluding true negatives.

```{r famRI, echo=FALSE, fig.cap="Violin plot of Rand Index (RI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')) # set order data is presented

# plot RI
p.ri.class = ggplot(fam_classification_df %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  scale_y_continuous(breaks = seq(0,1, by = 0.1))
p.ri.class
```

```{r classRI, echo=FALSE, fig.cap="Violin plot of Adjusted Rand Index (ARI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

p.ari.class = ggplot(fam_classification_df %>% group_by(Prediction_tool), aes(x=Prediction_tool, y=Adjusted_Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  scale_y_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))
p.ari.class
```

# Conclusions


