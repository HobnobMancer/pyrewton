---
title: "Evaluation of the CAZyme classifiers: dbCAN, CUPP and eCAMI"
author: "Emma E. M. Hobbs"
date: "2021 March"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 5
    toc_float:
      toc_collapsed: false
    number_sections: true
    css: "css/rmd_style.css"
    theme: lumen
---

```{r setup, include=FALSE}
#
# Import required libraries
#
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library('kableExtra')
library(magrittr) 
library('ggplot2')
library("DT")
library("datasets")
library("dplyr")
library("GGally")
library("ggridges")
library("rjson")
library("readr")
library(knitr)
library(tidyverse)
library("MLmetrics")
library('mclust')  # adjusted rand index
library('Metrics')  # Fbeta score
library("devtools")
library(ggpubr)
library(RColorBrewer)
library(cowplot)
library(scales)
library("MASS")
library("interp")
library('reshape2')
library(Rmisc)

#
# define global constants
#

# BG color for plots - should match .figure and .caption classes in rmd_style.css
figbg = "whitesmoke"

#
# Colour schemes for data
#

# define small colour set, with one colour per CAZyme classifier
colour_set <- c("#0a7a6d", "#d4af37", "#7844b8", "#acbf1b", "#c22176", "#3888e0")
# colour gradient from red-orange-yellow-blue-gree
colour_grad <- c(
  "#1c0a00", "#620021", "#940113",
  "#D73027", "#F38345", "#FDBA67",
  "#FEE168", "#fff966", "#BCF9FC",
  "#98D0E4", "#66A5CC", "#3D7A99",
  "#2e5a6b", "#2e663b", "#228537",
  "#1ba841", "#3fcc3f", "#6ddb1a",
  "#a5f200")
# define large colour set when many colours are needed
n <- 60
qual_col_pals = brewer.pal.info[brewer.pal.info$category == 'qual',]
col_vector = unlist(mapply(brewer.pal, qual_col_pals$maxcolors, rownames(qual_col_pals)))
qual_col = qual_col_pals[c(TRUE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE), ]
large_colour_set = unlist(mapply(brewer.pal, qual_col$maxcolors, rownames(qual_col)))

#
# Classifiers evaluated
#
# This is used to define factor levels and order the data output
classifiers = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI')

recombined_classifiers = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI', 'HMMER_DIAMOND_CUPP', 'HMMER_DIAMOND_eCAMI')
recombined_classifiers_abbrev = c('dbCAN','HMMER','DIAMOND','Hotpep','CUPP','eCAMI', 'H_D_C', 'H_D_E')

# define the colour set for using when plotting the recombined tools
rt.colour_set <- c("#0a7a6d", "#d4af37", "#7844b8", "#acbf1b", "#c22176", "#3888e0", "#e07e38", "#959c9e")
```

```{r importData, include=FALSE}
#
# Import data files
#


#
# Test set data
#

# import file with test set coverage of genomes and CAZomes
test_set_cov_df = read.table("data/cazome_coverage_2021_04_06-11_11_04.txt", sep="\t",header=TRUE)

#
# binary classification
#

# import "binary_classification_evaluation_<date>.csv"
# Written in long form, with the columns: Statistic_parameter, Genomic_assembly, Prediction_tool, Statistic_value
binary_stat_df <- read.csv("data/binary_classification_evaluation_2021_04_08.csv")

# import "binary_bootstrap_accuracy_evaluation_<date>.csv"
# Contains the output from the bootstrap resampling of the binary classification of CAZymes/non-CAZymes
bootstrap_results_df <- read.csv("data/binary_bootstrap_accuracy_evaluation_2021_04_08.csv")

# import binary false positive and negative predictions
binary_fp_predictions = read.csv("data/binary_false_positive_classifications_2021_11_09.csv")
binary_fn_predictions = read.csv("data/binary_false_negative_classifications_2021_11_09.csv")

#
# class classification
#

# import "class_predicted_classifications_<date>.csv"
# Contains the columns: Genomic_accession, Protein_accession, Prediciton_tool, one column per CAZy class, Rand_index and Adjusted_rand_index
# Contains the clasifications (0/1) for each prediction tool for every protein across all test sets
# Includes the calculated rand index and adjusted rand index
class_ri_ari_raw_df <- read.csv("data/class_predicted_classifications_2021_04_08.csv")

# import "class_stats_per_test_set_<date>.csv"
# Contains the calculated performance statistics when evaluating performance of CAZy class prediction per test set.
# Written in long form with the columns: Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
cazy_class_df <- read.csv("data/class_stats_per_test_set_2021_11_09.csv")

#
# family classification
#

# import "family_long_form_stats_df_<date>.csv"
# Contains the calculated performance satistics when evaluating performance of CAZy family prediction per test set.
# Written in long form with the columns: CAZy_family, Prediction_tool, Statistical_parameter, Statistic_value
cazy_family_long_df <- read.csv("data/family_long_form_stats_df_2021_11_09.csv")

# import "family_predicted_classifications_<date>.csv
# Contains the CAZy family epredictions for every protein for every CAZy family, and includes the 
# Rand indx and Adjusted Rand Index for every protein
fam_classification_df <- read.csv ("data/family_predicted_classifications_2021_11_09.csv")

# Load in data on CAZy family populations and sample sizes
fam.populations <- fromJSON(file = "data/CAZy_fam_populations_2021_12_07.json")
fam.sample.sizes <- fromJSON(file = "data/CAZy_fam_testset_freq_2021_12_07.json")

#
# Load in data for taxonomy group comparison
#

# import binary_classification_tax_comparison_<date>.csv
# Contains the same data as binary_stat_df with the additional 'Tax_group' column
binary_tax_df <- read.csv("data/binary_classification_tax_comparison_2021_12_01.csv")

# import "class_stats_per_test_set_tax_comparison_<tax_group>_<date>.csv"
# Contains the calculated performance statistics when evaluating performance of CAZy class prediction per test set.
# Written in long form with the columns: Genomic_accession, Prediction_tool, CAZy_class, Statistic_parameter, Statistic_value
# load in each dataframe for each tax group
bact.cazy_class_df <- read.csv("data/class_stats_per_test_set_tax_comparison_Bacteria_time_stamp.csv")
euk.cazy_class_df <- read.csv("data/class_stats_per_test_set_tax_comparison_Eukaryote_time_stamp.csv")

# import class_classification_tax_comparison_<date>.csv
# df containing CAZy class RI and ARI
class_tax_ri_ari_df <- read.csv("data/class_classification_tax_comparison_2021_12_07.csv")

# import "family_classification_tax_comparison_<date>.csv
# Contains the CAZy family predictions for every protein for every CAZy family, and includes the 
# Rand index and Adjusted Rand Index for every protein
fam_classification_df_tax <- read.csv ("data/family_classification_tax_comparison_2021_12_01.csv")

#
# Load in data for recombining tools
#

recombined_tools_binary_df <- read.csv('data/binary_classification_recombined_tools_2022_01_13.csv')

recombined_tools_class_df_pred <- read.csv('data/class_per_test_set.csv')  # class evaluation per test set
recombined_tools_class_ri_ari <- read.csv('data/class_rt_pred.csv')

recombined_tools_fam_df <- read.csv('data/fam_stats_df_long.csv')
#
#
#
```

<div id="summary">
The pCAZyme classifiers dbCAN, CUPP and eCAMI were independently evaluated against a high quality benchmark test set. The performances were evaluated upon the CAZyme/non-CAZyme differentiation and multilabel classification of CAZy family annotations. This notebook contains that statistical evaluation of the CAZyme classifiers.  
Results summary:    
- dbCAN and DIAMOND showed the strongest performances in CAZyme/non-CAZyme differentiation
- dbCAN was the strongest performing tool across all categories, Hotpep (a tool invoked by dbCAN) was the weakest
- The performances between CUPP and eCAMI were similar, although CUPP should a marginally better performance when comparing the multilabel classification of CAZy family annotations
- The performance of dbCAN may be optimised by substituting Hotpep with CUPP and/or eCAMI
</div>


# Introduction

The CAZyme classifiers dbCAN ([Zhange et al. 2018](https://academic.oup.com/nar/article/46/W1/W95/4996582)), CUPP ([Barrett and Lange, 2019](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6489277/)) and eCAMI ([Xu et al. 2019](https://academic.oup.com/bioinformatics/article/36/7/2068/5651014)) use different methods to predict if a protein is a CAZyme or non-CAZyme, and predict the CAZy family annotations for predicted CAZymes. These classifiers have not been independently evaluated against a high quality benchmark test set.

This notebook layouts out the independent evaluation of dbCAN, CUPP and eCAMI against a high quality benchmark test set. The tools were evaluated upon their ability to differentiate between CAZymes and non-CAZymes, and their performance of predicting the CAZy family annotations of predicted CAZymes.

dbCAN incorporates the three protein function classifiers HMMER ([Potter et al. 2018](https://pubmed.ncbi.nlm.nih.gov/29905871/)), Hotpep ([Busk et al. 2017](https://pubmed.ncbi.nlm.nih.gov/28403817/)), and DIAMOND ([Buchfink et al. 2015](https://www.nature.com/articles/nmeth.3176)). In order to comprehensively evaluate the preformance of dbCAN, the predictions from HMMER, Hotpep and DIAMOND were evaluated independently of each other, and the consensus prediction (a prediction which at least two of the tools agree upon) was defined as the dbCAN result.


# Test sets

A single test set of 100 CAZymes and 100 non-CAZymes with the highest sequence similarity (rated by bit-score ratio) was created per genomic assembly selected to be included in the benchmark test set. Choosing the 100 non-CAZymes with the highest sequence similarity was devised to increase the probability of causing confusion, to gather a better idea of the expected performance when using the classifiers. An equal number of CAZymes to non-CAZymes was selected to prevent over representation of one population over the other.

For inclusion of a genomic assembly for the creation of a test set, the assembly had to meet of all the following criteria:

- Contains at least 100 CAZymes
- Contains at least 100 non-CAZymes
- Has an 'Assembly level' of 'Complete Genome' in the NCBI Assembly database
- Protein records are still present in NCBI
- Not listed as an 'Anomalous assembly' in the NCBI Assembly database

The genomic assemblies were also chosen from a range of taxonomies to provide as informative image of the performance of the classifiers over a range of datasets that users may wish to analyse.

Table \@ref(tab:gassembly) contains the genomic assemblies used to create the test sets for the evaluation. In total 81 assemblies were chose, 1 from an Oomycete species (more Oomycete species with greater than 100 CAZymes in CAZy could not be found), 25 fungal Ascomycetes species were selected, 13 Yeast, 2 Eukaryote microorganisms, 20 Gram positive bacteria, and 20 Gram negative bacteria, and figure \@ref(fig:cazomeCovHisto) presents the distribution of CAZome coverage all 70 genomes.

```{r gassembly, echo=FALSE}
# print table of genomic assemblies and source organisms
```

```{r cazomeCovStats, echo=FALSE}
mean_genome_cov = mean(test_set_cov_df$Genome_CAZome_percentage)
print("Mean percentage of genome incorporated in the CAZome across all test sets:")
print(mean_genome_cov)
sd_genome_cov = sd(test_set_cov_df$Genome_CAZome_percentage)
print("Standard deviation of the percentage of genome incorporated in the CAZome across all test sets:")
print(sd_genome_cov)

mean_cazome_cov = mean(test_set_cov_df$CAZome_coverage_percentage)
print("Mean percentage of CAZomes incorporated in the test set across all genomes:")
print(mean_cazome_cov)
sd_cazome_cov = sd(test_set_cov_df$CAZome_coverage_percentage)
print("Standard deviation of the percentage of CAZome incorporated in the test set across all genomes:")
print(sd_cazome_cov)
```

```{r cazomeCovHisto, echo=FALSE, results='asis' ,fig.cap="Histogram of CAZome coverage of the test sets for each respective source genomic assembly, overlayed by a box and whisker plot of the percentage of the CAZome incorproated in the test set."}
p.cazome.c <- ggplot(test_set_cov_df, aes(x=CAZome_coverage_percentage)) +
  geom_histogram(fill="#138d91", alpha=0.7, color="#0d4a4d") +
  geom_boxplot(
    outlier.shape=NA,
    width = 0.5,
    data=test_set_cov_df,
    aes(x=CAZome_coverage_percentage, y=5),
    alpha=0.5) +
  labs(x="CAZome coverage (%)", y="Number of test sets")
p.cazome.c
```

```{r saveCov, include=FALSE}
pdf(file = "cazomeCovHis.pdf", width = 8.58, height = 5.5)
p.cazome.c
dev.off()
```

```{r saveCovPNG, include=FALSE}
png(file = "cazomeCovHis.png", units="in", width = 8.58, height = 5.5, res=900)
p.cazome.c
dev.off()
```

# CAZyme/non-CAZyme classification

The assignment of CAZy family annotations by a CAZyme classifier identifies the protein as a CAZyme. If no CAZy family annotations are assigned to a protein by a CAZyme classifier, the tool identified the protein as a non-CAZyme. This notebook evaluates the performance of the CAZyme classifiers dbCAN (which incorporates HMMER, Hotpep and DIAMOND), CUPP and eCAMI for this binary CAZyme/non-CAZyme classification.

## Summary statistics

For every classifier-test set pair, the specificity, sensitivity, prevision, F1-score and accuracy were calculated.

The mean of each statistical parameter was calculated for each classifier across all tests, to represent the overall performance of each CAZyme classifier.

These results are presented in table \@ref(tab:sumstats).

```{r funcCalcBinSumStats, include=FALSE}
get_binary_summary_stats <- function(stat_df){
  # Calculate statistics
  subset_spec <- stat_df[which(stat_df$Statistic_parameter == "Specificity"), ]
  binary_specificity <- subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_sens <- stat_df[which(stat_df$Statistic_parameter == "Sensitivity"), ]
  binary_sensitivity <- subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_prec <- stat_df[which(stat_df$Statistic_parameter == "Precision"), ]
  binary_precision <- subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_f1 <- stat_df[which(stat_df$Statistic_parameter == "FBeta-score"), ]
  binary_f1_score <- subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
  )
  
  subset_acc <- stat_df[which(stat_df$Statistic_parameter == "Accuracy"), ]
  binary_accuracy <- subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
  )
  
  # combine data and build a single dataframe
  binary_summary_df <- merge(binary_specificity, binary_sensitivity)
  binary_summary_df <- merge(binary_summary_df, binary_precision)
  binary_summary_df <- merge(binary_summary_df, binary_f1_score)
  binary_summary_df <- merge(binary_summary_df, binary_accuracy)
  
  return(
    list(
      binary_summary_df,
      subset_spec,
      subset_sens,
      subset_prec,
      subset_f1,
      subset_acc
    )
  )
}
```


```{r sumstats, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. Data collected is the mean of call calculated statistical parameters across all test sets, plus and minus the standard devliation. All figures are rounded to 4 decimal places. A confidence interval (CI) of 95% is presented. Spec, specificity; Sens, sensitivity; Prec, precision; Acc, accuracy"}

binary_dfs <- get_binary_summary_stats(binary_stat_df)
binary_summary_df <- binary_dfs[[1]]
subset_spec <- binary_dfs[[2]]
subset_sens <- binary_dfs[[3]]
subset_prec <- binary_dfs[[4]]
subset_f1 <- binary_dfs[[5]]
subset_acc <- binary_dfs[[6]]

# define factors
binary_summary_df$Prediction_tool <- factor(binary_summary_df$Prediction_tool, levels = classifiers) # set order data is presented

names(binary_summary_df)[names(binary_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

# binary_summary_df <- binary_summary_df[c(2,5,6,3,1,4), ]
row.names(binary_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  binary_summary_df,
  caption="Overall performance of CAZyme classifiers differentiation between CAZymes and non-CAZymes",
  align='c',
  digits = 4
) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

Owing to the skewing of the data towards 1, the 95% confidence interval (CI) was calculated and plotted as error bars around the mean CI and illustrated in figure \@ref(fig:binaryCI).

```{r binaryCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. The mean plus and minus the 95% confidence interval."}
cal_tool_ci <- function(stat_subset){
  # calculate lower, upper and mean 95% CI
  UpperCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
  MeanCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
  LowerCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])

  ci_df <- merge(MeanCI, UpperCI)
  ci_df <- merge(ci_df, LowerCI)
  
  return(ci_df)
}

bin.spec.ci <- cal_tool_ci(subset_spec)
bin.spec.ci$Statistic_parameter <- rep('Specificity', nrow(bin.spec.ci))
bin.sens.ci <- cal_tool_ci(subset_sens)
bin.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(bin.sens.ci))
bin.prec.ci <- cal_tool_ci(subset_prec)
bin.prec.ci$Statistic_parameter <- rep('Precision', nrow(bin.prec.ci))
bin.f1.ci <- cal_tool_ci(subset_f1)
bin.f1.ci$Statistic_parameter <- rep('F1-score', nrow(bin.f1.ci))
bin.acc.ci <- cal_tool_ci(subset_acc)
bin.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(bin.acc.ci))

ci_df <- rbind(bin.spec.ci, bin.sens.ci)
ci_df <- rbind(ci_df, bin.prec.ci)
ci_df <- rbind(ci_df, bin.f1.ci)
ci_df <- rbind(ci_df, bin.acc.ci)

ci_df$Prediction_tool <- factor(ci_df$Prediction_tool, levels = classifiers)
ci_df$Statistic_parameter <- factor(ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

binary.CI = ggplot(ci_df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
binary.CI
```


## Specificity

Specificity is the proportion of known negatives (known non-CAZymes) which are correctly classified as negatives (non-CAZymes).

Figure \@ref(fig:spec) is a graphical representation of the results calculated in table \@ref(tab:sumstats).

```{r spec, echo=FALSE, fig.cap="One-dimensional scatter plot of specificity scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_spec$Prediction_tool <- factor(subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.binary.spec = ggplot(subset_spec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.binary.spec
```

```{r savebinarySpec, include=FALSE}
pdf(file = "binarySpec.pdf", width = 8.58, height = 5.5)
p.binary.spec
dev.off()
```

## Sensitivity

Sensitivity (also known as recall) is the proportion of known positives (CAZymes) that are correctly identified as positives (CAZymes).

Figure \@ref(fig:recallbc) graphically represents of the results calculated in table \@ref(tab:sumstats).


```{r recallbc, echo=FALSE, fig.cap="One-dimensional scatter plot of recall (sensitivity) scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_sens$Prediction_tool <- factor(subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.binary.sens = ggplot(subset_sens %>% dplyr::group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.binary.sens
```

```{r saveBinSens, include=FALSE}
pdf(file = "binarySensitivity.pdf", width = 8.58, height = 5.5)
p.binary.sens
dev.off()
```

## Precision

Precision is the proportion of positive predictions by the classifiers that are correct.

In this case, precision represents the fraction of CAZyme predictions by the classifiers that are correct, specifically the proportion of predicted CAZymes that are known CAZymes.

Figure \@ref(fig:precbc) is a visual representation of the results calculated in table \@ref(tab:sumstats).

```{r precbc, echo=FALSE, fig.cap="One-dimensional scatter plot of precision scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

subset_prec$Prediction_tool <- factor(subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.binary.prec = ggplot(subset_prec %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.binary.prec
```

```{r saveBinPrec, include=FALSE}
pdf(file = "binaryPrec.pdf", width = 8.58, height = 5.5)
p.binary.prec
dev.off()
```

## F1-score

The F1-score is a harmonic (or weighted) average of recall and precision and provides an idea of the overall performance of the tool, 0 being the lowest and 1 being the best performance. Figure \@ref(fig:f1bc) shows the F1-score from each test set, for each classifier.

```{r f1bc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}

subset_f1$Prediction_tool <- factor(subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.binary.f1 = ggplot(subset_f1 %>% dplyr::group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.binary.f1
```


```{r saveBinF1, include=FALSE}
png(file = "binaryF1.png", units = 'in', width = 8.58, height = 5.5, res=900)
p.binary.f1
dev.off()
```

## Accuracy

Accuarcy (calculated using (TP + TN) / (TP + TN + FP + FN) ) provides an idea of the overall performance of the classifiers as a measure of the degree to which their CAZyme/non-CAZyme predictions conforms to the correct result. Figure \@ref(fig:accbc) is a plot of respective data from table \@ref(tab:sumstats).


```{r accbc, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}
subset_acc$Prediction_tool <- factor(subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.binary.acc = ggplot(subset_acc %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy") 
p.binary.acc
```

```{r saveBinAcc, include=FALSE}
pdf(file = "binaryAcc.pdf", width = 8.58, height = 5.5)
p.binary.acc
dev.off()
```

```{r binaryCombo, include=FALSE}
axis_values = seq(0.3,1, by = 0.05)
mp.binary.spec = ggplot(subset_spec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold"),
        axis.text.x=element_text(angle=-90)) +
  xlab("Classifier") +
  ylab("Specificity") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.sens = ggplot(subset_sens %>% dplyr::group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Sensitivity") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.prec = ggplot(subset_prec %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Precision") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.f1 = ggplot(subset_f1 %>% dplyr::group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("F1-score") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
mp.binary.acc = ggplot(subset_acc %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Accuracy") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))
plot_grid(mp.binary.spec, mp.binary.sens, mp.binary.prec, mp.binary.f1, mp.binary.acc, labes='AUTO', ncol=3)
```

```{r saveBinMuliPlot, include=FALSE}
png("binary_multiplot.png", units='in', width=8, height=6,res=900)
plot_grid(mp.binary.spec, mp.binary.sens, mp.binary.prec, mp.binary.f1, mp.binary.acc, labes='AUTO', ncol=3)
dev.off()
```

## Expected Range of Accruacy

The statistics evaluated above provide an idea of the general performance of the tools, but they do not provide an idea of the expect range of performance. Specifically, the data does not provide a clear image of the best and worse performance a user can expect when using these tools.

To compare the expected typical range in accuracies for each classifier, 6 test sets (identified by the source genomic assemblies) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times each, and for each bootstrap sample the accuracy calculated. The accuracies of the bootstrap samples for each classifier were plotted on stacked histograms, shown in figure \@ref(fig:bsacc).

```{r bsacc, echo=FALSE, fig.cap="Stacked histograms of bootstrap sample accuracies of CAZyme classifiers' differentiation between CAZymes and non-CAZymes. 6 test sets (identified by their source genomic assembly) were selected at random. The CAZyme/non-CAZyme predictions for each classifier, for each test set, were bootstrap resampled 100 times. The accuracy of each of the 600 bootstrap samples per test set were plotted as a stacked histogram."}
bootstrap_results_df$Prediction_tool <- factor(bootstrap_results_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot generated to see the number of items in each bin for the figure above, and additional boxplots to represent the distribution of the data
p.bs.annotated = ggplot(bootstrap_results_df %>% dplyr::group_by(Genomic_accession), aes(x=accuracy)) +
  geom_histogram(alpha=0.7, color="black", binwidth = 0.01, aes(fill=Genomic_accession), position = ) +
  geom_boxplot(outlier.shape=NA, width = 10, data=bootstrap_results_df, aes(x=accuracy, y=100), alpha=0.5) +
  scale_fill_brewer(palette="Set1") +
  # stat_bin(aes(y=..count.. + 5, label=..count..), geom="text", binwidth = 0.01, size=3.5, angle=90) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold"),
        strip.text = element_text(size=12),
        legend.position="bottom") +
  labs(x = "Accuracy", y = "Number of bootstrap samples", fill="Test set key") +
  # scale_y_continuous(breaks = seq(0,125, by = 25)) +
  # scale_x_continuous(breaks = seq(0.75,1, by = 0.05)) +
  facet_wrap(~ Prediction_tool, ncol=2) 
  # remove the comment if you want to see the histogram with the total number per column printed
p.bs.annotated
```

```{r saveBootstrap, include=FALSE}
pdf(file = "binaryBootstrapAccAnnotated.pdf", width = 8.25, height = 11)
p.bs.annotated
dev.off()

svg(file = "binaryBootstrapAccAnnotated.svg", width = 8.25, height = 11)
p.bs.annotated
dev.off()
```

## Conclusions on the Binary CAZyme/non-CAZyme Prediction Performance

Overall, all tools showed a low probability of producing false positives (missclassifying a non-CAZyme as a CAZyme), and few of the positive predictions are false positives. Therefore, we can be confident in that the CAZyme predictions made by each of these tools are most likely correct. However, all the classifiers demonstrated a consistent behaviour to not identify all CAZymes within a CAZome. Therefore, we can be confident in the CAZyme predictions, but should not presume all non-CAZyme predictions are correct; these classifiers are unlikely to identify the complete CAZome although a near-complete CAZome will be accurately identified.

dbCAN consistently demonstrated the strongest performance in all categories, inferring that eCAMI and CUPP are not suitable replacements of the CAZyme classifier. Hotpep consistently demonstrated the weakest performance, and is incorporated within dbCAN. Therefore, substituting eCAMI and/or CUPP into dbCAN instead of Hotpep may further improve the performance of dbCAN. The new k-mer based methods, eCAMI and CUPP demonstrated similar performances. CUPP showed a more consistent performance and eCAMI demonstrating a greater range in performance although its mean performance was fractionally greater than that of CUPP. However, more bootstrap calculated accuracy scores feel within the range of 0.9-1.0 for CUPP than eCAMI. This infers that a CUPP may typically provide a better performance than eCAMI, although eCAMI does have the potential on some occasions to out perform CUPP, depending on the test set.


# CAZy Class classification

CAZy groups CAZymes into CAZy families by sequence similarity, and CAZy families are grouped into one of 6 functional classes. The CAZyme classifiers predict the CAZy family annotations of predicted CAZymes, but it is of interest to see what the level of performance of the classiferis is at the CAZy class level. Specifically, a classifier may struggle to predict the correct CAZy class for a CAZyme but consistently predict the correct CAZy class. Therefore, the aim of this part of the evaluation is to evaluate the performance of the classifiers to predict the correct CAZy class of predict CAZymes.

## General trends in CAZy class classification performance

Below is a table summary all statistical parameters calculated in order to evaluate the performance of the CAZy class classification for each prediction tool across all CAZy classes.

```{r cazyClassStasTable, echo=FALSE}
# Calculate statistics
class_subset_spec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
class_subset_spec  <- class_subset_spec[complete.cases(class_subset_spec), ]
class_specificity <- class_subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
)

class_subset_sens <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
class_subset_sens  <- class_subset_sens[complete.cases(class_subset_sens), ]
class_sensitivity <- class_subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
)

class_subset_prec <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Precision"), ]
class_subset_prec  <- class_subset_prec[complete.cases(class_subset_prec), ]
class_precision <- class_subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
)

class_subset_f1 <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
class_subset_f1  <- class_subset_f1[complete.cases(class_subset_f1), ]
class_f1_score <- class_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
)

class_subset_acc <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Accuracy"), ]
class_subset_acc  <- class_subset_acc[complete.cases(class_subset_acc), ]
class_accuracy <- class_subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
)

# combine data and build a single dataframe
class_summary_df <- merge(class_specificity, class_sensitivity)
class_summary_df <- merge(class_summary_df, class_precision)
class_summary_df <- merge(class_summary_df, class_f1_score)
class_summary_df <- merge(class_summary_df, class_accuracy)

# define factors
class_summary_df$Prediction_tool <- factor(class_summary_df$Prediction_tool, levels = classifiers) # set order data is presented

names(class_summary_df)[names(class_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

class_summary_df <- class_summary_df[c(2,5,6,3,1,4), ]
row.names(class_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  class_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy class classification performance",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)

```

```{r classCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CAZy class classification, plotting the mean plus and minus the 95% confidence interval."}
cal_tool_ci <- function(stat_subset){
  # calculate lower, upper and mean 95% CI
  UpperCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
  MeanCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
  LowerCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])

  ci_df <- merge(MeanCI, UpperCI)
  ci_df <- merge(ci_df, LowerCI)
  
  return(ci_df)
}

class.spec.ci <- cal_tool_ci(class_subset_spec)
class.spec.ci$Statistic_parameter <- rep('Specificity', nrow(class.spec.ci))
class.sens.ci <- cal_tool_ci(class_subset_sens)
class.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(class.sens.ci))
class.prec.ci <- cal_tool_ci(class_subset_prec)
class.prec.ci$Statistic_parameter <- rep('Precision', nrow(class.prec.ci))
class.f1.ci <- cal_tool_ci(class_subset_f1)
class.f1.ci$Statistic_parameter <- rep('F1-score', nrow(class.f1.ci))
class.acc.ci <- cal_tool_ci(class_subset_acc)
class.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(class.acc.ci))

class.ci_df <- rbind(class.spec.ci, class.sens.ci)
class.ci_df <- rbind(class.ci_df, class.prec.ci)
class.ci_df <- rbind(class.ci_df, class.f1.ci)
class.ci_df <- rbind(class.ci_df, class.acc.ci)

class.ci_df$Prediction_tool <- factor(class.ci_df$Prediction_tool, levels = classifiers)
class.ci_df$Statistic_parameter <- factor(class.ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

p.class.CI = ggplot(class.ci_df %>% dplyr::group_by(Statistic_parameter),
                   aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
p.class.CI
```

Below a proportional area plot representing the F-beta score for each CAZyme classifier for each test set is generated. each square is sized proportional to the relative sample size. Every class was not included in every sample, resulting in different sample sizes between CAZy classes, the same between classifiers.

```{r statsFbeta, echo=FALSE, fig.cap="Proportional area plot of CAZy class classification performance. Performance is represented by the F1-score. Plots are proptional to the number of test sets, after excluding true negative classifications."}
cazy_class_df$Prediction_tool <- factor(cazy_class_df$Prediction_tool, levels = classifiers) # set order data is presented

# subset Fbeta-scores
class_fbeta_subset <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]

# Not every CAZy class is present in every test set, where it was not present a value of NA was given.
# these rows need to be dropped so that the proportional area plot represents the number of test sets containing
# that CAZy class
class_fbeta_subset  <- class_fbeta_subset[complete.cases(class_fbeta_subset), ]

# classify the Fbeta-scores into bins
val.class <- vector()

for(i in 1:nrow(class_fbeta_subset)) {
  if(class_fbeta_subset[i, 6] == 1){val.class <- append(val.class, '[1.00]')} 
  else if (class_fbeta_subset[i, 6] == 0){val.class <- append(val.class, '[0.00]')}
  else if (class_fbeta_subset[i, 6] < 1 && class_fbeta_subset[i, 6] >= 0.95){val.class <- append(val.class, '(0.95, 1.00]')}
  else if (class_fbeta_subset[i, 6] < 0.95 && class_fbeta_subset[i, 6] >= 0.9){val.class <- append(val.class, '(0.90, 0.95]')}
  else if (class_fbeta_subset[i, 6] < 0.90 && class_fbeta_subset[i, 6] >= 0.85){val.class <- append(val.class, '(0.85, 0.90]')}
  else if (class_fbeta_subset[i, 6] < 0.85 && class_fbeta_subset[i, 6] >= 0.80){val.class <- append(val.class, '(0.80, 0.85]')}
  else if (class_fbeta_subset[i, 6] < 0.80 && class_fbeta_subset[i, 6] >= 0.75){val.class <- append(val.class, '(0.75, 0.80]')}
  else if (class_fbeta_subset[i, 6] < 0.75 && class_fbeta_subset[i, 6] >= 0.70){val.class <- append(val.class, '(0.70, 0.75]')}
  else if (class_fbeta_subset[i, 6] < 0.70 && class_fbeta_subset[i, 6] >= 0.65){val.class <- append(val.class, '(0.65, 0.70]')}
  else if (class_fbeta_subset[i, 6] < 0.65 && class_fbeta_subset[i, 6] >= 0.60){val.class <- append(val.class, '(0.60, 0.65]')}
  else if (class_fbeta_subset[i, 6] < 0.60 && class_fbeta_subset[i, 6] >= 0.55){val.class <- append(val.class, '(0.55, 0.60]')}
  else if (class_fbeta_subset[i, 6] < 0.55 && class_fbeta_subset[i, 6] >= 0.50){val.class <- append(val.class, '(0.50, 0.55]')}
  else if (class_fbeta_subset[i, 6] < 0.50 && class_fbeta_subset[i, 6] >= 0.45){val.class <- append(val.class, '(0.45, 0.50]')}
  else if (class_fbeta_subset[i, 6] < 0.45 && class_fbeta_subset[i, 6] >= 0.40){val.class <- append(val.class, '(0.40, 0.45]')}
  else if (class_fbeta_subset[i, 6] < 0.40 && class_fbeta_subset[i, 6] >= 0.35){val.class <- append(val.class, '(0.35, 0.40]')}
  else if (class_fbeta_subset[i, 6] < 0.35 && class_fbeta_subset[i, 6] >= 0.30){val.class <- append(val.class, '(0.30, 0.35]')}
  else if (class_fbeta_subset[i, 6] < 0.30 && class_fbeta_subset[i, 6] >= 0.25){val.class <- append(val.class, '(0.25, 0.30]')}
  else if (class_fbeta_subset[i, 6] < 0.25 && class_fbeta_subset[i, 6] >= 0.20){val.class <- append(val.class, '(0.20, 0.25]')}
  else if (class_fbeta_subset[i, 6] < 0.20 && class_fbeta_subset[i, 6] >= 0.15){val.class <- append(val.class, '(0.15, 0.20]')}
  else if (class_fbeta_subset[i, 6] < 0.15 && class_fbeta_subset[i, 6] >= 0.10){val.class <- append(val.class, '(0.10, 0.15]')}
  else if (class_fbeta_subset[i, 6] < 0.10 && class_fbeta_subset[i, 6] >= 0.05){val.class <- append(val.class, '(0.05, 0.10]')}
  else if (class_fbeta_subset[i, 6] < 0.05 && class_fbeta_subset[i, 6] >= 0){val.class <- append(val.class, '(0.00, 0.05]')}
  else {val.class <- append(val.class, '< 0')}
}
class_fbeta_subset$val.class <- val.class

# set order data is presented
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = classifiers) 
class_fbeta_subset$CAZy_class <- factor(class_fbeta_subset$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
class_fbeta_subset$val.class <- factor(class_fbeta_subset$val.class, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.classF1 = ggally_count(class_fbeta_subset, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.class)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.classF1
```

```{r saveCazyClassFbeta, include=FALSE}
svg(file = "mlcClassF1.svg",  width = 8.25, height = 11)
p.classF1
dev.off()
```

A dataframe of the number of test sets containing each CAZy class is generated (table \@ref(tab:calcCazyClassSamples)).

```{r calcCazyClassSamples, echo=FALSE}
class_fbeta_subset$Prediction_tool <- factor(class_fbeta_subset$Prediction_tool, levels = classifiers) # set order data is presented

calc_class_sample_size <- function(class_df, tool) {
  tool_data <- c(tool)
  tool_df = class_df[which(class_df$Prediction_tool == tool), ]
  cazy_classes = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
  for (cazy_class in cazy_classes) {
    sample_size <- sum(tool_df$CAZy_class == cazy_class)
    tool_data = c(tool_data, sample_size)
  }
  tool_data_m = matrix(tool_data,nrow=1)
  return(tool_data_m)
}

dbcan_row = calc_class_sample_size(class_fbeta_subset, 'dbCAN')
hmmer_row = calc_class_sample_size(class_fbeta_subset, 'HMMER')
diamond_row = calc_class_sample_size(class_fbeta_subset, 'DIAMOND')
hotpep_row = calc_class_sample_size(class_fbeta_subset, 'Hotpep')
cupp_row = calc_class_sample_size(class_fbeta_subset, 'CUPP')
ecami_row = calc_class_sample_size(class_fbeta_subset, 'eCAMI')

cazy_class_sample_size <- rbind(dbcan_row, hmmer_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, diamond_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, hotpep_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, cupp_row)
cazy_class_sample_size <- rbind(cazy_class_sample_size, ecami_row)
cazy_class_sample_size_df <- as.data.frame(cazy_class_sample_size)
names(cazy_class_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
cazy_class_sample_size_df
```

The sensitivity of each CAZyme classifier can be plotted against the specificity for each CAZy class, however plotting all CAZy classes in a single plot produces a cramped plot, unless very few test sets were used.

```{r cazyClassSpecVsSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CAZy class members per CAZyme classier"}
# subset specificity scores
cazy_class_specificity_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Specificity"), ]
# rename columns
names(cazy_class_specificity_df)[names(cazy_class_specificity_df) == "Statistic_value"] <- "Specificity"
cazy_class_specificity_df <- cazy_class_specificity_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Specificity")]

# subset sensitivity scores
cazy_class_sens_df <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Sensitivity"), ]
names(cazy_class_sens_df)[names(cazy_class_sens_df) == "Statistic_value"] <- "Sensitivity"
cazy_class_sens_df <- cazy_class_sens_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Sensitivity")]

# merge dataframes
cazy_class_spec_sense_df <- merge(cazy_class_specificity_df, cazy_class_sens_df, by=c("Genomic_accession", "Prediction_tool", "CAZy_class"))

# specificity was not predicted for every CAZy class in eveyr test set, this is the result of the CAZy class not being present in the test set and the classifier not predicting the presence of the CAZy class in the test set
# In these cases specificity is NA
# NA values are removed
cazy_class_spec_sense_df  <- cazy_class_spec_sense_df[complete.cases(cazy_class_spec_sense_df), ]

# one plot per class with be made using facet_wrap by Prediciton_tool.
p.class.spec.sense = ggplot(cazy_class_spec_sense_df %>% dplyr::group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=colour_set) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity", color="Classifer") +
  facet_wrap(~CAZy_class)
p.class.spec.sense
```

```{r saveclassAllClassesSpecSens, include=FALSE}
pdf(file = "classAllSpecVsSens.pdf", width = 11.25, height = 7)
p.class.spec.sense
dev.off()

png(file = "classAllSpecVsSens.png", width = 11.25, height = 7, res=900, units='in')
p.class.spec.sense
dev.off()
```

## Performance per CAZy class

Below the prediction sensitivity is plotted against the specificity for each classifier, and a separate plot is generated for each CAZy class.

The scatter plots of sensitivity against specificity overlay a coloured contour to highlight the distribution of the points. When too many points have the same value a contour cannot be generated. In order to plot a contour noise is added to the data. The original data is used to plot the scatter plot and the data with added noise is used to plot the contour.

The percentage of the data points which need noise to be added to them in order to generate a contour varies from data set to data set. To change the percentage of the data points with noise added to them, change the third value of call to the function `plot.class.sens.vs.spec()`, which is used to generate the plots. The third value is the percentage of data points to add noise to, written in decimal form.

```{r addNoise, echo=FALSE}
# When plotting scatter plots overlaying a coloured contour, contours cannot be ploted if too many data points have the same value
# To sort this add noise to the data

add_noise <- function(data, corrupt_percent, min.val, max.val){
  corrupt <- rbinom(length(data), 1, corrupt_percent)    # choose an average of <corrupt_percent>% to corrupt at random
  corrupt <- as.logical(corrupt)
  noise <- rnorm(sum(corrupt), min.val, max.val) # generate the noise to add
  data[corrupt] <- data[corrupt] - noise      # about 10% of x has been corrupted
  return(data)
}

```

```{r buildClassSensVsSpecPlot, echo=FALSE}
plot.class.sens.vs.spec <- function(
  class.data.df,
  cazy_class,
  corruption_size,
  min.val,
  max.val,
  plot_colours,
  factors.list
){
  # subset rows containing data for the given CAZy class
  class.subset.df <- class.data.df[which(class.data.df$CAZy_class == cazy_class), ]
  
  # set the order prediction tools are plotted in the plot
  class.subset.df$Prediction_tool <- factor(class.subset.df$Prediction_tool, levels = factors.list)

  # subset and add noise to the data to enable plotting contours, contours cannot be plotted if too many data points have the same value
  class.sens.data <- class.subset.df$Sensitivity
  class.sens.data.noise <- add_noise(class.sens.data, corruption_size, min.val, max.val)

  class.spec.data <- class.subset.df$Specificity
  class.spec.data.noise <- add_noise(class.spec.data, corruption_size, min.val, max.val)
  
  # add the data with noise as new columns to the df
  class.subset.df$Sens_noise <- class.sens.data.noise
  class.subset.df$Spec_noise <- class.spec.data.noise
  
  # generate the plot
  p.class.sens.spec = ggplot(class.subset.df %>% dplyr::group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity)) +
    geom_density_2d_filled(alpha = 0.5, aes(x=Sens_noise, y=Spec_noise)) +
    geom_point() +
    scale_color_manual(values=plot_colours) +
    theme(plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11)) +
    labs(x = "Sensitivity", y = "Specificity", color="Classifer", fill="Density") +
    xlim(0.4, 1.0) +
    ylim(0.9, 1.0) +
    facet_wrap(~ Prediction_tool)
  p.class.sens.spec
}

get.class.summary.table.and.plots <- function(class.data.df, cazy_class){
  # retrieve rows for current CAZy class
  stat_df <- class.data.df[which(class.data.df$CAZy_class == cazy_class), ]
  # drop rows with null values
  stat_df  <- stat_df[complete.cases(stat_df), ]
  # Calculate statistics
  subset_spec <- stat_df[which(stat_df$Statistic_parameter == "Specificity"), ]
  binary_specificity <- subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec CI Lower"=CI(Statistic_value)[3],
    "Spec CI Upper"=CI(Statistic_value)[1]
  )
  
  subset_sens <- stat_df[which(stat_df$Statistic_parameter == "Sensitivity"), ]
  binary_sensitivity <- subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens CI Lower"=CI(Statistic_value)[3],
    "Sens CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_prec <- stat_df[which(stat_df$Statistic_parameter == "Precision"), ]
  binary_precision <- subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec CI Lower"=CI(Statistic_value)[3],
    "Prec CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_f1 <- stat_df[which(stat_df$Statistic_parameter == "Fbeta_score"), ]
  binary_f1_score <- subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score CI Lower"=CI(Statistic_value)[3],
    "F1-score CI Upper"=CI(Statistic_value)[1]
    )
  
  subset_acc <- stat_df[which(stat_df$Statistic_parameter == "Accuracy"), ]
  binary_accuracy <- subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc CI Lower"=CI(Statistic_value)[3],
    "Acc CI Upper"=CI(Statistic_value)[1]
    )
  
  # combine data and build a single dataframe
  binary_summary_df <- merge(binary_specificity, binary_sensitivity)
  binary_summary_df <- merge(binary_summary_df, binary_precision)
  binary_summary_df <- merge(binary_summary_df, binary_f1_score)
  binary_summary_df <- merge(binary_summary_df, binary_accuracy)
  
  return(
    list(
      binary_summary_df,
      subset_spec,
      subset_sens,
      subset_prec,
      subset_f1,
      subset_acc
    )
  )
}

plot.cazy.class.ci <- function(df){
  # extract CI data and combine into a single df
  means <- gh_binary_summary_df$`Spec Mean`
  means <- append(means, df$`Sens Mean`)
  means <- append(means, df$`Prec Mean`)
  means <- append(means, df$`F1-score Mean`)
  means <- append(means, df$`Acc Mean`)
  
  lower.ci <- df$`Spec CI Lower`
  lower.ci <- append(lower.ci, df$`Sens CI Lower`)
  lower.ci <- append(lower.ci, df$`Prec CI Lower`)
  lower.ci <- append(lower.ci, df$`F1-score CI Lower`)
  lower.ci <- append(lower.ci, df$`Acc CI Lower`)
  
  upper.ci <- df$`Spec CI Upper`
  upper.ci <- append(upper.ci, df$`Sens CI Upper`)
  upper.ci <- append(upper.ci, df$`Prec CI Upper`)
  upper.ci <- append(upper.ci, df$`F1-score CI Upper`)
  upper.ci <- append(upper.ci, df$`Acc CI Upper`)
  
  tools <- df$Prediction_tool
  tool.names <- c()
  for (t in tools){
    tool.names <- append(tool.names, t)
  }
  tool <- tool.names
  tool <- append(tool, tool.names)
  tool <- append(tool, tool.names)
  tool <- append(tool, tool.names)
  tool <- append(tool, tool.names)
  
  statistical_parameter <- rep('Specificity', 6)
  statistical_parameter <- append(statistical_parameter, rep('Sensitivity', 6))
  statistical_parameter <- append(statistical_parameter, rep('Precision', 6))
  statistical_parameter <- append(statistical_parameter, rep('F1-score', 6))
  statistical_parameter <- append(statistical_parameter, rep('Accuracy', 6))
  
  ci.df <- data.frame(means, lower.ci, upper.ci, tool, statistical_parameter)

  colnames(ci.df) <- c('Mean', 'LowerCI', 'UpperCI', 'Prediction_tool', 'Statistic_parameter')
  
  ci.df$Prediction_tool <- factor(ci.df$Prediction_tool, levels = classifiers) # set order data is presented
  
  # generate plot, facet wrapped by statistcal parameter
  p.class.pl <- ggplot(ci.df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
  
  return(p.class.pl)
}
```

### GH class classification

```{r classGhSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting GH CAZy class members per CAZyme classier, overlaying a density map."}
p.class.gh.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GH', 0.3, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.gh.spec.sens
```

```{r saveclassGhSpecSens, include=FALSE}
pdf(file = "classGhSpecVsSens", width = 11.25, height = 7)
p.class.gh.spec.sens
dev.off()
```

```{r ghSummaryTableClass, echo=FALSE}
gh_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'GH')
gh_binary_summary_df <- gh_class_dfs[[1]]
gh_subset_spec <- gh_class_dfs[[2]]
gh_subset_sens <- gh_class_dfs[[3]]
gh_subset_prec <- gh_class_dfs[[4]]
gh_subset_f1 <- gh_class_dfs[[5]]
gh_subset_acc <- gh_class_dfs[[6]]

kable(gh_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of GH class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r ghClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of GH class classification, plotting the mean plus and minus the 95% confidence interval."}
p.gh.class.ci <- plot.cazy.class.ci(gh_binary_summary_df)
p.gh.class.ci
```

```{r ghClassCIsave, include=FALSE}
pdf(file = "classGhCI", width = 11.25, height = 7)
p.gh.class.ci
dev.off()
```


```{r ghClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of GH class members, overlaying a box plot"}
gh_subset_spec$Prediction_tool <- factor(gh_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.class.spec = ggplot(gh_subset_spec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.gh.class.spec
```

```{r ghClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of GH class members, overlaying a box plot"}
gh_subset_sens$Prediction_tool <- factor(gh_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.class.sens = ggplot(gh_subset_sens %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.gh.class.sens
```

```{r ghClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of GH class members, overlaying a box plot"}
gh_subset_prec$Prediction_tool <- factor(gh_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.class.prec = ggplot(gh_subset_prec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.gh.class.prec
```

```{r ghClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of GH class members, overlaying a box plot"}
gh_subset_f1$Prediction_tool <- factor(gh_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.class.f1 = ggplot(gh_subset_f1 %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.gh.class.f1
```


```{r ghClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of GH class members, overlaying a box plot"}
gh_subset_acc$Prediction_tool <- factor(gh_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.class.acc = ggplot(gh_subset_acc %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.gh.class.acc
```

### GT class classification

```{r classGtSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting GT CAZy class members per CAZyme classier, overlaying a density map."}
p.class.gt.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GT', 0.5, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.gt.spec.sens
```

```{r saveclassGtSpecSens, include=FALSE}
pdf(file = "classGtSpecVsSens", width = 11.25, height = 7)
p.class.gt.spec.sens
dev.off()
```

```{r gtSummaryTableClass, echo=FALSE}
gt_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'GT')
gt_binary_summary_df <- gt_class_dfs[[1]]
gt_subset_spec <- gt_class_dfs[[2]]
gt_subset_sens <- gt_class_dfs[[3]]
gt_subset_prec <- gt_class_dfs[[4]]
gt_subset_f1 <- gt_class_dfs[[5]]
gt_subset_acc <- gt_class_dfs[[6]]

kable(gt_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of GT class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r gtClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of GT class classification, plotting the mean plus and minus the 95% confidence interval."}
p.gt.class.ci <- plot.cazy.class.ci(gt_binary_summary_df)
p.gt.class.ci
```

```{r gtClassCIsave, include=FALSE}
pdf(file = "classGtCI", width = 11.25, height = 7)
p.gt.class.ci
dev.off()
```

```{r gtClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of GT class members, overlaying a box plot"}
gt_subset_spec$Prediction_tool <- factor(gt_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.class.spec = ggplot(gt_subset_spec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heigtt=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.gt.class.spec
```

```{r gtClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of GT class members, overlaying a box plot"}
gt_subset_sens$Prediction_tool <- factor(gt_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.class.sens = ggplot(gt_subset_sens %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heigtt=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.gt.class.sens
```

```{r gtClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of GT class members, overlaying a box plot"}
gt_subset_prec$Prediction_tool <- factor(gt_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.class.prec = ggplot(gt_subset_prec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heigtt=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Preision")
p.gt.class.prec
```

```{r gtClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of GT class members, overlaying a box plot"}
gt_subset_f1$Prediction_tool <- factor(gt_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.class.f1 = ggplot(gt_subset_f1 %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heigtt=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.gt.class.f1
```

```{r gtClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of GT class members, overlaying a box plot"}
gt_subset_acc$Prediction_tool <- factor(gt_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.class.acc = ggplot(gt_subset_acc %>% dplyr::group_by(Prediction_tool),
                        aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heigtt=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.gt.class.acc
```

### PL class classification

```{r classPlSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting PL CAZy class members per CAZyme classier, overlaying a density map."}
p.class.pl.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'PL', 0.5, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.pl.spec.sens
```

```{r saveclassPlSpecSens, include=FALSE}
pdf(file = "classPlSpecVsSens", width = 11.25, height = 7)
p.class.pl.spec.sens
dev.off()
```

```{r plSummaryTableClass, echo=FALSE}
pl_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'PL')
pl_binary_summary_df <- pl_class_dfs[[1]]
pl_subset_spec <- pl_class_dfs[[2]]
pl_subset_sens <- pl_class_dfs[[3]]
pl_subset_prec <- pl_class_dfs[[4]]
pl_subset_f1 <- pl_class_dfs[[5]]
pl_subset_acc <- pl_class_dfs[[6]]

kable(pl_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of PL class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r plClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of PL class classification, plotting the mean plus and minus the 95% confidence interval."}
p.pl.class.ci <- plot.cazy.class.ci(pl_binary_summary_df)
p.pl.class.ci
```

```{r plClassCIsave, include=FALSE}
pdf(file = "classPlCI", width = 11.25, height = 7)
p.pl.class.ci
dev.off()
```

```{r plClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of PL class members, overlaying a box plot"}
pl_subset_spec$Prediction_tool <- factor(pl_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.class.spec = ggplot(pl_subset_spec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.pl.class.spec
```

```{r plClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of PL class members, overlaying a box plot"}
pl_subset_sens$Prediction_tool <- factor(pl_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.class.sens = ggplot(pl_subset_sens %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivty")
p.pl.class.sens
```

```{r plClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of PL class members, overlaying a box plot"}
pl_subset_prec$Prediction_tool <- factor(pl_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.class.prec = ggplot(pl_subset_prec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.pl.class.prec
```

```{r plClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of PL class members, overlaying a box plot"}
pl_subset_f1$Prediction_tool <- factor(pl_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.class.f1 = ggplot(pl_subset_f1 %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.pl.class.f1
```

```{r plClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of PL class members, overlaying a box plot"}
pl_subset_acc$Prediction_tool <- factor(pl_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.class.acc = ggplot(pl_subset_acc %>% dplyr::group_by(Prediction_tool),
                        aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.pl.class.acc
```

### CE class classification

```{r classCeSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CE CAZy class members per CAZyme classier, overlaying a density map."}
p.class.ce.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CE', 0.4, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.ce.spec.sens
```

```{r saveclassCeSpecSens, include=FALSE}
pdf(file = "classCeSpecVsSens", width = 11.25, height = 7)
p.class.ce.spec.sens
dev.off()
```

```{r ceSummaryTableClass, echo=FALSE}
ce_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'CE')
ce_binary_summary_df <- ce_class_dfs[[1]]
ce_subset_spec <- ce_class_dfs[[2]]
ce_subset_sens <- ce_class_dfs[[3]]
ce_subset_prec <- ce_class_dfs[[4]]
ce_subset_f1 <- ce_class_dfs[[5]]
ce_subset_acc <- ce_class_dfs[[6]]

kable(ce_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of CE class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r ceClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CE class classification, plotting the mean plus and minus the 95% confidence interval."}
p.ce.class.ci <- plot.cazy.class.ci(ce_binary_summary_df)
p.ce.class.ci
```

```{r ceClassCIsave, include=FALSE}
pdf(file = "classCeCI", width = 11.25, height = 7)
p.ce.class.ci
dev.off()
```

```{r ceClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of CE class members, overlaying a box plot"}
ce_subset_spec$Prediction_tool <- factor(ce_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.class.spec = ggplot(ce_subset_spec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heicet=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.ce.class.spec
```

```{r ceClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of CE class members, overlaying a box plot"}
ce_subset_sens$Prediction_tool <- factor(ce_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.class.sens = ggplot(ce_subset_sens %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heicet=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.ce.class.sens
```

```{r ceClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of CE class members, overlaying a box plot"}
ce_subset_prec$Prediction_tool <- factor(ce_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.class.prec = ggplot(ce_subset_prec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heicet=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.ce.class.prec
```

```{r ceClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of CE class members, overlaying a box plot"}
ce_subset_f1$Prediction_tool <- factor(ce_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.class.f1 = ggplot(ce_subset_f1 %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heicet=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.ce.class.f1
```


```{r ceClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of CE class members, overlaying a box plot"}
ce_subset_acc$Prediction_tool <- factor(ce_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.class.acc = ggplot(ce_subset_acc %>% dplyr::group_by(Prediction_tool),
                        aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heicet=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.ce.class.acc
```

### AA class classification

```{r classAaSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting AA CAZy class members per CAZyme classier, overlaying a density map."}
p.class.aa.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'AA', 0.3, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.aa.spec.sens
```

```{r saveclassAaSpecSens, include=FALSE}
pdf(file = "classAaSpecVsSens", width = 11.25, height = 7)
p.class.aa.spec.sens
dev.off()
```

```{r aaSummaryTableClass, echo=FALSE}
aa_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'AA')
aa_binary_summary_df <- aa_class_dfs[[1]]
aa_subset_spec <- aa_class_dfs[[2]]
aa_subset_sens <- aa_class_dfs[[3]]
aa_subset_prec <- aa_class_dfs[[4]]
aa_subset_f1 <- aa_class_dfs[[5]]
aa_subset_acc <- aa_class_dfs[[6]]

kable(aa_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of AA class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r aaClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of AA class classification, plotting the mean plus and minus the 95% confidence interval."}
p.aa.class.ci <- plot.cazy.class.ci(aa_binary_summary_df)
p.aa.class.ci
```

```{r aaClassCIsave, include=FALSE}
pdf(file = "classAaCI", width = 11.25, height = 7)
p.aa.class.ci
dev.off()
```

```{r aaClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of AA class members, overlaying a box plot"}
aa_subset_spec$Prediction_tool <- factor(aa_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.class.spec = ggplot(aa_subset_spec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heiaat=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.aa.class.spec
```

```{r aaClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of AA class members, overlaying a box plot"}
aa_subset_sens$Prediction_tool <- factor(aa_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.class.sens = ggplot(aa_subset_sens %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heiaat=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.aa.class.sens
```

```{r aaClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of AA class members, overlaying a box plot"}
aa_subset_prec$Prediction_tool <- factor(aa_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.class.prec = ggplot(aa_subset_prec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heiaat=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.aa.class.prec
```

```{r aaClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of AA class members, overlaying a box plot"}
aa_subset_f1$Prediction_tool <- factor(aa_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.class.f1 = ggplot(aa_subset_f1 %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heiaat=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.aa.class.f1
```


```{r aaClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of AA class members, overlaying a box plot"}
aa_subset_acc$Prediction_tool <- factor(aa_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.class.acc = ggplot(aa_subset_acc %>% dplyr::group_by(Prediction_tool),
                        aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, heiaat=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.aa.class.acc
```

### CBM class classification

```{r classCbmSpecSens, echo=FALSE, fig.cap="Scatter plot of sensitivity against specificity for predicting CBM CAZy class members per CAZyme classier, overlaying a density map."}
p.class.cbm.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CBM', 0.3, 0.001, 0.01,
  colour_set,
  classifiers
)
p.class.cbm.spec.sens
```

```{r saveclassCbmSpecSens, include=FALSE}
pdf(file = "classCbmSpecVsSens", width = 11.25, height = 7)
p.class.cbm.spec.sens
dev.off()
```

```{r cbmSummaryTableClass, echo=FALSE}
cbm_class_dfs <- get.class.summary.table.and.plots(cazy_class_df, 'CBM')
cbm_binary_summary_df <- cbm_class_dfs[[1]]
cbm_subset_spec <- cbm_class_dfs[[2]]
cbm_subset_sens <- cbm_class_dfs[[3]]
cbm_subset_prec <- cbm_class_dfs[[4]]
cbm_subset_f1 <- cbm_class_dfs[[5]]
cbm_subset_acc <- cbm_class_dfs[[6]]

kable(cbm_binary_summary_df, caption="Overall performance of CAZyme classifiers classification of CBM class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r cbmClassCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of CBM class classification, plotting the mean plus and minus the 95% confidence interval."}
p.cbm.class.ci <- plot.cazy.class.ci(cbm_binary_summary_df)
p.cbm.class.ci
```

```{r cbmClassCIsave, include=FALSE}
pdf(file = "classCbmCI", width = 11.25, height = 7)
p.cbm.class.ci
dev.off()
```

```{r cbmClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of CBM class members, overlaying a box plot"}
cbm_subset_spec$Prediction_tool <- factor(cbm_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.class.spec = ggplot(cbm_subset_spec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.cbm.class.spec
```

```{r cbmClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of CBM class members, overlaying a box plot"}
cbm_subset_sens$Prediction_tool <- factor(cbm_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.class.sens = ggplot(cbm_subset_sens %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.cbm.class.sens
```

```{r cbmClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of CBM class members, overlaying a box plot"}
cbm_subset_prec$Prediction_tool <- factor(cbm_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.class.prec = ggplot(cbm_subset_prec %>% dplyr::group_by(Prediction_tool),
                         aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.cbm.class.prec
```

```{r cbmClassClassificationF1score, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of CBM class members, overlaying a box plot"}
cbm_subset_f1$Prediction_tool <- factor(cbm_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.class.f1 = ggplot(cbm_subset_f1 %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.cbm.class.f1
```


```{r cbmClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of CBM class members, overlaying a box plot"}
cbm_subset_acc$Prediction_tool <- factor(cbm_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.class.acc = ggplot(cbm_subset_acc %>% dplyr::group_by(Prediction_tool),
                        aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy")
p.cbm.class.acc
```

## Rand Index and Adjusted Rand Index of CAZy Class Prediction

A single CAZyme can be included in multiple CAZy classes leading to the multilabel classification of CAZymes. To address this and evaluate the multilabel classification of CAZy classes the Rand Index (RI) and Adjusted Rand Index (ARI) were calculated.

The RI is the measure of accuracy across all potential classifications of a protein. The RI ranges from 0 (no correct annotations) to 1 (all annotations correct). The ARI is the RI adjusted for chance, where 0 is the equivalent to assigning the CAZy class annotations randomly, -1 where the annotations are systematically handed out incorrectly and 1 where the annotations are all correct.

```{r riCalc, include=FALSE}
class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = classifiers) # set order data is presented

class_ri_stats_df <- class_ri_ari_raw_df %>% 
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index)[3],
    "Mean"=mean(Adjusted_Rand_index),
    "Upper CI"=CI(Adjusted_Rand_index)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index)
  )

kable(class_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ariCalc, echo=FALSE}
class_ari_stats_df <- class_ri_ari_raw_df %>% 
  dplyr::group_by(Prediction_tool) %>%
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Lower CI"=CI(Adjusted_Rand_index)[3],
    "Mean"=mean(Adjusted_Rand_index),
    "Upper CI"=CI(Adjusted_Rand_index)[1],
    "Standard Deviation"=sd(Adjusted_Rand_index)
  )

kable(class_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy class annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

Plot are violin plots underlying scatter plots, presenting the RI and ARI for every protein across all test sets.

```{r classRI, echo=FALSE, fig.cap="Violin plot of Rand Index (RI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

class_ri_ari_raw_df$Prediction_tool <- factor(class_ri_ari_raw_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ri.class = ggplot(class_ri_ari_raw_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  scale_y_continuous(breaks = seq(0,1, by = 0.1))
p.ri.class
```

```{r classRIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Rand Index (RI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

class_ri_stats_df$Prediction_tool <- factor(class_ri_stats_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ri.class = ggplot(class_ri_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index")
p.ri.class
```

```{r classARI, echo=FALSE, fig.cap="Violin plot of Adjusted Rand Index (ARI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

p.ari.class = ggplot(class_ri_ari_raw_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Adjusted_Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  scale_y_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))
p.ari.class
```

```{r classARIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Adjusted Rand Index (ARI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

class_ari_stats_df$Prediction_tool <- factor(class_ari_stats_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ari.class = ggplot(class_ari_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.ari.class
```

# CAZy family classification

The following section evaluates the performance of the CAZyme classifiers to predict CAZy family classifications.

## General trends in performance across all CAZy families

Below is a table summarising the overall CAZy family classifications for each test set across all CAZy families.

```{r cazyFamStasTable, echo=FALSE}
# Calculate statistics
fam_subset_spec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Specificity"), ]
fam_subset_spec  <- fam_subset_spec[complete.cases(fam_subset_spec), ]
fam_specificity <- fam_subset_spec %>% 
  dplyr::group_by(Prediction_tool) %>% 
  dplyr::summarise(
    "Spec Mean"=mean(Statistic_value),
    "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_sens <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Sensitivity"), ]
fam_subset_sens  <- fam_subset_sens[complete.cases(fam_subset_sens), ]
fam_sensitivity <- fam_subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Sens Mean"=mean(Statistic_value),
    "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_prec <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Precision"), ]
fam_subset_prec  <- fam_subset_prec[complete.cases(fam_subset_prec), ]
fam_precision <- fam_subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Prec Mean"=mean(Statistic_value),
    "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_f1 <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Fbeta_score"), ]
fam_subset_f1  <- fam_subset_f1[complete.cases(fam_subset_f1), ]
fam_f1_score <- fam_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "F1-score Mean"=mean(Statistic_value),
    "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
  )

fam_subset_acc <- cazy_family_long_df[which(cazy_family_long_df$Statistical_parameter == "Accuracy"), ]
fam_subset_acc  <- fam_subset_acc[complete.cases(fam_subset_acc), ]
fam_accuracy <- fam_subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    "Acc Mean"=mean(Statistic_value),
    "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
  )

# combine data and build a single dataframe
fam_summary_df <- merge(fam_specificity, fam_sensitivity)
fam_summary_df <- merge(fam_summary_df, fam_precision)
fam_summary_df <- merge(fam_summary_df, fam_f1_score)
fam_summary_df <- merge(fam_summary_df, fam_accuracy)

# define factors
fam_summary_df$Prediction_tool <- factor(fam_summary_df$Prediction_tool, levels = classifiers) # set order data is presented

names(fam_summary_df)[names(fam_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

fam_summary_df <- fam_summary_df[c(2,5,6,3,1,4), ]
row.names(fam_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  fam_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy family classification performance across all CAZy families",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)

```

The evaluate the overall performance of each classifier, for each CAZy family, the F1-score was calculated for every family. Families were grouped by their parent CAZy class and the distribution of the F1-scores is shown in figure \@ref(fig:fbetaclass).

```{r fbetaclass, echo=FALSE, fig.cap="Proportaional area plot of F1-score per CAZy distribution per CAZy class."}
# F-beta scores already subset previously, and are stored in the var fam_subset_f1

# classify the Fbeta-scores into bins
val.fam <- vector()

for(i in 1:nrow(fam_subset_f1)){
  if(fam_subset_f1[i, 5] == 1){val.fam <- append(val.fam, '[1.00]')} 
  else if (fam_subset_f1[i, 5] == 0){val.fam <- append(val.fam, '[0.00]')}
  else if (fam_subset_f1[i, 5] < 1 && fam_subset_f1[i, 5] >= 0.95){val.fam <- append(val.fam, '(0.95, 1.00]')}
  else if (fam_subset_f1[i, 5] < 0.95 && fam_subset_f1[i, 5] >= 0.9){val.fam <- append(val.fam, '(0.90, 0.95]')}
  else if (fam_subset_f1[i, 5] < 0.90 && fam_subset_f1[i, 5] >= 0.85){val.fam <- append(val.fam, '(0.85, 0.90]')}
  else if (fam_subset_f1[i, 5] < 0.85 && fam_subset_f1[i, 5] >= 0.80){val.fam <- append(val.fam, '(0.80, 0.85]')}
  else if (fam_subset_f1[i, 5] < 0.80 && fam_subset_f1[i, 5] >= 0.75){val.fam <- append(val.fam, '(0.75, 0.80]')}
  else if (fam_subset_f1[i, 5] < 0.75 && fam_subset_f1[i, 5] >= 0.70){val.fam <- append(val.fam, '(0.70, 0.75]')}
  else if (fam_subset_f1[i, 5] < 0.70 && fam_subset_f1[i, 5] >= 0.65){val.fam <- append(val.fam, '(0.65, 0.70]')}
  else if (fam_subset_f1[i, 5] < 0.65 && fam_subset_f1[i, 5] >= 0.60){val.fam <- append(val.fam, '(0.60, 0.65]')}
  else if (fam_subset_f1[i, 5] < 0.60 && fam_subset_f1[i, 5] >= 0.55){val.fam <- append(val.fam, '(0.55, 0.60]')}
  else if (fam_subset_f1[i, 5] < 0.55 && fam_subset_f1[i, 5] >= 0.50){val.fam <- append(val.fam, '(0.50, 0.55]')}
  else if (fam_subset_f1[i, 5] < 0.50 && fam_subset_f1[i, 5] >= 0.45){val.fam <- append(val.fam, '(0.45, 0.50]')}
  else if (fam_subset_f1[i, 5] < 0.45 && fam_subset_f1[i, 5] >= 0.40){val.fam <- append(val.fam, '(0.40, 0.45]')}
  else if (fam_subset_f1[i, 5] < 0.40 && fam_subset_f1[i, 5] >= 0.35){val.fam <- append(val.fam, '(0.35, 0.40]')}
  else if (fam_subset_f1[i, 5] < 0.35 && fam_subset_f1[i, 5] >= 0.30){val.fam <- append(val.fam, '(0.30, 0.35]')}
  else if (fam_subset_f1[i, 5] < 0.30 && fam_subset_f1[i, 5] >= 0.25){val.fam <- append(val.fam, '(0.25, 0.30]')}
  else if (fam_subset_f1[i, 5] < 0.25 && fam_subset_f1[i, 5] >= 0.20){val.fam <- append(val.fam, '(0.20, 0.25]')}
  else if (fam_subset_f1[i, 5] < 0.20 && fam_subset_f1[i, 5] >= 0.15){val.fam <- append(val.fam, '(0.15, 0.20]')}
  else if (fam_subset_f1[i, 5] < 0.15 && fam_subset_f1[i, 5] >= 0.10){val.fam <- append(val.fam, '(0.10, 0.15]')}
  else if (fam_subset_f1[i, 5] < 0.10 && fam_subset_f1[i, 5] >= 0.05){val.fam <- append(val.fam, '(0.05, 0.10]')}
  else if (fam_subset_f1[i, 5] < 0.05 && fam_subset_f1[i, 5] >= 0){val.fam <- append(val.fam, '(0.00, 0.05]')}
  else {val.fam <- append(val.fam, '< 0')}
}
# add the bins to the df
fam_subset_f1$val.fam <- val.fam

# add the parent CAZy class to the df for facet wrapping

# retrieve all the family names
dbcan.fam.subset <- cazy_family_long_df[which(cazy_family_long_df$Prediction_tool == "dbCAN"), ]
tn.dbcan.fam.subset <- dbcan.fam.subset[which(dbcan.fam.subset$Statistical_parameter == "TNs"), ]

# separate names into one vector per CAZy class
gh.names = tn.dbcan.fam.subset[1:172,]$CAZy_family
gt.names = tn.dbcan.fam.subset[173:287,]$CAZy_family
pl.names = tn.dbcan.fam.subset[288:329,]$CAZy_family
ce.names = tn.dbcan.fam.subset[330:348,]$CAZy_family
aa.names = tn.dbcan.fam.subset[349:365,]$CAZy_family
cbm.names = tn.dbcan.fam.subset[366:454,]$CAZy_family

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class
  
pl.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class
  
ce.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class
  
aa.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class
  
cbm.subset <- fam_subset_f1[which(fam_subset_f1$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
fam_fbeta_df <- rbind(gh.subset, gt.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, pl.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, ce.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, aa.subset)
fam_fbeta_df <- rbind(fam_fbeta_df, cbm.subset)

# set order data is presented
fam_fbeta_df$Prediction_tool <- factor(fam_fbeta_df$Prediction_tool, levels = classifiers) 
fam_fbeta_df$CAZy_class <- factor(fam_fbeta_df$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
fam_fbeta_df$val.fam <- factor(fam_fbeta_df$val.fam, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.famF1 = ggally_count(fam_fbeta_df, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.fam)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.famF1
```

```{r saveFambeta, include=FALSE}
svg(file = "mlcFamF1.svg",  width = 8.25, height = 11)
p.famF1
dev.off()
```

\@ref(fig:fbetaclass)
Below is a table displaying the number of test sets in which each CAZy class was present, and were used to draw the proporitonal areas for each class in figure\@ref(fig:fbetaclass).

```{r calcFamFbetaSampleSize, echo=FALSE}
fam_dbcan_row = calc_class_sample_size(fam_fbeta_df, 'dbCAN')
fam_hmmer_row = calc_class_sample_size(fam_fbeta_df, 'HMMER')
fam_diamond_row = calc_class_sample_size(fam_fbeta_df, 'DIAMOND')
fam_hotpep_row = calc_class_sample_size(fam_fbeta_df, 'Hotpep')
fam_cupp_row = calc_class_sample_size(fam_fbeta_df, 'CUPP')
fam_ecami_row = calc_class_sample_size(fam_fbeta_df, 'eCAMI')

cazy_class_fam_sample_size_df <- rbind(fam_dbcan_row, fam_hmmer_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_diamond_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_hotpep_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_cupp_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_ecami_row)
cazy_class_fam_sample_size_df <- as.data.frame(cazy_class_fam_sample_size_df)
names(cazy_class_fam_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')

kable(
  cazy_class_fam_sample_size_df,
  caption="The number of CAZy families included in the evaluation of CAZy family classification for each CAZyme classifier",
  align='c',
  digits = 4) %>% kable_styling(full_width = F)
```


## Performance per CAZy family

To evaluate the performance of predicting each CAZy family independent of all other CAZy families, the sensitivity and precision for each CAZy family, for each CAZyme classifier was calculated and plotted against each other (Fig.\@ref(fig:famrllvspc)). Whereas sensitivity was plotted against sensitivity for CAZy classes, owing to the extremely small variation in specificity scores, sensitivity was plotted as a percentage against log10 of the specificity percentage.

```{r prepPerformancePerFam, include=FALSE}
# func for adding CAZy class column for a given CAZy class
add_parent_class <- function(fam.df, fam.names, cazy.class){
  fam.df.subset <- fam.df[which(fam.df$CAZy_family %in% fam.names), ]
  class.names <- rep(cazy.class, nrow(fam.df.subset))
  fam.df.subset$CAZy_class <- class.names
  
  return(fam.df.subset)
}

# add parent CAZy classes
fam_subset_spec.gh <- add_parent_class(fam_subset_spec, gh.names, 'GH')
fam_subset_spec.gt <- add_parent_class(fam_subset_spec, gt.names, 'GT')
fam_subset_spec.pl <- add_parent_class(fam_subset_spec, pl.names, 'PL')
fam_subset_spec.ce <- add_parent_class(fam_subset_spec, ce.names, 'CE')
fam_subset_spec.aa <- add_parent_class(fam_subset_spec, aa.names, 'AA')
fam_subset_spec.cbm <- add_parent_class(fam_subset_spec, cbm.names, 'CBM')

fam_subset_sens.gh <- add_parent_class(fam_subset_sens, gh.names, 'GH')
fam_subset_sens.gt <- add_parent_class(fam_subset_sens, gt.names, 'GT')
fam_subset_sens.pl <- add_parent_class(fam_subset_sens, pl.names, 'PL')
fam_subset_sens.ce <- add_parent_class(fam_subset_sens, ce.names, 'CE')
fam_subset_sens.aa <- add_parent_class(fam_subset_sens, aa.names, 'AA')
fam_subset_sens.cbm <- add_parent_class(fam_subset_sens, cbm.names, 'CBM')

fam_subset_prec.gh <- add_parent_class(fam_subset_prec, gh.names, 'GH')
fam_subset_prec.gt <- add_parent_class(fam_subset_prec, gt.names, 'GT')
fam_subset_prec.pl <- add_parent_class(fam_subset_prec, pl.names, 'PL')
fam_subset_prec.ce <- add_parent_class(fam_subset_prec, ce.names, 'CE')
fam_subset_prec.aa <- add_parent_class(fam_subset_prec, aa.names, 'AA')
fam_subset_prec.cbm <- add_parent_class(fam_subset_prec, cbm.names, 'CBM')

fam_subset_f1.gh <- add_parent_class(fam_subset_f1, gh.names, 'GH')
fam_subset_f1.gt <- add_parent_class(fam_subset_f1, gt.names, 'GT')
fam_subset_f1.pl <- add_parent_class(fam_subset_f1, pl.names, 'PL')
fam_subset_f1.ce <- add_parent_class(fam_subset_f1, ce.names, 'CE')
fam_subset_f1.aa <- add_parent_class(fam_subset_f1, aa.names, 'AA')
fam_subset_f1.cbm <- add_parent_class(fam_subset_f1, cbm.names, 'CBM')

fam_subset_acc.gh <- add_parent_class(fam_subset_acc, gh.names, 'GH')
fam_subset_acc.gt <- add_parent_class(fam_subset_acc, gt.names, 'GT')
fam_subset_acc.pl <- add_parent_class(fam_subset_acc, pl.names, 'PL')
fam_subset_acc.ce <- add_parent_class(fam_subset_acc, ce.names, 'CE')
fam_subset_acc.aa <- add_parent_class(fam_subset_acc, aa.names, 'AA')
fam_subset_acc.cbm <- add_parent_class(fam_subset_acc, cbm.names, 'CBM')

# recombine the dataframes
fam_subset_spec_all <- rbind(fam_subset_spec.gh, fam_subset_spec.gt)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.pl)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.ce)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.aa)
fam_subset_spec_all <- rbind(fam_subset_spec_all, fam_subset_spec.cbm)

fam_subset_sens_all <- rbind(fam_subset_sens.gh, fam_subset_sens.gt)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.pl)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.ce)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.aa)
fam_subset_sens_all <- rbind(fam_subset_sens_all, fam_subset_sens.cbm)

fam_subset_prec_all <- rbind(fam_subset_prec.gh, fam_subset_prec.gt)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.pl)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.ce)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.aa)
fam_subset_prec_all <- rbind(fam_subset_prec_all, fam_subset_prec.cbm)

fam_subset_f1_all <- rbind(fam_subset_f1.gh, fam_subset_f1.gt)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.pl)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.ce)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.aa)
fam_subset_f1_all <- rbind(fam_subset_f1_all, fam_subset_f1.cbm)

fam_subset_acc_all <- rbind(fam_subset_acc.gh, fam_subset_acc.gt)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.pl)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.ce)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.aa)
fam_subset_acc_all <- rbind(fam_subset_acc_all, fam_subset_acc.cbm)

# rename columns to faciltate merging
names(fam_subset_sens_all)[names(fam_subset_sens_all) == "Statistic_value"] <- "Sensitivity"
names(fam_subset_spec_all)[names(fam_subset_spec_all) == "Statistic_value"] <- "Specificity"

# drop the column called X - from the index number created by pandas in Python
fam_subset_sens_all <- fam_subset_sens_all[c("CAZy_family", "Prediction_tool", "Sensitivity", "CAZy_class")]
fam_subset_spec_all <- fam_subset_spec_all[c("CAZy_family", "Prediction_tool", "Specificity", "CAZy_class")]

# merge the dataframes
cazy_fam_spec_sense_df <- merge(
  fam_subset_sens_all,
  fam_subset_spec_all,
  by=c("Prediction_tool", "CAZy_class", "CAZy_family")
)

# set order data is presented
fam_subset_spec_all$Prediction_tool <- factor(fam_subset_spec_all$Prediction_tool, levels = classifiers) 
fam_subset_spec_all$CAZy_class <- factor(fam_subset_spec_all$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

fam_subset_sens_all$Prediction_tool <- factor(fam_subset_sens_all$Prediction_tool, levels = classifiers) 
fam_subset_sens_all$CAZy_class <- factor(fam_subset_sens_all$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

fam_subset_prec_all$Prediction_tool <- factor(fam_subset_prec_all$Prediction_tool, levels = classifiers) 
fam_subset_prec_all$CAZy_class <- factor(fam_subset_prec_all$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

fam_subset_f1_all$Prediction_tool <- factor(fam_subset_f1_all$Prediction_tool, levels = classifiers) 
fam_subset_f1_all$CAZy_class <- factor(fam_subset_f1_all$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

fam_subset_acc_all$Prediction_tool <- factor(fam_subset_acc_all$Prediction_tool, levels = classifiers) 
fam_subset_acc_all$CAZy_class <- factor(fam_subset_acc_all$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

cazy_fam_spec_sense_df$Prediction_tool <- factor(cazy_fam_spec_sense_df$Prediction_tool, levels = classifiers) 
cazy_fam_spec_sense_df$CAZy_class <- factor(cazy_fam_spec_sense_df$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

# calculate sensitivity and specificity as a percentage
cazy_fam_spec_sense_df$Sens_percent <- cazy_fam_spec_sense_df$Sensitivity * 100
cazy_fam_spec_sense_df$Spec_percent <- cazy_fam_spec_sense_df$Specificity * 100

# log10 of specificity is calculated when plotting the chart
# define func for plotting sens % vs spec % log10
plot_fam_sens_vs_spec <- function(
  fam.class.df,
  cazy.class,
  noise.sample.size,
  noise.min.val,
  noise.max.value,
  lower.limit,
  upper.limit,
  plot_colours
  ){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  # add noise to the data for plotting the contour
  sens_perc_data <- fam.class.df.class.subset$Sens_percent
  class.sens.data.noise <- add_noise(sens_perc_data, noise.sample.size, noise.min.val, noise.max.value)
  spec_perc_data <- fam.class.df.class.subset$Spec_percent
  class.spec.data.noise <- add_noise(spec_perc_data, noise.sample.size, noise.min.val, noise.max.value)
  
  # add the data with noise as new columns to the df
  fam.class.df.class.subset$Sens_noise <- class.sens.data.noise
  fam.class.df.class.subset$Spec_noise <- class.spec.data.noise
  
  # generate the plot
  p.fam = ggplot(fam.class.df.class.subset %>% dplyr::group_by(Prediction_tool), aes(
      x=Sens_percent,
      y=Spec_percent
    )) +
    geom_density_2d_filled(alpha = 0.5, aes(x=Sens_noise, y=Spec_noise)) +
    geom_point() +
    scale_color_manual(values=plot_colours) +
    theme(plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=11,face="bold"),
          strip.text = element_text(size=11)) +
    labs(x = "Sensitivity (%)", y = "Specificity (log10)", color="Classifer", fill="Density") +
    scale_y_log10(breaks = trans_breaks("log10", function(x) 10^x), labels = trans_format("log10", math_format(10^.x)), limits=c(lower.limit,upper.limit)) +
    coord_trans(y="log10") +
    facet_wrap(~ Prediction_tool)
  
  return(p.fam)
}

plot.fam.sens.jitter <- function(fam.class.df, cazy.class, plot_colour){
  # subset for the class of interest
  fam.class.df.class.subset <- fam.class.df[which(fam.class.df$CAZy_class == cazy.class), ]
  
  p.fam.sens <- ggplot(fam.class.df.class.subset %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Sensitivity, fill=Prediction_tool)) +
	  geom_boxplot(outlier.shape=NA) +
	  geom_jitter(width=0.1, height=0) +
	  scale_fill_manual(values = plot_colour) +
	  theme(legend.position = "none",
		plot.background = element_rect(fill = figbg, color = figbg),
		axis.text=element_text(size=10),
		axis.title=element_text(size=12,face="bold")) +
	  xlab("Classifier") + 
	  ylab("Sensitivity") 
  
  return(p.fam.sens)
}
```
The following plots present the specificity (Fig.\@ref(fig:famsSpec)), sensitivity (Fig.\@ref(fig:famsSens)), precision (Fig.\@ref(fig:famsPrec)), F1-score (Fig.\@ref(fig:famsF1)) and accuracy (Fig.\@ref(fig:famsAcc)) for each CAZy family per classifier. In accompaniment to each plot is a table summarising the mean statistic value for each classifier across all CAZy families for each CAZy class.

### Specificity

```{r famsSpecTable, echo=FALSE}
colnames(fam_subset_spec_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')
fam_subset_spec_all.means <- fam_subset_spec_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>%
  dplyr::summarise(
    "Mean"=mean(Statistical_parameter),
    "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)

kable(fam_subset_spec_all.means, caption="Specificity of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsSpec, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of specificity for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_spec_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')

p.fam.spec <- ggplot(fam_subset_spec_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistical_parameter, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("Specificity") +
  facet_wrap(~ CAZy_class)
p.fam.spec
```
### Sensitivity

```{r famsSensTable, echo=FALSE}
colnames(fam_subset_sens_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')
fam_subset_sens_all.means <- fam_subset_sens_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Statistical_parameter), "Standard Deviation"=sd(Statistical_parameter),
    "Lower CI"=CI(Statistical_parameter)[3],
    "Upper CI"=CI(Statistical_parameter)[1]
)


kable(fam_subset_sens_all.means, caption="Sensitivity of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsSens, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of sensitivity for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
# set order data is presented
cazy_fam_spec_sense_df$Prediction_tool <- factor(cazy_fam_spec_sense_df$Prediction_tool, levels = classifiers) 
cazy_fam_spec_sense_df$CAZy_class <- factor(cazy_fam_spec_sense_df$CAZy_class, levels = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM'))

p.fam.sens.og <- ggplot(cazy_fam_spec_sense_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Sensitivity, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  facet_wrap(~ CAZy_class)

colnames(fam_subset_sens_all) <- c('CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'CAZy_class')

p.fam.sens <- ggplot(fam_subset_sens_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistical_parameter, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  facet_wrap(~ CAZy_class)
p.fam.sens
```

### Precision

```{r famsPrecTable, echo=FALSE}
fam_subset_prec_all.means <- fam_subset_prec_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Statistic_value), "Standard Deviation"=sd(Statistic_value),
  "Lower CI"=CI(Statistic_value)[3],
  "Upper CI"=CI(Statistic_value)[1]
)

kable(fam_subset_prec_all.means, caption="Precision of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsPrec, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of precision for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_prec_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'Statistic_value', 'CAZy_class')

p.fam.prec <- ggplot(fam_subset_prec_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("Precision") +
  facet_wrap(~ CAZy_class)
p.fam.prec
```

### F1-score

```{r famsF1Table, echo=FALSE}
fam_subset_f1_all.means <- fam_subset_f1_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>% dplyr::summarise("Mean"=mean(Statistic_value), "Standard Deviation"=sd(Statistic_value),
  "Lower CI"=CI(Statistic_value)[3],
  "Upper CI"=CI(Statistic_value)[1]
)

kable(fam_subset_f1_all.means, caption="Sensitivity of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsF1, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of the F1-score for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_f1_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'Statistic_value', 'class_group', 'CAZy_class')

p.fam.f1 <- ggplot(fam_subset_f1_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("F1-score") +
  facet_wrap(~ CAZy_class)
p.fam.f1
```

### Accuracy

```{r famsAccTable, echo=FALSE}
fam_subset_acc_all.means <- fam_subset_acc_all %>% dplyr::group_by(CAZy_class, Prediction_tool) %>% dplyr::summarise("Mean"=mean(Statistic_value), "Standard Deviation"=sd(Statistic_value),
   "Lower CI"=CI(Statistic_value)[3],
  "Upper CI"=CI(Statistic_value)[1]
)


kable(fam_subset_acc_all.means, caption="Accuracy of CAZy family classification per CAZyme classifier", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famsAcc, echo=FALSE, fig.cap="Scatter plot of overlaying a one-dimensional box-and-whisker plot of the accuracy for each CAZy family for each CAZyme classifier. Each CAZy family is represented as a single point on the plot."}
colnames(fam_subset_f1_all) <- c('X', 'CAZy_family', 'Prediction_tool', 'Statistical_parameter', 'Statistic_value', 'class_group', 'CAZy_class')

p.fam.f1 <- ggplot(fam_subset_f1_all %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold"),
	axis.text.x=element_text(angle=90)) +
  xlab("Classifier") + 
  ylab("Accuracy") +
  facet_wrap(~ CAZy_class)
p.fam.f1
```

```{r saveFamSensBoxplot, include=FALSE}
pdf(file = "famSensBoxplots.pdf", width = 11.25, height = 7)
p.fam.sens
dev.off()
```

### Plotting senstivity against specificity

For better resolution we can group the CAZy families by their parent CAzy classes, and compare the performances of the tools CAZy class, by CAZy class. Owing to the minimal variation in specificity scores, specificity was plotted as the percentage specificity log10.

### Glycoside Hydrolases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Glycoside Hydrolase CAZy family.

```{r ghfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycoside Hydrolases. Each GH CAZy family is represented as a single point on the plot."}
p.gh.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GH', 0.8, 0.1, 0.8, 99.7, 100, colour_set)
p.gh.sens.spec.fam
```

```{r saveGhSensSpec, include=FALSE}
pdf(file = "famSpecSensGh.pdf", width = 11.25, height = 7)
p.gh.sens.spec.fam
dev.off()
```

### Glycosyltransferases

Figure \@ref(fig:gtfamrllvspc) shows the plotting of sensitivity against specificity for each Glycosyltransferases CAZy family.

```{r gtfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycosyltransferases. Each GT CAZy family is represented as a single point on the plot."}
p.gt.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'GT', 0.8, 0.1, 0.8, 99.8, 100, colour_set)
p.gt.sens.spec.fam
```

```{r saveGtSensSpec, include=FALSE}
pdf(file = "famSpecSensGt.pdf", width = 11.25, height = 7)
p.gt.sens.spec.fam
dev.off()
```

### Polysaccharide Lyases

Figure \@ref(fig:ghfamrllvspc) shows the plotting of sensitivity against specificity for each Polysaccharide Lyases CAZy family.

```{r plfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Polysaccharide Lyases. Each PL CAZy family is represented as a single point on the plot."}
p.pl.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'PL', 0.9, 0.1, 0.4, 99.9, 100, colour_set)
p.pl.sens.spec.fam
```

```{r savePlSensSpec, include=FALSE}
pdf(file = "famSpecSensPl.pdf", width = 11.25, height = 7)
p.pl.sens.spec.fam
dev.off()
```

### Carbohydrate Esterases

Figure \@ref(fig:cefamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Esterases CAZy family.

```{r cefamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Esterases. Each CE CAZy family is represented as a single point on the plot."}
p.ce.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CE', 0.8, 0.1, 0.5, 99.7, 100, colour_set)
p.ce.sens.spec.fam
```

```{r saveCeSensSpec, include=FALSE}
pdf(file = "famSpecSensCe.pdf", width = 11.25, height = 7)
p.ce.sens.spec.fam
dev.off()
```

### Auxillary Activities

Figure \@ref(fig:afamrllvspc) shows the plotting of sensitivity against specificity for each Auxillary Activities CAZy family.

```{r aafamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Auxillary Activities. Each AA CAZy family is represented as a single point on the plot."}
p.aa.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'AA', 0.8, 0.1, 0.7, 99.7, 100, colour_set)
p.aa.sens.spec.fam
```

```{r saveAaSensSpec, include=FALSE}
pdf(file = "famSpecSensAa.pdf", width = 11.25, height = 7)
p.aa.sens.spec.fam
dev.off()
```


### Carbohydate Binding Modules

Figure \@ref(fig:cbmfamrllvspc) shows the plotting of sensitivity against specificity for each Carbohydrate Binding Module CAZy family.

```{r cbmfamrllvspc, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Binding Modules. Each CBM CAZy family is represented as a single point on the plot."}
p.cbm.sens.spec.fam <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df, 'CBM', 0.8, 0.1, 1, 99.4, 100, colour_set)
p.cbm.sens.spec.fam
```

```{r saveCbmSensSpec, include=FALSE}
pdf(file = "famSpecSensCbm.pdf", width = 11.25, height = 7)
p.cbm.sens.spec.fam
dev.off()
```

## Consistently poor performing CAZy families

We then pulled out the CAZy families with which at least three classifiers produced a sensitivity score of less than 0.75.

### GH difficult to classify families

```{r poorCAZyFams, include=FALSE}
# add class names

# retrieve all the family names
dbcan.fam.subset <- cazy_family_long_df[which(cazy_family_long_df$Prediction_tool == "dbCAN"), ]
tn.dbcan.fam.subset <- dbcan.fam.subset[which(dbcan.fam.subset$Statistical_parameter == "TNs"), ]

# separate names into one vector per CAZy class
gh.names = tn.dbcan.fam.subset[1:172,]$CAZy_family
gt.names = tn.dbcan.fam.subset[173:287,]$CAZy_family
pl.names = tn.dbcan.fam.subset[288:329,]$CAZy_family
ce.names = tn.dbcan.fam.subset[330:348,]$CAZy_family
aa.names = tn.dbcan.fam.subset[349:365,]$CAZy_family
cbm.names = tn.dbcan.fam.subset[366:454,]$CAZy_family

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class
  
pl.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class
  
ce.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class
  
aa.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class
  
cbm.subset <- cazy_family_long_df[which(cazy_family_long_df$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
cazy_family_long_df.class <- rbind(gh.subset, gt.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, pl.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, ce.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, aa.subset)
cazy_family_long_df.class <- rbind(cazy_family_long_df.class, cbm.subset)

# set order data is presented
gh.subset$Prediction_tool <- factor(gh.subset$Prediction_tool, levels = classifiers) 
gt.subset$Prediction_tool <- factor(gt.subset$Prediction_tool, levels = classifiers) 
pl.subset$Prediction_tool <- factor(pl.subset$Prediction_tool, levels = classifiers) 
ce.subset$Prediction_tool <- factor(ce.subset$Prediction_tool, levels = classifiers) 
aa.subset$Prediction_tool <- factor(aa.subset$Prediction_tool, levels = classifiers) 
cbm.subset$Prediction_tool <- factor(cbm.subset$Prediction_tool, levels = classifiers) 

get_poor_fams <- function(df){
  df.sens.subset <- df[which(df$Statistical_parameter == "Sensitivity"), ]
  low.sens <- df.sens.subset[which(df.sens.subset$Statistic_value <= "0.75"), ]
  
  # low.sens contains only the rows with a sensitivity less than 0.75, so some tools sensitivity scores are excluded
  low.sens.families = unique(low.sens$CAZy_family)
  
  low.sens.df <- data.frame(
    X=c(),
    CAZy_family=c(),
    Prediction_tool=c(),
    Statistical_parameter=c(),
    Statistic_value=c(),
    CAZy_class=c()
  )

  for (fam in low.sens.families){
     new_df <- df.sens.subset[which(df.sens.subset$CAZy_family == fam), ]
     low.sens.df <- rbind(low.sens.df, new_df)
  }
  
  # this retrieves all CAZy families with at least one tool producing a sensitivity of less than 0.75
  # retrieve CAZy families for which at least 3 tools produced a sensitivity score of less than 0.75
  tools <- classifiers
  
  poor.fam.df <- data.frame(
    X=c(),
    CAZy_family=c(),
    Prediction_tool=c(),
    Statistical_parameter=c(),
    Statistic_value=c(),
    CAZy_class=c()
  )
  
  parsed_fams <- vector()
  
  
  for (row in 1:nrow(low.sens.df)){
    no.of.tools <- 0

    # retrieve all rows for the CAZy fam
    cazy.fam <- low.sens.df[row, "CAZy_family"]
    fam.low.sens.df <- low.sens.df[which(low.sens.df$CAZy_fam == cazy.fam), ]
    
    fam.low.sens.df <- fam.low.sens.df[complete.cases(fam.low.sens.df), ]
    if (length(fam.low.sens.df$CAZy_family == 6)){
    
    
      if (!(cazy.fam %in% parsed_fams)){
        
        for (fam.row in 1:nrow(fam.low.sens.df)){
          
          sens.score <- fam.low.sens.df[fam.row, "Statistic_value"]
          if (sens.score <= 0.75) {no.of.tools = no.of.tools + 1}
        }
        
        if (no.of.tools >= 3){poor.fam.df <- rbind(poor.fam.df, fam.low.sens.df)}
      }
      
      parsed_fams = append(parsed_fams, cazy.fam)
      
    }
    
  }
  return(poor.fam.df)
}

add_sample_data <- function(df){
  sample_sizes = c()
  populations = c()
  
  for (row in 1:nrow(df)){
    cazy.fam <- df[row, "CAZy_family"]
  
    pop <- fam.populations[[cazy.fam]]
    sample <- fam.sample.sizes[[cazy.fam]]

    sample_sizes = append(sample_sizes, sample)
    populations = append(populations, pop)

  }
  
  df$Sample <- sample_sizes
  df$Population <- populations
  
  return(df)
}

build_poor_fam_plot <- function(df){
  new.plot <- ggplot(
    df %>% dplyr::group_by(Prediction_tool),
    aes(x=Prediction_tool, y=CAZy_family, fill=Statistic_value)
  ) +
    geom_tile() +
    scale_fill_viridis_c() +
    geom_text(data=df, aes(label=round(Statistic_value, digits=3))) +
    geom_text(data=df, aes(label=Sample, x='Sample')) +
    geom_text(data=df, aes(label=Population, x='Population')) +
    theme(legend.background = element_rect(fill = figbg, color = figbg),
          plot.background = element_rect(fill = figbg, color = figbg), 
          text = element_text(size=10), 
          legend.text=element_text(size=10),
          legend.title=element_text(size=12),
          axis.text=element_text(size=10),
          axis.title=element_text(size=12,face="bold")) +
    xlab("Classifier") +
    ylab("CAZy family") +
    labs(fill = "Sensitivity")
  
  return(new.plot)
}

```

```{r poorGHfams, echo=FALSE}
gh.low.sens <- get_poor_fams(gh.subset)

gh.low.sens <- add_sample_data(gh.low.sens)

# mannually set the CAZy family order
gh.fam.order <- c( "GH170", "GH166", "GH163", "GH152", "GH151", "GH136", "GH135", "GH128", "GH123", "GH79", "GH50", "GH12", "GH0")
gh.low.sens$CAZy_family <- factor(gh.low.sens$CAZy_family, levels = gh.fam.order) 

# generate plot
p.gh.poor.pops <- build_poor_fam_plot(gh.low.sens)
p.gh.poor.pops
```
```{r saveGhPoorFams, include=FALSE}
pdf(file = "poorFamSensGH.pdf", width = 8.58, height = 5.5)
p.gh.poor.pops
dev.off()

png(file = "poorFamSensGh.png", width = 8.58, height = 5.5, res=900, units='in')
p.gh.poor.pops
dev.off()
```

### GT diffcult to classify families

```{r poorGTfams, echo=FALSE}
gt.low.sens <- get_poor_fams(gt.subset)

gt.low.sens <- add_sample_data(gt.low.sens)

# manually set the CAZy family order
gt.fam.order <- c( "GT113", "GT111", "GT109", "GT80", "GT77", "GT69", "GT60", "GT52", "GT47", "GT32", "GT31", "GT29", "GT25", "GT10", "GT7", "GT1", "GT0")
gt.low.sens$CAZy_family <- factor(gt.low.sens$CAZy_family, levels = gt.fam.order) 

# generate plot
p.gt.poor.pops <- build_poor_fam_plot(gt.low.sens)
p.gt.poor.pops
```

```{r saveGtPoorFams, include=FALSE}
pdf(file = "poorFamSensGt.pdf", width = 8.58, height = 5.5)
p.gt.poor.pops
dev.off()

png(file = "poorFamSensGt.png", width = 8.58, height = 5.5, res=900, units='in')
p.gt.poor.pops
dev.off()
```


### PL diffcult to classify families

```{r poorPLfams, echo=FALSE}
pl.low.sens <- get_poor_fams(pl.subset)

pl.low.sens <- add_sample_data(pl.low.sens)

# manually set the CAZy family order
pl.fam.order <- c("PL38", "PL33", "PL31", "PL29", "PL27", "PL20", "PL17", "PL8", "PL7", "PL6", "PL0")
pl.low.sens$CAZy_family <- factor(pl.low.sens$CAZy_family, levels = pl.fam.order) 

# generate plot
p.pl.poor.pops <- build_poor_fam_plot(pl.low.sens)
p.pl.poor.pops
```

```{r savePlPoorFams, include=FALSE}
pdf(file = "poorFamSensPl.pdf", width = 8.58, height = 5.5)
p.pl.poor.pops
dev.off()

png(file = "poorFamSensPl.png", width = 8.58, height = 5.5, res=900, units='in')
p.pl.poor.pops
dev.off()
```


### CE diffcult to classify families

```{r poorCEfams, echo=FALSE}
ce.low.sens <- get_poor_fams(ce.subset)

ce.low.sens <- add_sample_data(ce.low.sens)

# manually set the CAZy family order
ce.fam.order <- c( "CE13", "CE0")
ce.low.sens$CAZy_family <- factor(ce.low.sens$CAZy_family, levels = ce.fam.order) 

# generate plot
p.ce.poor.pops <- build_poor_fam_plot(ce.low.sens)
p.ce.poor.pops
```

```{r saveCePoorFams, include=FALSE}
pdf(file = "poorFamSensCe.pdf", width = 8.58, height = 1.5)
p.ce.poor.pops
dev.off()

png(file = "poorFamSensCe.png", width = 8.58, height = 1.5, res=900, units='in')
p.ce.poor.pops
dev.off()
```


### AA diffcult to classify families

```{r poorAAfams, echo=FALSE}
aa.low.sens <- get_poor_fams(aa.subset)

aa.low.sens <- add_sample_data(aa.low.sens)

# manually set the CAZy family order
aa.fam.order <- c("AA16", "AA8", "AA2", "AA0")
aa.low.sens$CAZy_family <- factor(aa.low.sens$CAZy_family, levels = aa.fam.order) 

# generate plot
p.aa.poor.pops <- build_poor_fam_plot(aa.low.sens)
p.aa.poor.pops
```

```{r saveAaPoorFams, include=FALSE}
pdf(file = "poorFamSensAa.pdf", width = 8.58, height = 2.25)
p.aa.poor.pops
dev.off()

png(file = "poorFamSensAa.png", width = 8.58, height = 2.25, res=900, units='in')
p.aa.poor.pops
dev.off()
```


### CBM diffcult to classify families

```{r poorCBMfams, echo=FALSE}
cbm.low.sens <- get_poor_fams(cbm.subset)

cbm.low.sens <- add_sample_data(cbm.low.sens)

# manually set the CAZy family order
cbm.fam.order <- c("CBM88", "CBM69", "CBM66", "CBM61", "CBM54", "CBM50", "CBM47", "CBM32", "CBM25", "CBM21", "CBM20", "CBM16", "CBM15", "CBM14", "CBM13", "CBM10", "CBM9", "CBM5", "CBM0")
cbm.low.sens$CAZy_family <- factor(cbm.low.sens$CAZy_family, levels = cbm.fam.order) 

# generate plot
p.cbm.poor.pops <- build_poor_fam_plot(cbm.low.sens)
p.cbm.poor.pops
```

```{r saveCbmPoorFams, include=FALSE}
pdf(file = "poorFamSensCbm.pdf", width = 8.58, height = 5.5)
p.cbm.poor.pops
dev.off()

png(file = "poorFamSensCbm.png", width = 8.58, height = 5.5, res=900, units='in')
p.cbm.poor.pops
dev.off()
```


## Evaluation of multi-label CAZy family classification performance

CAZy annotates proteins in a domain-wise manner. Consequently, a single protein may be assigned to multiple CAZy families. The ability of a classifier to assign all the correct CAZy family annotations for a given protein when only evaluating the CAZy family classification performance per CAZy family, independently of all other CAZy classes.

The CAZy family multi-label classification performance is represented by the Rand Index (RI) and Adjusted Rand Index (ARI). The RI is a quantitive measure of similarity between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings. In this case the two clusters are the predicted and groud truth CAZy family annotations. The raw RI score is then “adjusted for chance” into the ARI score using the following scheme:  
`ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)`
This produces a score between 1 and -1. A score of 1 is produced if all predicted and known CAZy family annotations are identical, 0 if completely random clustering of -1 if systematically incorrect clustering and the number of incorrect classifications of proteins is greater than would be expected from randomly annotating proteins with CAZy families.

```{r famRi, echo=FALSE}
fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = classifiers) # set order data is presented

fam_ri_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index),
  "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3],
  "Upper CI"=CI(Rand_index)[1]
)

kable(fam_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r famAriCalc, echo=FALSE}
fam_ari_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3],
  "Upper CI"=CI(Adjusted_Rand_index)[1]
)

kable(fam_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

Multilabel classification raises when a single instance can be assinged to multiple classes. In this evaluation a single instance is a protein and the classes are CAZy families, a single CAZyme can be assigned to multiple CAZy families. This is important to take into consideration because the same approaches for statistical evaluation of binary classification provided a limited view of the performance of the classifiers when applied to multilabel classification.

Plot are violin plots overlayed by scatter plots of the Rand Index and Adjusted Rand Index for every protein in every test set, excluding true negatives.

```{r famRI, echo=FALSE, fig.cap="Violin plot of Rand Index (RI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ri.class = ggplot(fam_classification_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  scale_y_continuous(breaks = seq(0,1, by = 0.1))
p.ri.class
```

```{r famRIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Rand Index (RI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

fam_ri_stats_df$Prediction_tool <- factor(fam_ri_stats_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ri.class = ggplot(fam_ri_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.ri.class
```

```{r famARI, echo=FALSE, fig.cap="Violin plot of Adjusted Rand Index (ARI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

p.ari.class = ggplot(fam_classification_df %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Adjusted_Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  scale_y_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))
p.ari.class
```

```{r famARIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Adjusted Rand Index (ARI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy families"}

fam_ari_stats_df$Prediction_tool <- factor(fam_ari_stats_df$Prediction_tool, levels = classifiers) # set order data is presented

# plot RI
p.ari.fam = ggplot(fam_ari_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.ari.fam
```

# Performance per taxonomy group

The performance for a classifier per taxonomy group may vary. For this evaluation the test sets were separated into the taxonomy groups:
- Bacteria
- Eukaryote

The evaluation per classifier per taxonomy group, versus all test sets pooled together was evaluated.

## Binary classification of CAZymes and non-CAZymes

Here we calculate the mean plus and minus the standard deviation of the F1-score of each prediction tool for each taxonomy group, to represent the overall performance per taxonomy group.

```{r summaryTaxGroupBinary, echo=FALSE}
tax_binary_f1_df <- binary_tax_df[which(binary_stat_df$Statistic_parameter == "FBeta-score"), ]

tax_binary_f1_df.bact <- tax_binary_f1_df[which(tax_binary_f1_df$Tax_group == "Bacteria"), ]
tax_binary_f1_df.euk <- tax_binary_f1_df[which(tax_binary_f1_df$Tax_group == "Eukaryote"), ]

# create dataframe with tax_group set as "All"
all.vector <- rep("All", nrow(tax_binary_f1_df))
all.tax_binary_f1_df <- tax_binary_f1_df
all.tax_binary_f1_df$Tax_group <- all.vector

bact.tax_binary_f1_mean_sd <- tax_binary_f1_df.bact %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Statistic_value), "Bact Standard Deviation"=sd(Statistic_value),
  "Bact Lower CI"=CI(Statistic_value)[3],
  "Bact Upper CI"=CI(Statistic_value)[1]
  )
euk.tax_binary_f1_mean_sd <- tax_binary_f1_df.euk %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Euk Mean"=mean(Statistic_value), "Euk Standard Deviation"=sd(Statistic_value),
  "Euk Lower CI"=CI(Statistic_value)[3],
  "Euk Upper CI"=CI(Statistic_value)[1]
  )
all.tax_binary_f1_mean_sd <- all.tax_binary_f1_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "All Mean"=mean(Statistic_value), "All Standard Deviation"=sd(Statistic_value),
  "All Lower CI"=CI(Statistic_value)[3],
  "All Upper CI"=CI(Statistic_value)[1]
  )

tax_binary_f1_mean_sd <- bact.tax_binary_f1_mean_sd

tax_binary_f1_mean_sd$`Euk Mean` <- euk.tax_binary_f1_mean_sd$`Euk Mean`
tax_binary_f1_mean_sd$`Euk Standard Deviation` <- euk.tax_binary_f1_mean_sd$`Euk Standard Deviation`
tax_binary_f1_mean_sd$`Euk Lower CI` <- euk.tax_binary_f1_mean_sd$`Euk Lower CI`
tax_binary_f1_mean_sd$`Euk Upper CI` <- euk.tax_binary_f1_mean_sd$`Euk Upper CI`

tax_binary_f1_mean_sd$`All Mean` <- all.tax_binary_f1_mean_sd$`All Mean`
tax_binary_f1_mean_sd$`All Standard Deviation` <- all.tax_binary_f1_mean_sd$`All Standard Deviation`
tax_binary_f1_mean_sd$`All Lower CI` <- all.tax_binary_f1_mean_sd$`All Lower CI`
tax_binary_f1_mean_sd$`All Upper CI` <- all.tax_binary_f1_mean_sd$`All Upper CI`

tax_binary_f1_mean_sd$Prediction_tool <- factor(tax_binary_f1_mean_sd$Prediction_tool, levels=classifiers) # set order data is presented

kable(tax_binary_f1_mean_sd, caption="The F1-score of binary CAZyme/non-CAZyme classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r taxBinCI, echo=FALSE, fig.cap="95% confidence interval around the mean F1-score of the binary classification of CAZymes and non-CAZymes per taxonomic group."}
t.b.means <- tax_binary_f1_mean_sd$`Bact Mean`
t.b.means <- append(tax_binary_f1_mean_sd$`Euk Mean`, t.b.means)
t.b.means <- append(tax_binary_f1_mean_sd$`All Mean`, t.b.means)

t.b.lower.cis <- tax_binary_f1_mean_sd$`Bact Lower CI`
t.b.lower.cis <- append(tax_binary_f1_mean_sd$`Euk Lower CI`, t.b.lower.cis)
t.b.lower.cis <- append(tax_binary_f1_mean_sd$`All Lower CI`, t.b.lower.cis)

t.b.upper.cis <- tax_binary_f1_mean_sd$`Bact Upper CI`
t.b.upper.cis <- append(tax_binary_f1_mean_sd$`Euk Upper CI`, t.b.upper.cis)
t.b.upper.cis <- append(tax_binary_f1_mean_sd$`All Upper CI`, t.b.upper.cis)

t.b.tools <- tax_binary_f1_mean_sd$Prediction_tool
t.b.tool.names <- c()
for (t in t.b.tools){
  t.b.tool.names <- append(t.b.tool.names, t)
}

t.b.taxonomic_group <- rep('Bacteria', 6)
t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('Eukaryote', 6))
t.b.taxonomic_group <- append(t.b.taxonomic_group, rep('All', 6))

ci.tax.bin.df <- data.frame(t.b.means, t.b.lower.cis, t.b.upper.cis, t.b.tool.names, t.b.taxonomic_group)
colnames(ci.tax.bin.df) <- c('MeanCI', 'LowerCI', 'UpperCI', 'Prediction_tool', 'Taxonomic_group')

ci.tax.bin.df$Prediction_tool <- factor(ci.tax.bin.df$Prediction_tool, levels = classifiers)

p.t.b.ci <- ggplot(ci.tax.bin.df %>% dplyr::group_by(Taxonomic_group),
              aes(x=Taxonomic_group, y=MeanCI, color=Taxonomic_group)) +
geom_point() +
geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
scale_colour_manual(values = colour_set) +
theme(legend.position = "none",
      plot.background = element_rect(fill = figbg, color = figbg),
      axis.text=element_text(size=10),
      axis.title=element_text(size=12,face="bold")) +
xlab("Classifier") + 
ylab("Statistical value") +
facet_wrap(~ Prediction_tool, ncol=2)

p.t.b.ci
```


```{r buildTaxGroupBinaryPlots, include=FALSE}
build.tax.group.binary.plot <- function(stat_parameter, tax.df){
  # subset stat parameter
  tax_binary_stat <- tax.df[which(tax.df$Statistic_parameter == stat_parameter), ]
  
  tax_binary_stat$Prediction_tool <- factor(tax_binary_stat$Prediction_tool, levels = classifiers) # set order data is presented
  
  # create Bacteria plot
  bact.tax_binary_stat <- binary_tax_df[which(tax_binary_stat$Tax_group == "Bacteria"), ]
  
  # create Eukaryote plot
  euk.tax_binary_stat <- binary_tax_df[which(tax_binary_stat$Tax_group == "Eukaryote"), ]
  
  # create dataframe with tax_group set as "All"
  all.vector <- rep("All", nrow(tax_binary_stat))
  all.tax_binary_stat <- tax_binary_stat
  all.tax_binary_stat$Tax_group <- all.vector
  
  complete.tax_binary_stat <- rbind(tax_binary_stat, all.tax_binary_stat)
  
  p.tax.binary = ggplot(complete.tax_binary_stat %>% dplyr::group_by(Tax_group),
                aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
    geom_boxplot(outlier.shape=NA) +
    geom_jitter(width=0.1, height=0) +
    scale_fill_manual(values = colour_set) +
    theme(legend.position = "right",
          plot.background = element_rect(fill = figbg, color = figbg),
          axis.text=element_text(size=10),
          axis.title=element_text(size=12,face="bold")) +
    xlab("Classifier") + 
    ylab(stat_parameter) +
    facet_wrap(~ Prediction_tool)
  
  return(p.tax.binary)
}
```

### Specificity

```{r taxGroupBinarySpecificity, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the specificity of binary classification per CAZyme/non-CAZyme classifier per taxonomy group. Each point represents the score from one test set."}
p.tax.binary.spec <- build.tax.group.binary.plot("Specificity", binary_tax_df)
p.tax.binary.spec
```

```{r saveTaxBinSpec, include=FALSE}
pdf(file = "taxBinarySpec.pdf", width = 8.58, height = 5.5)
p.tax.binary.spec
dev.off()
```

### Sensitivity

```{r taxGroupBinarySensitivity, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the sensitivity of binary classification per CAZyme/non-CAZyme classifier per taxonomy group. Each point represents the score from one test set."}
p.tax.binary.sens <- build.tax.group.binary.plot("Sensitivity", binary_tax_df)
p.tax.binary.sens
```

```{r saveTaxBinSens, include=FALSE}
pdf(file = "taxBinarySens.pdf", width = 8.58, height = 5.5)
p.tax.binary.sens
dev.off()
```

### Percision

```{r taxGroupBinaryPrecision, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the precision of binary classification per CAZyme/non-CAZyme classifier per taxonomy group. Each point represents the score from one test set."}
p.tax.binary.prec <- build.tax.group.binary.plot("Precision", binary_tax_df)
p.tax.binary.prec
```

```{r saveTaxBinPrec, include=FALSE}
pdf(file = "taxBinaryPrec.pdf", width = 8.58, height = 5.5)
p.tax.binary.prec
dev.off()
```

### F1-score

```{r taxGroupBinaryF1, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of binary classification per CAZyme/non-CAZyme classifier per taxonomy group. Each point represents the score from one test set."}
p.tax.binary.f1 <- build.tax.group.binary.plot("FBeta-score", binary_tax_df)
p.tax.binary.f1
```

```{r saveTaxBinF1, include=FALSE}
pdf(file = "taxBinaryF1.pdf", width = 8.58, height = 5.5)
p.tax.binary.f1
dev.off()
```

### Accuracy

```{r taxGroupBinaryAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the accuracy of binary classification per CAZyme/non-CAZyme classifier per taxonomy group. Each point represents the score from one test set."}
p.tax.binary.acc <- build.tax.group.binary.plot("Accuracy", binary_tax_df)
p.tax.binary.acc
```

```{r saveTaxBinAcc, include=FALSE}
pdf(file = "taxBinaryAcc.pdf", width = 8.58, height = 5.5)
p.tax.binary.acc
dev.off()
```

## CAZy class classification

Below a table containing the mean F1-score plus/minus standard deviation for per CAZyme classifier per taxonomy group is presented, in order to represent the overall performance per CAZyme classifier per taxonomy group for all CAZy class classification.

```{r summaryTaxGroupClass, echo=FALSE}
# subset the F1-scores 
bact.cazy_class_df.f1 <- bact.cazy_class_df[which(bact.cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
bact.cazy_class_df.f1  <- bact.cazy_class_df.f1[complete.cases(bact.cazy_class_df.f1), ]
bact.cazy_class_df.f1.stat <- bact.cazy_class_df.f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Statistic_value), "Bact Standard Deviation"=sd(Statistic_value),
    "Bact Lower CI"=CI(Statistic_value)[3],
    "Bact Upper CI"=CI(Statistic_value)[1]
  )

euk.cazy_class_df.f1 <- euk.cazy_class_df[which(euk.cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
euk.cazy_class_df.f1  <- euk.cazy_class_df.f1[complete.cases(euk.cazy_class_df.f1), ]
euk.cazy_class_df.f1.stat <- euk.cazy_class_df.f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Statistic_value), "Standard Deviation"=sd(Statistic_value),
    "Lower CI"=CI(Statistic_value)[3],
    "Upper CI"=CI(Statistic_value)[1]
  )

class_subset_f1 <- cazy_class_df[which(cazy_class_df$Statistic_parameter == "Fbeta_score"), ]
# 'Tax_group' column
all.vector <- rep("All", nrow(class_subset_f1))
all.cazy_class_df.f1 <- class_subset_f1
all.cazy_class_df.f1$Tax_group <- all.vector
class_subset_f1  <- class_subset_f1[complete.cases(class_subset_f1), ]
all.cazy_class_df.f1.stat <- class_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Statistic_value), "Standard Deviation"=sd(Statistic_value),
    "Lower CI"=CI(Statistic_value)[3],
    "Upper CI"=CI(Statistic_value)[1]
  )

tax_class_f1_mean_sd <- bact.tax_binary_f1_mean_sd

tax_class_f1_mean_sd$`Euk Mean` <- euk.cazy_class_df.f1.stat$`Mean`
tax_class_f1_mean_sd$`Euk Standard Deviation` <- euk.cazy_class_df.f1.stat$`Standard Deviation`
tax_class_f1_mean_sd$`Euk Lower CI` <- euk.cazy_class_df.f1.stat$`Lower CI`
tax_class_f1_mean_sd$`Euk Upper CI` <- euk.cazy_class_df.f1.stat$`Upper CI`

tax_class_f1_mean_sd$`All Mean` <- all.cazy_class_df.f1.stat$`Mean`
tax_class_f1_mean_sd$`All Standard Deviation` <- all.cazy_class_df.f1.stat$`Standard Deviation`
tax_class_f1_mean_sd$`All Lower CI` <- all.cazy_class_df.f1.stat$`Lower CI`
tax_class_f1_mean_sd$`All Upper CI` <- all.cazy_class_df.f1.stat$`Upper CI`

tax_class_f1_mean_sd$Prediction_tool <- factor(tax_binary_f1_mean_sd$Prediction_tool, levels=classifiers) # set order data is presented

kable(tax_class_f1_mean_sd, caption="Overall performance (represented by the F1-score) of CAZy class classification by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)

```
```{r taxClassCI, echo=FALSE, fig.cap="95% confidence interval around the mean F1-score of the classification of CAZy classes per taxonomic group."}
t.c.means <- tax_class_f1_mean_sd$`Bact Mean`
t.c.means <- append(tax_class_f1_mean_sd$`Euk Mean`, t.c.means)
t.c.means <- append(tax_class_f1_mean_sd$`All Mean`, t.c.means)

t.c.lower.cis <- tax_class_f1_mean_sd$`Bact Lower CI`
t.c.lower.cis <- append(tax_class_f1_mean_sd$`Euk Lower CI`, t.c.lower.cis)
t.c.lower.cis <- append(tax_class_f1_mean_sd$`All Lower CI`, t.c.lower.cis)

t.c.upper.cis <- tax_class_f1_mean_sd$`Bact Upper CI`
t.c.upper.cis <- append(tax_class_f1_mean_sd$`Euk Upper CI`, t.c.upper.cis)
t.c.upper.cis <- append(tax_class_f1_mean_sd$`All Upper CI`, t.c.upper.cis)

t.c.tools <- tax_class_f1_mean_sd$Prediction_tool
t.c.tool.names <- c()
for (t in t.c.tools){
  t.c.tool.names <- append(t.c.tool.names, t)
}

t.c.taxonomic_group <- rep('Bacteria', 6)
t.c.taxonomic_group <- append(t.c.taxonomic_group, rep('Eukaryote', 6))
t.c.taxonomic_group <- append(t.c.taxonomic_group, rep('All', 6))

ci.tax.class.df <- data.frame(t.c.means, t.c.lower.cis, t.c.upper.cis, t.c.tool.names, t.c.taxonomic_group)
colnames(ci.tax.class.df) <- c('MeanCI', 'LowerCI', 'UpperCI', 'Prediction_tool', 'Taxonomic_group')

ci.tax.class.df$Prediction_tool <- factor(ci.tax.class.df$Prediction_tool, levels = classifiers)

ci.tax.class.df$Taxonomic_group <- factor(ci.tax.class.df$Taxonomic_group, levels = c('Bacteria','Eukaryote','All'))

p.t.c.ci <- ggplot(ci.tax.class.df %>% dplyr::group_by(Taxonomic_group),
                   aes(x=Taxonomic_group, y=MeanCI, color=Taxonomic_group)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_colour_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Prediction_tool, ncol=2)

p.t.c.ci
```


```{r taxClassSummaryData, echo=FALSE}
# add tax group column to the dataframes
bact.cazy_class_df$Tax_group <- rep('Bacteria', nrow(bact.cazy_class_df))
euk.cazy_class_df$Tax_group <- rep('Eukaryote', nrow(euk.cazy_class_df))
cazy_class_df$Tax_group <- rep('All', nrow(cazy_class_df))

# merge the dataframes into a single df
all_class_tax_df <- rbind(bact.cazy_class_df, euk.cazy_class_df)
all_class_tax_df <- rbind(all_class_tax_df, cazy_class_df)

# separate out into one df per CAZy class
class.tax.gh <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "GH"), ]
class.tax.gt <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "GT"), ]
class.tax.pl <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "PL"), ]
class.tax.ce <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "CE"), ]
class.tax.aa <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "AA"), ]
class.tax.cbm <- all_class_tax_df[which(all_class_tax_df$CAZy_class == "CBM"), ]

# func for plotting a scatter plot, facet wrapped by prediciton tools and grouped by tax_group
plot.class.tax.comparison <- function(df){
  df.f1 <- df[which(df$Statistic_parameter == "Fbeta_score"), ]

  p.tax.class = ggplot(df %>% dplyr::group_by(Tax_group),
              aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab('Fbeta_score') +
  facet_wrap(~ Prediction_tool)

return(p.tax.class)
}

get.tax.group.data <- function(df, tax){
   stat_df <- df[which(df$Tax_group == tax), ]
   spec.m.title <- paste("Mean", tax, "Specificity")
   spec.sd.title <- paste(tax, "Specificity Standard Deviation")
   spec.lower.ci <- paste(tax, "Lower CI")
   spec.upper.ci <- paste(tax, "Upper CI")
   
   sens.m.title <- paste("Mean", tax, "Sensitivity")
   sens.sd.title <- paste(tax, "Sensitivity Standard Deviation")
   sens.lower.ci <- paste(tax, "Lower CI")
   sens.upper.ci <- paste(tax, "Upper CI")
   
   prec.m.title <- paste("Mean", tax, "Precision")
   prec.sd.title <- paste(tax, "Precision Standard Deviation")
   prec.lower.ci <- paste(tax, "Lower CI")
   prec.upper.ci <- paste(tax, "Upper CI")
   
   f1.m.title <- paste("Mean", tax, "F1-score")
   f1.sd.title <- paste(tax, "F1-score Standard Deviation")
   f1.lower.ci <- paste(tax, "Lower CI")
   f1.upper.ci <- paste(tax, "Upper CI")
   
   acc.m.title <- paste("Mean", tax, "Accuracy")
   acc.sd.title <- paste(tax, "Accuracy Standard Deviation")
   acc.lower.ci <- paste(tax, "Lower CI")
   acc.upper.ci <- paste(tax, "Upper CI")
   
   col.names <- c('Classifier', 
                  spec.m.title,
                  spec.sd.title,
                  spec.lower.ci,
                  spec.upper.ci,
                  sens.m.title,
                  sens.sd.title,
                  sens.lower.ci,
                  sens.upper.ci,
                  prec.m.title,
                  prec.sd.title,
                  prec.lower.ci,
                  prec.upper.ci,
                  f1.m.title,
                  f1.sd.title,
                  f1.lower.ci,
                  f1.upper.ci,
                  acc.m.title,
                  acc.sd.title,
                  acc.lower.ci,
                  acc.upper.ci)
   
  # Calculate statistics
  subset_spec <- stat_df[which(stat_df$Statistic_parameter == "Specificity"), ]
  binary_specificity <- subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    spec.m.title=mean(Statistic_value),
    spec.sd.title=sd(Statistic_value),
    spec.lower.ci=CI(Statistic_value)[3],
    spec.upper.ci=CI(Statistic_value)[1]
    )
  
  subset_sens <- stat_df[which(stat_df$Statistic_parameter == "Sensitivity"), ]
  binary_sensitivity <- subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    sens.m.title=mean(Statistic_value),
    sens.sd.title=sd(Statistic_value),
    sens.lower.ci=CI(Statistic_value)[3],
    sens.upper.ci=CI(Statistic_value)[1]
  )
  
  subset_prec <- stat_df[which(stat_df$Statistic_parameter == "Precision"), ]
  binary_precision <- subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    prec.m.title=mean(Statistic_value),
    prec.sd.title=sd(Statistic_value),
    prec.lower.ci=CI(Statistic_value)[3],
    prec.upper.ci=CI(Statistic_value)[1]
  )
  
  subset_f1 <- stat_df[which(stat_df$Statistic_parameter == "Fbeta_score"), ]
  binary_f1_score <- subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    f1.m.title=mean(Statistic_value),
    f1.sd.title=sd(Statistic_value),
    f1.lower.ci=CI(Statistic_value)[3],
    f1.upper.ci=CI(Statistic_value)[1]
  )
  
  subset_acc <- stat_df[which(stat_df$Statistic_parameter == "Accuracy"), ]
  binary_accuracy <- subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
    acc.m.title=mean(Statistic_value),
    acc.sd.title=sd(Statistic_value),
    acc.lower.ci=CI(Statistic_value)[3],
    acc.upper.ci=CI(Statistic_value)[1]
  )
  
  # combine data and build a single dataframe
  binary_summary_df <- merge(binary_specificity, binary_sensitivity)
  binary_summary_df <- merge(binary_summary_df, binary_precision)
  binary_summary_df <- merge(binary_summary_df, binary_f1_score)
  binary_summary_df <- merge(binary_summary_df, binary_accuracy)
  
  subset_spec <- subset(subset_spec, select = c('Genomic_accession','Prediction_tool','CAZy_class','Statistic_parameter','Statistic_value','Tax_group'))
  subset_sens <- subset(subset_sens, select = c('Genomic_accession','Prediction_tool','CAZy_class','Statistic_parameter','Statistic_value','Tax_group'))
  subset_prec <- subset(subset_prec, select = c('Genomic_accession','Prediction_tool','CAZy_class','Statistic_parameter','Statistic_value','Tax_group'))
  subset_f1 <- subset(subset_f1, select = c('Genomic_accession','Prediction_tool','CAZy_class','Statistic_parameter','Statistic_value','Tax_group'))
  subset_acc <- subset(subset_acc, select = c('Genomic_accession','Prediction_tool','CAZy_class','Statistic_parameter','Statistic_value','Tax_group'))

  colnames(binary_summary_df) <- col.names
  
  return(
    list(
      binary_summary_df,
      subset_spec,
      subset_sens,
      subset_prec,
      subset_f1,
      subset_acc
    )
  )
}

get.tax.class.summary.tables <- function(class.tax.df){
  # drop rows with null values
  stat_df  <- class.tax.df[complete.cases(class.tax.df), ]
  
  tax.bact.dfs <- get.tax.group.data(stat_df, 'Bacteria')
  tax.bact_binary_summary_df <- tax.bact.dfs[[1]]
  tax.bact_subset_spec <- tax.bact.dfs[[2]]
  tax.bact_subset_sens <- tax.bact.dfs[[3]]
  tax.bact_subset_prec <- tax.bact.dfs[[4]]
  tax.bact_subset_f1 <- tax.bact.dfs[[5]]
  tax.bact_subset_acc <- tax.bact.dfs[[6]]
  
  tax.euk.dfs <- get.tax.group.data(stat_df, 'Eukaryote')
  tax.euk_binary_summary_df <- tax.euk.dfs[[1]]
  tax.euk_subset_spec <- tax.euk.dfs[[2]]
  tax.euk_subset_sens <- tax.euk.dfs[[3]]
  tax.euk_subset_prec <- tax.euk.dfs[[4]]
  tax.euk_subset_f1 <- tax.euk.dfs[[5]]
  tax.euk_subset_acc <- tax.euk.dfs[[6]]
      
  tax.all.dfs <- get.tax.group.data(stat_df, 'All')
  tax.all_binary_summary_df <- tax.all.dfs[[1]]
  tax.all_subset_spec <- tax.all.dfs[[2]]
  tax.all_subset_sens <- tax.all.dfs[[3]]
  tax.all_subset_prec <- tax.all.dfs[[4]]
  tax.all_subset_f1 <- tax.all.dfs[[5]]
  tax.all_subset_acc <- tax.all.dfs[[6]]
  
  tax.subset.spec <- rbind(tax.bact_subset_spec, tax.euk_subset_spec)
  tax.subset.spec <- rbind(tax.subset.spec, tax.all_subset_spec)

  tax.subset.sens <- rbind(tax.bact_subset_sens, tax.euk_subset_sens)
  tax.subset.sens <- rbind(tax.subset.sens, tax.all_subset_sens)
    
  tax.subset.prec <- rbind(tax.bact_subset_prec, tax.euk_subset_prec)
  tax.subset.prec <- rbind(tax.subset.prec, tax.all_subset_prec)
  
  tax.subset.f1 <- rbind(tax.bact_subset_f1, tax.euk_subset_f1)
  tax.subset.f1 <- rbind(tax.subset.f1, tax.all_subset_f1)
  
  tax.subset.acc <- rbind(tax.bact_subset_acc, tax.euk_subset_acc)
  tax.subset.acc <- rbind(tax.subset.acc, tax.all_subset_acc)

  return(
    list(
      tax.bact_binary_summary_df,
      tax.euk_binary_summary_df,
      tax.all_binary_summary_df,
      tax.subset.spec,
      tax.subset.sens,
      tax.subset.prec,
      tax.subset.f1,
      tax.subset.acc
    )
  )
}
```


To evaluate the difference between the taxonomic kingdoms per CAZy class, the data was separated into each of the CAZy classes. The F1-score was then plotted as a one-dimensional scatter plot overlaying a boxplot, with data grouped by the taxonomic kingdom and facet wrapped by classifier.

### Difference in taxonomic performance for GH classification

Figure \@ref{fig:ghClassTax} plots a summary the difference in performance between bacterial and eukaryota GH class members.

Overall, the classifiers demonstrated similar performances between the bacterial and eukaryotic test sets. eCAMI showed the greater difference in performance between bacteria and eukaryotes, demonstrating a more consistent perforamnce against bacterial proteins, as inferred from the smaller interquartile range.

```{r ghClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying GH class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}

p.tax.gh <- plot.class.tax.comparison(class.tax.gh)
p.tax.gh
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:ghTaxClassSummary)) and eukaryota (table \@ref(fig:ghTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:ghTaxClassAllTable)).

#### Specificity 

```{r ghTaxClassSummary, echo=FALSE}
tax.gh_class_dfs <- get.tax.class.summary.tables(class.tax.gh)
tax.gh_bact.binary_summary_df <- tax.gh_class_dfs[[1]]
tax.gh_euk.binary_summary_df <- tax.gh_class_dfs[[2]]
tax.gh_all.binary_summary_df <- tax.gh_class_dfs[[3]]
tax.gh_subset_spec <- tax.gh_class_dfs[[4]]
tax.gh_subset_sens <- tax.gh_class_dfs[[5]]
tax.gh_subset_prec <- tax.gh_class_dfs[[6]]
tax.gh_subset_f1 <- tax.gh_class_dfs[[7]]
tax.gh_subset_acc <- tax.gh_class_dfs[[8]]

kable(tax.gh_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial GH class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```


```{r ghTaxClassEukTable, echo=FALSE}
kable(tax.gh_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote GH class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r ghTaxClassAllTable, echo=FALSE}
kable(tax.gh_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote GH class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:ghTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:ghTaxClassClassificationSensitivity)), precision (\@ref(fig:ghTaxClassClassificationPrecision)), F1-score (\@ref(fig:ghTaxClassClassificationF1)), and accuracy (\@ref(fig:ghTaxClassClassificationAccuracy)).

```{r ghTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of GH class members, overlaying a box plot"}
tax.gh_subset_spec$Prediction_tool <- factor(tax.gh_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.tax.class.spec = ggplot(tax.gh_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.gh.tax.class.spec
```

```{r ghTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of GH class members, overlaying a box plot"}
tax.gh_subset_sens$Prediction_tool <- factor(tax.gh_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.tax.class.sens = ggplot(tax.gh_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.gh.tax.class.sens
```

```{r ghTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of GH class members, overlaying a box plot"}
tax.gh_subset_prec$Prediction_tool <- factor(tax.gh_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.tax.class.prec = ggplot(tax.gh_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.gh.tax.class.prec
```

```{r ghTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of GH class members, overlaying a box plot"}
tax.gh_subset_f1$Prediction_tool <- factor(tax.gh_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.tax.class.f1 = ggplot(tax.gh_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.gh.tax.class.f1
```

```{r ghTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of GH class members, overlaying a box plot"}
tax.gh_subset_acc$Prediction_tool <- factor(tax.gh_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.gh.tax.class.acc = ggplot(tax.gh_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "right",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.gh.tax.class.acc
```

### Difference in taxonomic performance for GT classification

Figure \@ref{fig:gtClassTax} plots the difference in performance between bacterial and eukaryota GT class members. Hotpep demonstrates the greatest difference in performance between bacteria and eukaryotes, with a more consistent performance for eukaryotes as inferred from the smaller interquartile ranage. Otherwise, there was not significant difference between performance against the two kingdoms.

```{r gtClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying GT class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}
p.tax.gt <- plot.class.tax.comparison(class.tax.gt)
p.tax.gt
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:gtTaxClassSummary)) and eukaryota (table \@ref(fig:gtTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:gtTaxClassAllTable)).

```{r gtTaxClassSummary, echo=FALSE}
tax.gt_class_dfs <- get.tax.class.summary.tables(class.tax.gt)
tax.gt_bact.binary_summary_df <- tax.gt_class_dfs[[1]]
tax.gt_euk.binary_summary_df <- tax.gt_class_dfs[[2]]
tax.gt_all.binary_summary_df <- tax.gt_class_dfs[[3]]
tax.gt_subset_spec <- tax.gt_class_dfs[[4]]
tax.gt_subset_sens <- tax.gt_class_dfs[[5]]
tax.gt_subset_prec <- tax.gt_class_dfs[[6]]
tax.gt_subset_f1 <- tax.gt_class_dfs[[7]]
tax.gt_subset_acc <- tax.gt_class_dfs[[8]]

kable(tax.gt_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial GT class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r gtTaxClassEukTable, echo=FALSE}
kable(tax.gt_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote GT class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r gtTaxClassAllTable, echo=FALSE}
kable(tax.gt_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote GT class members", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:gtTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:gtTaxClassClassificationSensitivity)), precision (\@ref(fig:gtTaxClassClassificationPrecision)), F1-score (\@ref(fig:gtTaxClassClassificationF1)), and accuracy (\@ref(fig:gtTaxClassClassificationAccuracy)).

```{r gtTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of GT class members, overlaying a box plot"}
tax.gt_subset_spec$Prediction_tool <- factor(tax.gt_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.tax.class.spec = ggplot(tax.gt_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "rigtt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.gt.tax.class.spec
```

```{r gtTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of GT class members, overlaying a box plot"}
tax.gt_subset_sens$Prediction_tool <- factor(tax.gt_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.tax.class.sens = ggplot(tax.gt_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "rigtt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.gt.tax.class.sens
```

```{r gtTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of GT class members, overlaying a box plot"}
tax.gt_subset_prec$Prediction_tool <- factor(tax.gt_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.tax.class.prec = ggplot(tax.gt_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "rigtt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.gt.tax.class.prec
```

```{r gtTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of GT class members, overlaying a box plot"}
tax.gt_subset_f1$Prediction_tool <- factor(tax.gt_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.tax.class.f1 = ggplot(tax.gt_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "rigtt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.gt.tax.class.f1
```

```{r gtTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of GT class members, overlaying a box plot"}
tax.gt_subset_acc$Prediction_tool <- factor(tax.gt_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.gt.tax.class.acc = ggplot(tax.gt_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "rigtt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.gt.tax.class.acc
```

### Difference in taxonomic performance for PL classification

Figure \@ref{fig:plClassTax} plots the difference in performance between bacterial and eukaryota PL class members. Most classifiers showed a strong consistency in performance between the bacterial and eukaryotic test sets (as inferred from the small interquartile ranges), except eCAMI which showed a signficantly greater range in performance when classifying bacterial proteins.

```{r plClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying PL class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}
p.tax.pl <- plot.class.tax.comparison(class.tax.pl)
p.tax.pl
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:plTaxClassSummary)) and eukaryota (table \@ref(fig:plTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:plTaxClassAllTable)).

```{r plTaxClassSummary, echo=FALSE}
tax.pl_class_dfs <- get.tax.class.summary.tables(class.tax.pl)
tax.pl_bact.binary_summary_df <- tax.pl_class_dfs[[1]]
tax.pl_euk.binary_summary_df <- tax.pl_class_dfs[[2]]
tax.pl_all.binary_summary_df <- tax.pl_class_dfs[[3]]
tax.pl_subset_spec <- tax.pl_class_dfs[[4]]
tax.pl_subset_sens <- tax.pl_class_dfs[[5]]
tax.pl_subset_prec <- tax.pl_class_dfs[[6]]
tax.pl_subset_f1 <- tax.pl_class_dfs[[7]]
tax.pl_subset_acc <- tax.pl_class_dfs[[8]]

kable(tax.pl_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial PL class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r plTaxClassEukTable, echo=FALSE}
kable(tax.pl_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote PL class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r plTaxClassAllTable, echo=FALSE}
kable(tax.pl_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote PL class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:plTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:plTaxClassClassificationSensitivity)), precision (\@ref(fig:plTaxClassClassificationPrecision)), F1-score (\@ref(fig:plTaxClassClassificationF1)), and accuracy (\@ref(fig:plTaxClassClassificationAccuracy)).

```{r plTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of PL class members, overlaying a box plot"}
tax.pl_subset_spec$Prediction_tool <- factor(tax.pl_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.tax.class.spec = ggplot(tax.pl_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riplt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.pl.tax.class.spec
```

```{r plTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of PL class members, overlaying a box plot"}
tax.pl_subset_sens$Prediction_tool <- factor(tax.pl_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.tax.class.sens = ggplot(tax.pl_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riplt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.pl.tax.class.sens
```

```{r plTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of PL class members, overlaying a box plot"}
tax.pl_subset_prec$Prediction_tool <- factor(tax.pl_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.tax.class.prec = ggplot(tax.pl_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riplt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.pl.tax.class.prec
```

```{r plTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of PL class members, overlaying a box plot"}
tax.pl_subset_f1$Prediction_tool <- factor(tax.pl_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.tax.class.f1 = ggplot(tax.pl_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riplt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.pl.tax.class.f1
```

```{r plTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of PL class members, overlaying a box plot"}
tax.pl_subset_acc$Prediction_tool <- factor(tax.pl_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.pl.tax.class.acc = ggplot(tax.pl_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riplt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.pl.tax.class.acc
```

### Difference in taxonomic performance for CE classification

Figure \@ref{fig:ceClassTax} plots the difference in performance between bacterial and eukaryota PL class members. Most classifiers showed a strong consistency in performance between the bacterial and eukaryotic test sets (as inferred from the small interquartile ranges), except eCAMI which showed a signficantly greater range in performance when classifying bacterial proteins.

```{r ceClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying CE class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}
p.tax.ce <- plot.class.tax.comparison(class.tax.ce)
p.tax.ce
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:ceTaxClassSummary)) and eukaryota (table \@ref(fig:ceTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:ceTaxClassAllTable)).

```{r ceTaxClassSummary, echo=FALSE}
tax.ce_class_dfs <- get.tax.class.summary.tables(class.tax.ce)
tax.ce_bact.binary_summary_df <- tax.ce_class_dfs[[1]]
tax.ce_euk.binary_summary_df <- tax.ce_class_dfs[[2]]
tax.ce_all.binary_summary_df <- tax.ce_class_dfs[[3]]
tax.ce_subset_spec <- tax.ce_class_dfs[[4]]
tax.ce_subset_sens <- tax.ce_class_dfs[[5]]
tax.ce_subset_prec <- tax.ce_class_dfs[[6]]
tax.ce_subset_f1 <- tax.ce_class_dfs[[7]]
tax.ce_subset_acc <- tax.ce_class_dfs[[8]]

kable(tax.ce_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial CE class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ceTaxClassEukTable, echo=FALSE}
kable(tax.ce_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote CE class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r ceTaxClassAllTable, echo=FALSE}
kable(tax.ce_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote CE class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:ceTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:ceTaxClassClassificationSensitivity)), precision (\@ref(fig:ceTaxClassClassificationPrecision)), F1-score (\@ref(fig:ceTaxClassClassificationF1)), and accuracy (\@ref(fig:ceTaxClassClassificationAccuracy)).

```{r ceTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of CE class members, overlaying a box plot"}
tax.ce_subset_spec$Prediction_tool <- factor(tax.ce_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.tax.class.spec = ggplot(tax.ce_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricet",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.ce.tax.class.spec
```

```{r ceTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of CE class members, overlaying a box plot"}
tax.ce_subset_sens$Prediction_tool <- factor(tax.ce_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.tax.class.sens = ggplot(tax.ce_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricet",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.ce.tax.class.sens
```

```{r ceTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of CE class members, overlaying a box plot"}
tax.ce_subset_prec$Prediction_tool <- factor(tax.ce_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.tax.class.prec = ggplot(tax.ce_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricet",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.ce.tax.class.prec
```

```{r ceTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of CE class members, overlaying a box plot"}
tax.ce_subset_f1$Prediction_tool <- factor(tax.ce_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.tax.class.f1 = ggplot(tax.ce_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricet",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.ce.tax.class.f1
```

```{r ceTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of CE class members, overlaying a box plot"}
tax.ce_subset_acc$Prediction_tool <- factor(tax.ce_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.ce.tax.class.acc = ggplot(tax.ce_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricet",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.ce.tax.class.acc
```

### Difference in taxonomic performance for AA classification

Figure \@ref{fig:aaClassTax} plots the difference in performance between bacterial and eukaryota AA class members. As inferred from comparing the interquartile ranges, all classifiers demonstrates a more consistent performance against bacterial than eukaryotic AA class members. However, this most likely due to the AA class predominately containing eukaryotic proteins. Therefore, it is relatively 'easier' for a classifier to determine a bacterial protein does not belong to the class because there is low sequence similarity between bacterial proteins and the representative models of the AA class, which over represents eukaryotic proteins. Additionally, with fewer bacterial AA proteins, there are fewer oppurtunities for the classifier to miss classify a AA member as a non-AA member, resulting in a more consistent higher F1-score than eukaryotes, which have many opprutnities for miss classification of AA members as non-AA members.

```{r aaClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying AA class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}
p.tax.aa <- plot.class.tax.comparison(class.tax.aa)
p.tax.aa
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:aaTaxClassSummary)) and eukaryota (table \@ref(fig:aaTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:aaTaxClassAllTable)).

```{r aaTaxClassSummary, echo=FALSE}
tax.aa_class_dfs <- get.tax.class.summary.tables(class.tax.aa)
tax.aa_bact.binary_summary_df <- tax.aa_class_dfs[[1]]
tax.aa_euk.binary_summary_df <- tax.aa_class_dfs[[2]]
tax.aa_all.binary_summary_df <- tax.aa_class_dfs[[3]]
tax.aa_subset_spec <- tax.aa_class_dfs[[4]]
tax.aa_subset_sens <- tax.aa_class_dfs[[5]]
tax.aa_subset_prec <- tax.aa_class_dfs[[6]]
tax.aa_subset_f1 <- tax.aa_class_dfs[[7]]
tax.aa_subset_acc <- tax.aa_class_dfs[[8]]

kable(tax.aa_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial AA class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r aaTaxClassEukTable, echo=FALSE}
kable(tax.aa_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote AA class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r aaTaxClassAllTable, echo=FALSE}
kable(tax.aa_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote AA class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:aaTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:aaTaxClassClassificationSensitivity)), precision (\@ref(fig:aaTaxClassClassificationPrecision)), F1-score (\@ref(fig:aaTaxClassClassificationF1)), and accuracy (\@ref(fig:aaTaxClassClassificationAccuracy)).

```{r aaTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of AA class members, overlaying a box plot"}
tax.aa_subset_spec$Prediction_tool <- factor(tax.aa_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.tax.class.spec = ggplot(tax.aa_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riaat",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.aa.tax.class.spec
```

```{r aaTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of AA class members, overlaying a box plot"}
tax.aa_subset_sens$Prediction_tool <- factor(tax.aa_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.tax.class.sens = ggplot(tax.aa_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riaat",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.aa.tax.class.sens
```

```{r aaTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of AA class members, overlaying a box plot"}
tax.aa_subset_prec$Prediction_tool <- factor(tax.aa_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.tax.class.prec = ggplot(tax.aa_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riaat",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.aa.tax.class.prec
```

```{r aaTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of AA class members, overlaying a box plot"}
tax.aa_subset_f1$Prediction_tool <- factor(tax.aa_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.tax.class.f1 = ggplot(tax.aa_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riaat",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.aa.tax.class.f1
```

```{r aaTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of AA class members, overlaying a box plot"}
tax.aa_subset_acc$Prediction_tool <- factor(tax.aa_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.aa.tax.class.acc = ggplot(tax.aa_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "riaat",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.aa.tax.class.acc
```

### Difference in taxonomic performance for CBM classification

Figure \@ref{fig:cbmClassTax} plots the difference in performance between bacterial and eukaryota CBM class members. Most classifiers demonstrated a greater variation in performance against eukaryotic than bacterial proteins, which may be the result of greater sequence diversity within the eukaryotic CBMs than bacterial CBMs.

```{r cbmClassTax, echo=FALSE, fig.cap="One dimensional scatter plot overlaying a box and whisker plot of the F1-score of classifying CBM class members for CAZyme classifiers, when parsing data from bacterial, eukaryote or both (identified as 'all') kingdoms. One point on the scatter plot represents the F1-score for one test set."}
p.tax.cbm <- plot.class.tax.comparison(class.tax.cbm)
p.tax.cbm
```

The following tables summarise the performance for each classifier across all test sets for each taxonomic group (bacteria (table \@ref(fig:cbmTaxClassSummary)) and eukaryota (table \@ref(fig:cbmTaxClassEukTable))), and when all test sets are pooled (which is assinged the taxonomic group 'All') (table \@ref(fig:cbmTaxClassAllTable)).

```{r cbmTaxClassSummary, echo=FALSE}
tax.cbm_class_dfs <- get.tax.class.summary.tables(class.tax.cbm)
tax.cbm_bact.binary_summary_df <- tax.cbm_class_dfs[[1]]
tax.cbm_euk.binary_summary_df <- tax.cbm_class_dfs[[2]]
tax.cbm_all.binary_summary_df <- tax.cbm_class_dfs[[3]]
tax.cbm_subset_spec <- tax.cbm_class_dfs[[4]]
tax.cbm_subset_sens <- tax.cbm_class_dfs[[5]]
tax.cbm_subset_prec <- tax.cbm_class_dfs[[6]]
tax.cbm_subset_f1 <- tax.cbm_class_dfs[[7]]
tax.cbm_subset_acc <- tax.cbm_class_dfs[[8]]

kable(tax.cbm_bact.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial CBM class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r cbmTaxClassEukTable, echo=FALSE}
kable(tax.cbm_euk.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of eukaryote CBM class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r cbmTaxClassAllTable, echo=FALSE}
kable(tax.cbm_all.binary_summary_df, caption="Overall performance of CAZyme classifiers for the classification of bacterial and eukaryote CBM class members", align='c', digits = 4) %>% kable_styling(full_width = F)
```

The following plots present the performance for each classifier for each test set for the following performance statistics: specificity (figure \@ref(fig:cbmTaxClassClassificationSpecificity)), sensitivity (\@ref(fig:cbmTaxClassClassificationSensitivity)), precision (\@ref(fig:cbmTaxClassClassificationPrecision)), F1-score (\@ref(fig:cbmTaxClassClassificationF1)), and accuracy (\@ref(fig:cbmTaxClassClassificationAccuracy)).

```{r cbmTaxClassClassificationSpecificity, echo=FALSE, fig.cap="One dimensional scatter plot of the specificity per test set for the classification of CBM class members, overlaying a box plot"}
tax.cbm_subset_spec$Prediction_tool <- factor(tax.cbm_subset_spec$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.tax.class.spec = ggplot(tax.cbm_subset_spec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricbmt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Specificty") +
  facet_wrap(~Prediction_tool)
p.cbm.tax.class.spec
```

```{r cbmTaxClassClassificationSensitivity, echo=FALSE, fig.cap="One dimensional scatter plot of the sensitivity per test set for the classification of CBM class members, overlaying a box plot"}
tax.cbm_subset_sens$Prediction_tool <- factor(tax.cbm_subset_sens$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.tax.class.sens = ggplot(tax.cbm_subset_sens %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricbmt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Sensitivity") +
  facet_wrap(~Prediction_tool)
p.cbm.tax.class.sens
```

```{r cbmTaxClassClassificationPrecision, echo=FALSE, fig.cap="One dimensional scatter plot of the precision per test set for the classification of CBM class members, overlaying a box plot"}
tax.cbm_subset_prec$Prediction_tool <- factor(tax.cbm_subset_prec$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.tax.class.prec = ggplot(tax.cbm_subset_prec %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricbmt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Precision") +
  facet_wrap(~Prediction_tool)
p.cbm.tax.class.prec
```

```{r cbmTaxClassClassificationF1, echo=FALSE, fig.cap="One dimensional scatter plot of the F1-score per test set for the classification of CBM class members, overlaying a box plot"}
tax.cbm_subset_f1$Prediction_tool <- factor(tax.cbm_subset_f1$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.tax.class.f1 = ggplot(tax.cbm_subset_f1 %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricbmt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("F1-score") +
  facet_wrap(~Prediction_tool)
p.cbm.tax.class.f1
```

```{r cbmTaxClassClassificationAccuracy, echo=FALSE, fig.cap="One dimensional scatter plot of the accuracy per test set for the classification of CBM class members, overlaying a box plot"}
tax.cbm_subset_acc$Prediction_tool <- factor(tax.cbm_subset_acc$Prediction_tool, levels = classifiers) # set order data is presented

p.cbm.tax.class.acc = ggplot(tax.cbm_subset_acc %>% dplyr::group_by(Tax_group),
                             aes(x=Tax_group, y=Statistic_value, fill=Tax_group)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "ricbmt",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Kingdom") + 
  ylab("Accuracy") +
  facet_wrap(~Prediction_tool)
p.cbm.tax.class.acc
```

### Multilabel classification of CAZy classes

To represent the overall CAZy class classification performance, and take into consideration of CAZy class multi-label classification, the Rand Index was calculated for each taxonomy group per CAZy classifier.

```{r taxClassRI, echo=FALSE}
bact.class_tax_ri_ari_df <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Bacteria"), ]
euk.class_tax_ri_ari_df <- class_tax_ri_ari_df[which(class_tax_ri_ari_df$Tax_group == "Eukaryote"), ]

all.vector <- rep("All", nrow(class_tax_ri_ari_df))
all.class_tax_ri_ari_df <- class_tax_ri_ari_df
all.class_tax_ri_ari_df$Tax_group <- all.vector

stat.bact.class_tax_ri_ari_df <- bact.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Rand_index), "Bact Standard Deviation"=sd(Rand_index),
  "Bact Lower CI"=CI(Rand_index)[3], "Bact Upper CI"=CI(Rand_index)[1]
  )

stat.euk.class_tax_ri_ari_df <- euk.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3], "Upper CI"=CI(Rand_index)[1]
  )

stat.all.class_tax_ri_ari_df <- all.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3], "Upper CI"=CI(Rand_index)[1]
  )


tax.class_tax_ri_df <- stat.bact.class_tax_ri_ari_df

tax.class_tax_ri_df$`Euk Mean` <- stat.euk.class_tax_ri_ari_df$Mean
tax.class_tax_ri_df$`Euk Standard Deviation` <- stat.euk.class_tax_ri_ari_df$`Standard Deviation`
tax.class_tax_ri_df$`Euk Lower CI` <- stat.euk.class_tax_ri_ari_df$`Lower CI`
tax.class_tax_ri_df$`Euk Upper CI` <- stat.euk.class_tax_ri_ari_df$`Upper CI`

tax.class_tax_ri_df$`All Mean` <- stat.all.class_tax_ri_ari_df$Mean
tax.class_tax_ri_df$`All Standard Deviation` <- stat.all.class_tax_ri_ari_df$`Standard Deviation`
tax.class_tax_ri_df$`All Lower CI` <- stat.euk.class_tax_ri_ari_df$`Lower CI`
tax.class_tax_ri_df$`All Upper CI` <- stat.euk.class_tax_ri_ari_df$`Upper CI`

tax.class_tax_ri_df$Prediction_tool <- factor(tax.class_tax_ri_df$Prediction_tool, levels=classifiers) # set order data is presented
kable(tax.class_tax_ri_df, caption="Overall performance of CAZy class classification (represented by the Rand Index) by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)

```

The Adjusted Rand Index was also calculated in order to take into consideration chance.

```{r taxClassARI, echo=FALSE}

stat.bact.class_tax_ri_ari_df <- bact.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Adjusted_Rand_index), "Bact Standard Deviation"=sd(Adjusted_Rand_index),
  "Bact Lower CI"=CI(Adjusted_Rand_index)[3], "Bact Upper CI"=CI(Adjusted_Rand_index)[1]
)

stat.euk.class_tax_ri_ari_df <- euk.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3], "Upper CI"=CI(Adjusted_Rand_index)[1]
)

stat.all.class_tax_ri_ari_df <- all.class_tax_ri_ari_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3], "Upper CI"=CI(Adjusted_Rand_index)[1]
)


tax.class_tax_ari_df <- stat.bact.class_tax_ri_ari_df

tax.class_tax_ari_df$`Euk Mean` <- stat.euk.class_tax_ri_ari_df$Mean
tax.class_tax_ari_df$`Euk Standard Deviation` <- stat.euk.class_tax_ri_ari_df$`Standard Deviation`
tax.class_tax_ari_df$`Euk Lower CI` <- stat.euk.class_tax_ri_ari_df$`Lower CI`
tax.class_tax_ari_df$`Euk Upper CI` <- stat.euk.class_tax_ri_ari_df$`Upper CI`

tax.class_tax_ari_df$`All Mean` <- stat.all.class_tax_ri_ari_df$Mean
tax.class_tax_ari_df$`All Standard Deviation` <- stat.all.class_tax_ri_ari_df$`Standard Deviation`
tax.class_tax_ari_df$`All Lower CI` <- stat.euk.class_tax_ri_ari_df$`Lower CI`
tax.class_tax_ari_df$`All Upper CI` <- stat.euk.class_tax_ri_ari_df$`Upper CI`

tax.class_tax_ari_df$Prediction_tool <- factor(tax.class_tax_ari_df$Prediction_tool, levels=classifiers) # set order data is presented
kable(tax.class_tax_ari_df, caption="Overall performance of CAZy class classification (represented by the Adjusted Rand Index) by CAZy classifiers per taxonomy group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

## CAZy family classification

```{r famRiTax, echo=FALSE}
fam_classification_df_tax.bact <- fam_classification_df_tax[which(fam_classification_df_tax$Tax_group == "Bacteria"), ]
fam_classification_df_tax.euk <- fam_classification_df_tax[which(fam_classification_df_tax$Tax_group == "Eukaryote"), ]

all.vector <- rep("All", nrow(fam_classification_df_tax))
fam_classification_df_tax.all <- fam_classification_df_tax
fam_classification_df_tax.all$Tax_group <- all.vector

fam_classification_df_tax.ri.bact <- fam_classification_df_tax.bact %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Rand_index), "Bact Standard Deviation"=sd(Rand_index),
  "Bact Lower CI"=CI(Rand_index)[3], "Bact Upper CI"=CI(Rand_index)[1]
  )
fam_classification_df_tax.ri.euk <- fam_classification_df_tax.euk %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3], "Upper CI"=CI(Rand_index)[1]
  )
fam_classification_df_tax.ri.all <- fam_classification_df_tax.all %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3], "Upper CI"=CI(Rand_index)[1]
  )

fam_classification_df_tax.ri.complete <- fam_classification_df_tax.ri.bact

fam_classification_df_tax.ri.complete$`Euk Mean` <- fam_classification_df_tax.ri.euk$`Mean`
fam_classification_df_tax.ri.complete$`Euk Standard Deviation` <- fam_classification_df_tax.ri.euk$`Standard Deviation`
fam_classification_df_tax.ri.complete$`Euk Lower CI` <- fam_classification_df_tax.ri.euk$`Lower CI`
fam_classification_df_tax.ri.complete$`Euk Upper CI` <- fam_classification_df_tax.ri.euk$`Upper CI`

fam_classification_df_tax.ri.complete$`All Mean` <- fam_classification_df_tax.ri.all$`Mean`
fam_classification_df_tax.ri.complete$`All Standard Deviation` <- fam_classification_df_tax.ri.all$`Standard Deviation`
fam_classification_df_tax.ri.complete$`All Lower CI` <- fam_classification_df_tax.ri.all$`Lower CI`
fam_classification_df_tax.ri.complete$`All Upper CI` <- fam_classification_df_tax.ri.all$`Upper CI`

fam_classification_df_tax.ri.complete$Prediction_tool <- factor(fam_classification_df_tax.ri.complete$Prediction_tool, levels=classifiers) # set order data is presented

kable(fam_classification_df_tax.ri.complete, caption="Rand Index of CAZyme classifier classification of CAZy family annotations per taxonomt group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```

```{r famAriCalcTax, echo=FALSE}
fam_classification_df_tax.ari.bact <- fam_classification_df_tax.bact %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Bact Mean"=mean(Adjusted_Rand_index), "Bact Standard Deviation"=sd(Adjusted_Rand_index),
  "Bact Lower CI"=CI(Adjusted_Rand_index)[3], "Bact Upper CI"=CI(Adjusted_Rand_index)[1]
  )
fam_classification_df_tax.ari.euk <- fam_classification_df_tax.euk %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3], "Upper CI"=CI(Adjusted_Rand_index)[1]
  )
fam_classification_df_tax.ari.all <- fam_classification_df_tax.all %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3], "Upper CI"=CI(Adjusted_Rand_index)[1]
  )

fam_classification_df_tax.ari.complete <- fam_classification_df_tax.ari.bact
fam_classification_df_tax.ari.complete$`Eukaryote Mean` <- fam_classification_df_tax.ari.euk$`Mean`
fam_classification_df_tax.ari.complete$`Eukaryote Standard Deviation` <- fam_classification_df_tax.ari.euk$`Standard Deviation`
fam_classification_df_tax.ari.complete$`Euk Lower CI` <- fam_classification_df_tax.ari.euk$`Lower CI`
fam_classification_df_tax.ari.complete$`Euk Upper CI` <- fam_classification_df_tax.ari.euk$`Upper CI`

fam_classification_df_tax.ari.complete$`All Mean` <- fam_classification_df_tax.ari.all$`Mean`
fam_classification_df_tax.ari.complete$`All Standard Deviation` <- fam_classification_df_tax.ari.all$`Standard Deviation`
fam_classification_df_tax.ari.complete$`All Lower CI` <- fam_classification_df_tax.ari.all$`Lower CI`
fam_classification_df_tax.ari.complete$`All Upper CI` <- fam_classification_df_tax.ari.all$`Upper CI`

fam_classification_df_tax.ari.complete$Prediction_tool <- factor(fam_classification_df_tax.ari.complete$Prediction_tool, levels=classifiers) # set order data is presented

kable(fam_classification_df_tax.ari.complete, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy family annotations per taxonomt group", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13), border_right=TRUE)
```


# Evaluation of (re)combining tools

Often, classifiers are not used in isolation. Frequently, classifiers are combined to produce an overall more accurate classifier. An example of this is dbCAN. dbCAN contains the classifiers HMMER, Hotpep and DIAMOND, the consensus classifications of these classifiers are interpreted as the output for dbCAN.

Defining new combinations of classifiers may reveal a combination that is more accurate than existing combinations and/or using the tools in isolation.

The following combinations of tools were evaluted:
- HMMER, DIAMOND and CUPP
- HMMER, DIAMOND and eCAMI

## Binary classification

Table \@ref{sumstatsRecombined} contains the summary statistics for the binary classification of proteins, for the inividual and combined classifiers.

```{r RTsumstatsRecombined, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. Data collected is the mean of call calculated statistical parameters across all test sets, plus and minus the standard devliation. All figures are rounded to 4 decimal places."}

rt.binary_dfs <- get_binary_summary_stats(recombined_tools_binary_df)
rt.binary_summary_df <- rt.binary_dfs[[1]]
rt.subset_spec <- rt.binary_dfs[[2]]
rt.subset_sens <- rt.binary_dfs[[3]]
rt.subset_prec <- rt.binary_dfs[[4]]
rt.subset_f1 <- rt.binary_dfs[[5]]
rt.subset_acc <- rt.binary_dfs[[6]]

names(rt.binary_summary_df)[names(rt.binary_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows
rt.binary_summary_df$Classifier <- factor(rt.binary_summary_df$Classifier, levels = recombined_classifiers_abbrev) # set order data is presented

kable(rt.binary_summary_df, caption="Overall performance of CAZyme classifiers differentiation between CAZymes and non-CAZymes", align='c', digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

```{r rEbinaryCI, echo=FALSE, fig.cap="Summary statistics of CAZyme classifiers performances of binary CAZyme/non-CAZyme prediction. The mean plus and minus the 95% confidence interval."}
cal_tool_ci <- function(stat_subset){
  # calculate lower, upper and mean 95% CI
  UpperCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
  MeanCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
  LowerCI <- stat_subset %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])

  ci_df <- merge(MeanCI, UpperCI)
  ci_df <- merge(ci_df, LowerCI)
  
  return(ci_df)
}

rt.spec.ci <- cal_tool_ci(rt.subset_spec)
rt.spec.ci$Statistic_parameter <- rep('Specificity', nrow(rt.spec.ci))
rt.sens.ci <- cal_tool_ci(rt.subset_sens)
rt.sens.ci$Statistic_parameter <- rep('Sensitivity', nrow(rt.sens.ci))
rt.prec.ci <- cal_tool_ci(rt.subset_prec)
rt.prec.ci$Statistic_parameter <- rep('Precision', nrow(rt.prec.ci))
rt.f1.ci <- cal_tool_ci(rt.subset_f1)
rt.f1.ci$Statistic_parameter <- rep('F1-score', nrow(rt.f1.ci))
rt.acc.ci <- cal_tool_ci(rt.subset_acc)
rt.acc.ci$Statistic_parameter <- rep('Accuracy', nrow(rt.acc.ci))

rt.ci_df <- rbind(rt.spec.ci, rt.sens.ci)
rt.ci_df <- rbind(rt.ci_df, rt.prec.ci)
rt.ci_df <- rbind(rt.ci_df, rt.f1.ci)
rt.ci_df <- rbind(rt.ci_df, rt.acc.ci)

rt.ci_df$Prediction_tool <- factor(rt.ci_df$Prediction_tool, levels = recombined_classifiers_abbrev)
rt.ci_df$Statistic_parameter <- factor(rt.ci_df$Statistic_parameter, levels = c("Specificity", "Sensitivity", "Precision", "F1-score", "Accuracy"))

rt.bin.CI = ggplot(rt.ci_df %>% dplyr::group_by(Statistic_parameter),
                aes(x=Prediction_tool, y=MeanCI, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_colour_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Statistical value") +
  facet_wrap(~ Statistic_parameter, ncol=2)
rt.bin.CI
```

Figure \@ref{RTstatsRecombined} presents the distribution of statistical parameters per CAZyme classifer (including recombined classifiers) for each statistical parameter for evaluating differentiation of CAZymes and non-CAZymes.

```{r RTstatsRecombined, echo=FALSE, fig.cap="Proportional area plot of the disitrubution of statistical parameters across all test sets."}

# rename prediction tools
recombined_tools_binary_df$Prediction_tool <- str_replace(recombined_tools_binary_df$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
recombined_tools_binary_df$Prediction_tool <- str_replace(recombined_tools_binary_df$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")

# classify the Fbeta-scores into bins
val.class <- vector()

for(i in 1:nrow(recombined_tools_binary_df)) {
  if(recombined_tools_binary_df[i, 5] == 1){val.class <- append(val.class, '[1.00]')} 
  else if (recombined_tools_binary_df[i, 5] == 0){val.class <- append(val.class, '[0.00]')}
  else if (recombined_tools_binary_df[i, 5] < 1 && recombined_tools_binary_df[i, 5] >= 0.95){val.class <- append(val.class, '(0.95, 1.00]')}
  else if (recombined_tools_binary_df[i, 5] < 0.95 && recombined_tools_binary_df[i, 5] >= 0.9){val.class <- append(val.class, '(0.90, 0.95]')}
  else if (recombined_tools_binary_df[i, 5] < 0.90 && recombined_tools_binary_df[i, 5] >= 0.85){val.class <- append(val.class, '(0.85, 0.90]')}
  else if (recombined_tools_binary_df[i, 5] < 0.85 && recombined_tools_binary_df[i, 5] >= 0.80){val.class <- append(val.class, '(0.80, 0.85]')}
  else if (recombined_tools_binary_df[i, 5] < 0.80 && recombined_tools_binary_df[i, 5] >= 0.75){val.class <- append(val.class, '(0.75, 0.80]')}
  else if (recombined_tools_binary_df[i, 5] < 0.75 && recombined_tools_binary_df[i, 5] >= 0.70){val.class <- append(val.class, '(0.70, 0.75]')}
  else if (recombined_tools_binary_df[i, 5] < 0.70 && recombined_tools_binary_df[i, 5] >= 0.65){val.class <- append(val.class, '(0.65, 0.70]')}
  else if (recombined_tools_binary_df[i, 5] < 0.65 && recombined_tools_binary_df[i, 5] >= 0.60){val.class <- append(val.class, '(0.60, 0.65]')}
  else if (recombined_tools_binary_df[i, 5] < 0.60 && recombined_tools_binary_df[i, 5] >= 0.55){val.class <- append(val.class, '(0.55, 0.60]')}
  else if (recombined_tools_binary_df[i, 5] < 0.55 && recombined_tools_binary_df[i, 5] >= 0.50){val.class <- append(val.class, '(0.50, 0.55]')}
  else if (recombined_tools_binary_df[i, 5] < 0.50 && recombined_tools_binary_df[i, 5] >= 0.45){val.class <- append(val.class, '(0.45, 0.50]')}
  else if (recombined_tools_binary_df[i, 5] < 0.45 && recombined_tools_binary_df[i, 5] >= 0.40){val.class <- append(val.class, '(0.40, 0.45]')}
  else if (recombined_tools_binary_df[i, 5] < 0.40 && recombined_tools_binary_df[i, 5] >= 0.35){val.class <- append(val.class, '(0.35, 0.40]')}
  else if (recombined_tools_binary_df[i, 5] < 0.35 && recombined_tools_binary_df[i, 5] >= 0.30){val.class <- append(val.class, '(0.30, 0.35]')}
  else if (recombined_tools_binary_df[i, 5] < 0.30 && recombined_tools_binary_df[i, 5] >= 0.25){val.class <- append(val.class, '(0.25, 0.30]')}
  else if (recombined_tools_binary_df[i, 5] < 0.25 && recombined_tools_binary_df[i, 5] >= 0.20){val.class <- append(val.class, '(0.20, 0.25]')}
  else if (recombined_tools_binary_df[i, 5] < 0.20 && recombined_tools_binary_df[i, 5] >= 0.15){val.class <- append(val.class, '(0.15, 0.20]')}
  else if (recombined_tools_binary_df[i, 5] < 0.15 && recombined_tools_binary_df[i, 5] >= 0.10){val.class <- append(val.class, '(0.10, 0.15]')}
  else if (recombined_tools_binary_df[i, 5] < 0.10 && recombined_tools_binary_df[i, 5] >= 0.05){val.class <- append(val.class, '(0.05, 0.10]')}
  else if (recombined_tools_binary_df[i, 5] < 0.05 && recombined_tools_binary_df[i, 5] >= 0){val.class <- append(val.class, '(0.00, 0.05]')}
  else {val.class <- append(val.class, '< 0')}
}
recombined_tools_binary_df$val.class <- val.class

# set order data is presented
recombined_tools_binary_df$Prediction_tool <- factor(recombined_tools_binary_df$Prediction_tool, levels = recombined_classifiers_abbrev)

recombined_tools_binary_df$CAZy_class <- factor(recombined_tools_binary_df$Statistic_parameter, levels = c('Specificity', 'Sensitivity', 'Precision', 'FBeta-score', 'Accuracy'))

recombined_tools_binary_df$val.class <- factor(recombined_tools_binary_df$val.class, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

parea.colours <- c(
  "#1c0a00", "#620021", "#940113",
  "#D73027", "#F38345", "#FDBA67",
  "#FEE168", "#228537",
  "#1ba841", "#3fcc3f", "#6ddb1a",
  "#a5f200")

p.recombined.binary.c.nc.parea = ggally_count(recombined_tools_binary_df, mapping=ggplot2::aes(x=Prediction_tool, y=Statistic_parameter, fill=val.class)) +
  scale_fill_manual(values = parea.colours) +
  xlab("Classifier") + 
  ylab("Statistic parmeter") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.recombined.binary.c.nc.parea

```

### Specificity

Specificity is the proportion of known negatives (known non-CAZymes) which are correctly classified as negatives (non-CAZymes).

Figure \@ref(fig:spec) is a graphical representation of the results calculated in table \@ref(tab:sumstats).

```{r specRT, echo=FALSE, fig.cap="One-dimensional scatter plot of specificity scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

rt.subset_spec$Prediction_tool <- str_replace(rt.subset_spec$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.subset_spec$Prediction_tool <- str_replace(rt.subset_spec$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
rt.subset_spec$Prediction_tool <- factor(rt.subset_spec$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

p.rt.binary.spec = ggplot(rt.subset_spec %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Specificity")
p.rt.binary.spec
```

```{r savebinarySpecRT, include=FALSE}
pdf(file = "rt_binarySpec.pdf", width = 8.58, height = 5.5)
p.binary.spec
dev.off()
```

### Sensitivity

Sensitivity (also known as recall) is the proportion of known positives (CAZymes) that are correctly identified as positives (CAZymes).

Figure \@ref(fig:recallbc) graphically represents of the results calculated in table \@ref(tab:sumstats).


```{r recallbcRT, echo=FALSE, fig.cap="One-dimensional scatter plot of recall (sensitivity) scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

rt.subset_sens$Prediction_tool <- str_replace(rt.subset_sens$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.subset_sens$Prediction_tool <- str_replace(rt.subset_sens$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
rt.subset_sens$Prediction_tool <- factor(rt.subset_sens$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

p.rt.binary.sens = ggplot(rt.subset_sens %>% dplyr::group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity")
p.rt.binary.sens
```

```{r saveBinSensRT, include=FALSE}
pdf(file = "rt_binarySensitivity.pdf", width = 8.58, height = 5.5)
p.binary.sens
dev.off()
```

### Precision

Precision is the proportion of positive predictions by the classifiers that are correct.

In this case, precision represents the fraction of CAZyme predictions by the classifiers that are correct, specifically the proportion of predicted CAZymes that are known CAZymes.

Figure \@ref(fig:precbc) is a visual representation of the results calculated in table \@ref(tab:sumstats).

```{r precbcRT, echo=FALSE, fig.cap="One-dimensional scatter plot of precision scores of CAZyme and non-CAZyme predictions per test set, overlaying box plot of standard deviation."}

rt.subset_prec$Prediction_tool <- str_replace(rt.subset_prec$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.subset_prec$Prediction_tool <- str_replace(rt.subset_prec$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
rt.subset_prec$Prediction_tool <- factor(rt.subset_prec$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

p.rt.binary.prec = ggplot(rt.subset_prec %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Precision")
p.rt.binary.prec
```

```{r saveBinPrecRT, include=FALSE}
pdf(file = "rt_binaryPrec.pdf", width = 8.58, height = 5.5)
p.binary.prec
dev.off()
```

### F1-score

The F1-score is a harmonic (or weighted) average of recall and precision and provides an idea of the overall performance of the tool, 0 being the lowest and 1 being the best performance. Figure \@ref(fig:f1bc) shows the F1-score from each test set, for each classifier.

```{r f1bcRT, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}

rt.subset_f1$Prediction_tool <- str_replace(rt.subset_f1$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.subset_f1$Prediction_tool <- str_replace(rt.subset_f1$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
rt.subset_f1$Prediction_tool <- factor(rt.subset_f1$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

p.rt.binary.f1 = ggplot(rt.subset_f1 %>% dplyr::group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score")
p.rt.binary.f1
```


```{r saveBinF1RT, include=FALSE}
png(file = "binaryF1.png", units = 'in', width = 8.58, height = 5.5, res=900)
p.rt.binary.f1
dev.off()
```

### Accuracy

Accuarcy (calculated using (TP + TN) / (TP + TN + FP + FN) ) provides an idea of the overall performance of the classifiers as a measure of the degree to which their CAZyme/non-CAZyme predictions conforms to the correct result. Figure \@ref(fig:accbc) is a plot of respective data from table \@ref(tab:sumstats).


```{r accbcRT, echo=FALSE, fig.cap="Bar chart of specificity of CAZyme classifiers differentiation between CAZymes and non-CAZymes."}

rt.subset_acc$Prediction_tool <- str_replace(rt.subset_acc$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.subset_acc$Prediction_tool <- str_replace(rt.subset_acc$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
rt.subset_acc$Prediction_tool <- factor(rt.subset_acc$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

p.rt.binary.acc = ggplot(rt.subset_acc %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Accuracy") 
p.rt.binary.acc
```

```{r saveBinAccRT, include=FALSE}
pdf(file = "binaryAccRT.pdf", width = 8.58, height = 5.5)
p.rt.binary.acc
dev.off()
```

Below is a combination (3x2) plot of the above plots for evaluating the binary CAZyme/non-CAZyme classification performance between dbCAN and the user defined combination of tools. In this case:
- dbCAN
- HMMER, DIAMOND, CUPP
- HMMER, DIAMOND, eCAMI

```{r binaryComboRT, include=FALSE}
select_recombined_tool_rows <- function(df){
  dbcan_rows <- df[which(df$Prediction_tool == "dbCAN"), ]
  hd_cupp_rows <- df[which(df$Prediction_tool == "HMMER_DIAMOND_CUPP"), ]
  hd_ecami_rows <- df[which(df$Prediction_tool == "HMMER_DIAMOND_eCAMI"), ]
  
  filtered_df <- rbind(dbcan_rows, hd_cupp_rows)
  filtered_df <- rbind(filtered_df, hd_ecami_rows)
  
  filtered_df$Prediction_tool <- str_replace(filtered_df$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
  filtered_df$Prediction_tool <- str_replace(filtered_df$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
  
  return(filtered_df)
}

axis_values = seq(0.3,1, by = 0.05)

# select the rows for dbCAN and the user defined combinations of tools
rt.spec.bin.rt <- select_recombined_tool_rows(rt.subset_spec)
rt.sens.bin.rt <- select_recombined_tool_rows(rt.subset_sens)
rt.prec.bin.rt <- select_recombined_tool_rows(rt.subset_prec)
rt.f1.bin.rt <- select_recombined_tool_rows(rt.subset_f1)
rt.acc.bin.rt <- select_recombined_tool_rows(rt.subset_acc)

rt.mp.binary.spec = ggplot(rt.spec.bin.rt %>% dplyr::group_by(Prediction_tool),
                aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Specificity")

rt.mp.binary.sens = ggplot(rt.sens.bin.rt %>% dplyr::group_by(Prediction_tool),
                  aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Sensitivity") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))

rt.mp.binary.prec = ggplot(rt.prec.bin.rt %>% dplyr::group_by(Prediction_tool),
                       aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Precision") +
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))

rt.mp.binary.f1 = ggplot(rt.f1.bin.rt %>% dplyr::group_by(Prediction_tool),
              aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("F1-score") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))

rt.mp.binary.acc = ggplot(rt.acc.bin.rt %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Statistic_value, fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=8),
        axis.title=element_text(size=9,face="bold")) +
  xlab("Classifier") +
  ylab("Accuracy") + 
  scale_y_continuous(limits = c(0.3, 1), breaks = seq(0.3,1.0,0.1))

plot_grid(rt.mp.binary.spec,  rt.mp.binary.sens,  rt.mp.binary.prec,  rt.mp.binary.f1,  rt.mp.binary.acc,  labes='AUTO',  ncol=3)
```

```{r saveBinMuliPlotRT, include=FALSE}
png("rt_binary_multiplotRT.png", units='in', width=8, height=6,res=900)
plot_grid(rt.mp.binary.spec,  rt.mp.binary.sens,  rt.mp.binary.prec,  rt.mp.binary.f1,  rt.mp.binary.acc,  labes='AUTO',  ncol=3)
dev.off()
```

## Classification of CAZy classes

recombined_tools_class_df_pred

```{r cazyClassStasTableRT, echo=FALSE}
# Calculate statistics
rt.class_subset_spec <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Specificity"), ]
rt.class_subset_spec  <- rt.class_subset_spec[complete.cases(rt.class_subset_spec), ]
rt.class_specificity <- rt.class_subset_spec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Spec Mean"=mean(Statistic_value), "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
  )

rt.class_subset_sens <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Sensitivity"), ]
rt.class_subset_sens  <- rt.class_subset_sens[complete.cases(rt.class_subset_sens), ]
rt.class_sensitivity <- rt.class_subset_sens %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Sens Mean"=mean(Statistic_value), "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
  )

rt.class_subset_prec <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Precision"), ]
rt.class_subset_prec  <- rt.class_subset_prec[complete.cases(rt.class_subset_prec), ]
rt.class_precision <- rt.class_subset_prec %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Prec Mean"=mean(Statistic_value), "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
  )

rt.class_subset_f1 <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Fbeta_score"), ]
rt.class_subset_f1  <- rt.class_subset_f1[complete.cases(rt.class_subset_f1), ]
rt.class_f1_score <- rt.class_subset_f1 %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "F1-score Mean"=mean(Statistic_value), "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
  )

rt.class_subset_acc <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Accuracy"), ]
rt.class_subset_acc  <- rt.class_subset_acc[complete.cases(rt.class_subset_acc), ]
rt.class_accuracy <- rt.class_subset_acc %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Acc Mean"=mean(Statistic_value), "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
  )

# combine data and build a single dataframe
rt.class_summary_df <- merge(rt.class_specificity, rt.class_sensitivity)
rt.class_summary_df <- merge(rt.class_summary_df, rt.class_precision)
rt.class_summary_df <- merge(rt.class_summary_df, rt.class_f1_score)
rt.class_summary_df <- merge(rt.class_summary_df, rt.class_accuracy)

names(rt.class_summary_df)[names(rt.class_summary_df) == "Prediction_tool"] <- "Classifier"
# reorder the rows

# define factors
rt.class_summary_df$Prediction_tool <- factor(rt.class_summary_df$Classifier, levels = recombined_classifiers) # set order data is presented

row.names(rt.class_summary_df) = NULL  # hides row names which are added by reordering the rows

kable(
  rt.class_summary_df,
  caption="Overall performance of CAZyme classifiers CAZy class classification performance",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)

```


Below a proportional area plot representing the F-beta score for each CAZyme classifier for each test set is generated. each square is sized proportional to the relative sample size. Every class was not included in every sample, resulting in different sample sizes between CAZy classes, the same between classifiers.

```{r statsFbetaRT, echo=FALSE}
# subset Fbeta-scores
rt.class_fbeta_subset <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Fbeta_score"), ]

# use abbreviated tool recombination names
rt.class_fbeta_subset$Prediction_tool <- str_replace(rt.class_fbeta_subset$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
rt.class_fbeta_subset$Prediction_tool <- str_replace(rt.class_fbeta_subset$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")

# Not every CAZy class is present in every test set, where it was not present a value of NA was given.
# these rows need to be dropped so that the proportional area plot represents the number of test sets containing
# that CAZy class
rt.class_fbeta_subset  <- rt.class_fbeta_subset[complete.cases(rt.class_fbeta_subset), ]

# classify the Fbeta-scores into bins
val.class <- vector()

for(i in 1:nrow(rt.class_fbeta_subset)) {
  if(rt.class_fbeta_subset[i, 6] == 1){val.class <- append(val.class, '[1.00]')} 
  else if (rt.class_fbeta_subset[i, 6] == 0){val.class <- append(val.class, '[0.00]')}
  else if (rt.class_fbeta_subset[i, 6] < 1 && rt.class_fbeta_subset[i, 6] >= 0.95){val.class <- append(val.class, '(0.95, 1.00]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.95 && rt.class_fbeta_subset[i, 6] >= 0.9){val.class <- append(val.class, '(0.90, 0.95]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.90 && rt.class_fbeta_subset[i, 6] >= 0.85){val.class <- append(val.class, '(0.85, 0.90]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.85 && rt.class_fbeta_subset[i, 6] >= 0.80){val.class <- append(val.class, '(0.80, 0.85]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.80 && rt.class_fbeta_subset[i, 6] >= 0.75){val.class <- append(val.class, '(0.75, 0.80]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.75 && rt.class_fbeta_subset[i, 6] >= 0.70){val.class <- append(val.class, '(0.70, 0.75]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.70 && rt.class_fbeta_subset[i, 6] >= 0.65){val.class <- append(val.class, '(0.65, 0.70]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.65 && rt.class_fbeta_subset[i, 6] >= 0.60){val.class <- append(val.class, '(0.60, 0.65]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.60 && rt.class_fbeta_subset[i, 6] >= 0.55){val.class <- append(val.class, '(0.55, 0.60]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.55 && rt.class_fbeta_subset[i, 6] >= 0.50){val.class <- append(val.class, '(0.50, 0.55]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.50 && rt.class_fbeta_subset[i, 6] >= 0.45){val.class <- append(val.class, '(0.45, 0.50]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.45 && rt.class_fbeta_subset[i, 6] >= 0.40){val.class <- append(val.class, '(0.40, 0.45]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.40 && rt.class_fbeta_subset[i, 6] >= 0.35){val.class <- append(val.class, '(0.35, 0.40]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.35 && rt.class_fbeta_subset[i, 6] >= 0.30){val.class <- append(val.class, '(0.30, 0.35]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.30 && rt.class_fbeta_subset[i, 6] >= 0.25){val.class <- append(val.class, '(0.25, 0.30]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.25 && rt.class_fbeta_subset[i, 6] >= 0.20){val.class <- append(val.class, '(0.20, 0.25]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.20 && rt.class_fbeta_subset[i, 6] >= 0.15){val.class <- append(val.class, '(0.15, 0.20]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.15 && rt.class_fbeta_subset[i, 6] >= 0.10){val.class <- append(val.class, '(0.10, 0.15]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.10 && rt.class_fbeta_subset[i, 6] >= 0.05){val.class <- append(val.class, '(0.05, 0.10]')}
  else if (rt.class_fbeta_subset[i, 6] < 0.05 && rt.class_fbeta_subset[i, 6] >= 0){val.class <- append(val.class, '(0.00, 0.05]')}
  else {val.class <- append(val.class, '< 0')}
}
rt.class_fbeta_subset$val.class <- val.class

# set order data is presented
rt.class_fbeta_subset$Prediction_tool <- factor(rt.class_fbeta_subset$Prediction_tool, levels = recombined_classifiers_abbrev)

rt.class_fbeta_subset$CAZy_class <- factor(rt.class_fbeta_subset$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

rt.class_fbeta_subset$val.class <- factor(rt.class_fbeta_subset$val.class, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.rt.classF1 = ggally_count(rt.class_fbeta_subset, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.class)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.rt.classF1
```

```{r saveCazyClassFbetaRT, include=FALSE}
svg(file = "mlcClassF1rt.svg",  width = 8.25, height = 11)
p.rt.classF1
dev.off()
```

A dataframe of the number of test sets containing each CAZy class is generated.

```{r calcCazyClassSamplesRT, echo=FALSE}
rt.class_fbeta_subset$Prediction_tool <- factor(rt.class_fbeta_subset$Prediction_tool, levels = recombined_classifiers_abbrev) # set order data is presented

rt.dbcan_row = calc_class_sample_size(rt.class_fbeta_subset, 'dbCAN')
rt.hmmer_row = calc_class_sample_size(rt.class_fbeta_subset, 'HMMER')
rt.diamond_row = calc_class_sample_size(rt.class_fbeta_subset, 'DIAMOND')
rt.hotpep_row = calc_class_sample_size(rt.class_fbeta_subset, 'Hotpep')
rt.cupp_row = calc_class_sample_size(rt.class_fbeta_subset, 'CUPP')
rt.ecami_row = calc_class_sample_size(rt.class_fbeta_subset, 'eCAMI')
rt.hdc_row = calc_class_sample_size(rt.class_fbeta_subset, 'H_D_C')
rt.hde_row = calc_class_sample_size(rt.class_fbeta_subset, 'H_D_E')

rt.cazy_class_sample_size <- rbind(rt.dbcan_row, rt.hmmer_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.diamond_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.hotpep_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.cupp_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.ecami_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.hdc_row)
rt.cazy_class_sample_size <- rbind(rt.cazy_class_sample_size, rt.hde_row)

rt.cazy_class_sample_size <- as.data.frame(rt.cazy_class_sample_size)
names(rt.cazy_class_sample_size) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
rt.cazy_class_sample_size
```

```{r classF1CiRT, echo=FALSE, fig.cap="95% confidence interval around the mean CAZy class classification per CAZy class"}

cal_tool_ci.class <- function(stat_subset){
  c.classes <- c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
  fam.ci.df <- data.frame()
  
  for (cc in c.classes){
    subset_class <- stat_subset[which(stat_subset$CAZy_class == cc), ]
    
    # calculate lower, upper and mean 95% CI
    UpperCI <- subset_class %>%
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
    MeanCI <- subset_class %>% 
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
    LowerCI <- subset_class %>%
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])
  
    class.ci_df <- merge(MeanCI, UpperCI)
    class.ci_df <- merge(class.ci_df, LowerCI)
    class.ci_df$CAZy_class <- rep(cc, nrow(class.ci_df))
    
    fam.ci.df <- rbind(fam.ci.df, class.ci_df)
  }
  
  return(fam.ci.df)
}
class_fbeta_df.rt.ci <- cal_tool_ci.class(rt.class_fbeta_subset)

# set order data is presented
class_fbeta_df.rt.ci$CAZy_class <- factor(
  class_fbeta_df.rt.ci$CAZy_class, levels = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
)
class_fbeta_df.rt.ci$Prediction_tool <- factor(
  class_fbeta_df.rt.ci$Prediction_tool, levels = recombined_classifiers_abbrev
)

p.class.f1.ci.rt <- ggplot(
  class_fbeta_df.rt.ci %>% dplyr::group_by(Prediction_tool),
  aes(x=Prediction_tool,
      y=MeanCI,
      color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_color_manual(values = rt.colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score") +
  facet_wrap(~ CAZy_class)
p.class.f1.ci.rt
```

The sensitivity of each CAZyme classifier can be plotted against the specificity for each CAZy class, however plotting all CAZy classes in a single plot produces an overally cramped plot, unless very few test sets were used.

```{r cazyClassSpecVsSensRT, echo=FALSE}
# subset specificity scores
cazy_class_specificity_df <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Specificity"), ]
# rename columns
names(cazy_class_specificity_df)[names(cazy_class_specificity_df) == "Statistic_value"] <- "Specificity"
cazy_class_specificity_df <- cazy_class_specificity_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Specificity")]

# subset sensitivity scores
cazy_class_sens_df <- recombined_tools_class_df_pred[which(recombined_tools_class_df_pred$Statistic_parameter == "Sensitivity"), ]
names(cazy_class_sens_df)[names(cazy_class_sens_df) == "Statistic_value"] <- "Sensitivity"
cazy_class_sens_df <- cazy_class_sens_df[c("Genomic_accession", "Prediction_tool", "CAZy_class", "Sensitivity")]

# merge dataframes
cazy_class_spec_sense_df <- merge(cazy_class_specificity_df, cazy_class_sens_df, by=c("Genomic_accession", "Prediction_tool", "CAZy_class"))

# specificity was not predicted for every CAZy class in eveyr test set, this is the result of the CAZy class not being present in the test set and the classifier not predicting the presence of the CAZy class in the test set
# In these cases specificity is NA
# NA values are removed
cazy_class_spec_sense_df  <- cazy_class_spec_sense_df[complete.cases(cazy_class_spec_sense_df), ]

# one plot per class with be made using facet_wrap by Prediciton_tool.
p.class.spec.sense = ggplot(cazy_class_spec_sense_df %>% dplyr::group_by(Prediction_tool), aes(x=Sensitivity, y=Specificity, color=Prediction_tool)) +
  geom_point() +
  scale_color_manual(values=rt.colour_set) +
  theme(plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=11,face="bold"),
        strip.text = element_text(size=11)) +
  labs(x = "Sensitivity", y = "Specificity", color="Classifer") +
  facet_wrap(~CAZy_class)
p.class.spec.sense
```


## Performance per CAZy class

Below the prediction sensitivity is plotted against the specificity for each classifier, and a separate plot is generated for each CAZy class.

The scatter plots of sensitivity against specificity overlay a coloured contour to highlight the distribution of the points. When too many points have the same value a contour cannot be generated. In order to plot a contour noise is added to the data. The original data is used to plot the scatter plot and the data with added noise is used to plot the contour.

The percentage of the data points which need noise to be added to them in order to generate a contour varies from data set to data set. To change the percentage of the data points with noise added to them, change the third value of call to the function `plot.class.sens.vs.spec()`, which is used to generate the plots. The third value is the percentage of data points to add noise to, written in decimal form.

#### CAZy class classification for GH

```{r classGhSpecSensRT, echo=FALSE}
# use abbreviated tool recombination names
cazy_class_spec_sense_df$Prediction_tool <- str_replace(cazy_class_spec_sense_df$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
cazy_class_spec_sense_df$Prediction_tool <- str_replace(cazy_class_spec_sense_df$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")

p.class.gh.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GH', 0.3, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.gh.spec.sens
```

```{r saveclassGhSpecSensRT, echo=FALSE}
pdf(file = "classGhSpecVsSensRT", width = 11.25, height = 7)
p.class.gh.spec.sens
dev.off()
```

#### CAZy class classification for GT

```{r classGtSpecSensRT, echo=FALSE}
p.class.gt.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'GT', 0.5, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.gt.spec.sens
```

```{r saveclassGtSpecSensRT, echo=FALSE}
pdf(file = "classGtSpecVsSensRT", width = 11.25, height = 7)
p.class.gt.spec.sens
dev.off()
```

#### CAZy class classification for PL

```{r classPlSpecSensRT, echo=FALSE}
p.class.pl.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'PL', 0.5, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.pl.spec.sens
```

```{r saveclassPlSpecSensRT, echo=FALSE}
pdf(file = "classPlSpecVsSensRT", width = 11.25, height = 7)
p.class.pl.spec.sens
dev.off()
```

#### CAZy class classification for CE

```{r classCeSpecSensRT, echo=FALSE}
p.class.ce.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CE', 0.4, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.ce.spec.sens
```

```{r saveclassCeSpecSensRT, echo=FALSE}
pdf(file = "classCeSpecVsSensRT", width = 11.25, height = 7)
p.class.ce.spec.sens
dev.off()
```

#### CAZy class classification for AA

```{r classAaSpecSensRT, echo=FALSE}
p.class.aa.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'AA', 0.3, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.aa.spec.sens
```

```{r saveclassAaSpecSensRT, echo=FALSE}
pdf(file = "classAaSpecVsSensRT", width = 11.25, height = 7)
p.class.aa.spec.sens
dev.off()
```

#### CAZy class classification for CBM

```{r classCbmSpecSensRT, echo=FALSE}
p.class.cbm.spec.sens <- plot.class.sens.vs.spec(
  cazy_class_spec_sense_df,
  'CBM', 0.3, 0.001, 0.01,
  rt.colour_set,
  recombined_classifiers_abbrev
)
p.class.cbm.spec.sens
```

```{r saveclassCbmSpecSensRT, echo=FALSE}
pdf(file = "classCbmSpecVsSensRT", width = 11.25, height = 7)
p.class.cbm.spec.sens
dev.off()
```

## Rand Index and Adjusted Rand Index of CAZy Class Prediction

A single CAZyme can be included in multiple CAZy classes leading to the multilabel classification of CAZymes. To address this and evaluate the multilabel classification of CAZy classes the Rand Index (RI) and Adjusted Rand Index (ARI) were calculated.

The RI is the measure of accuracy across all potential classifications of a protein. The RI ranges from 0 (no correct annotations) to 1 (all annotations correct). The ARI is the RI adjusted for chance, where 0 is the equivalent to assigning the CAZy class annotations randomly, -1 where the annotations are systematically handed out incorrectly and 1 where the annotations are all correct.

```{r riCalcRT, include=FALSE}
recombined_tools_class_ri_ari$Prediction_tool <- factor(recombined_tools_class_ri_ari$Prediction_tool, levels = recombined_classifiers) # set order data is presented

rt.class_ri_stats_df <- recombined_tools_class_ri_ari %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index),
  "Lower CI"=CI(Rand_index)[3], "Upper CI"=CI(Rand_index)[1]
  )

kable(rt.class_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy class annotations and 95% confidence interval", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r rtclassRIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Rand Index (RI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

rt.class_ri_stats_df$Prediction_tool <- factor(rt.class_ri_stats_df$Prediction_tool, levels = recombined_classifiers) # set order data is presented

# plot RI
p.rt.ri.class = ggplot(rt.class_ri_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.rt.ri.class
```

```{r ariCalcRT, echo=FALSE}
rt.class_ari_stats_df <- recombined_tools_class_ri_ari %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index),
  "Lower CI"=CI(Adjusted_Rand_index)[3], "Upper CI"=CI(Adjusted_Rand_index)[1]
  )

kable(rt.class_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy class annotations and 95% confidence interval", align='c', digits = 4) %>% kable_styling(full_width = F)
```

```{r rtclassARIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Adjusted Rand Index (ARI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

rt.class_ari_stats_df$Prediction_tool <- factor(rt.class_ari_stats_df$Prediction_tool, levels = recombined_classifiers) # set order data is presented

# plot RI
p.rt.ari.class = ggplot(rt.class_ari_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.rt.ari.class
```


Plot are violin plots underlying scatter plots, presenting the RI and ARI for every protein across all test sets.

```{r classRIRT, echo=FALSE, fig.cap="Violin plot of Rand Index (RI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

recombined_tools_class_ri_ari$Prediction_tool <- factor(recombined_tools_class_ri_ari$Prediction_tool, levels = recombined_classifiers) # set order data is presented

# plot RI
p.rt.ri.class = ggplot(recombined_tools_class_ri_ari %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Rand Index") +
  scale_y_continuous(breaks = seq(0,1, by = 0.1))
p.rt.ri.class
```

```{r classARIRT, echo=FALSE, fig.cap="Violin plot of Adjusted Rand Index (ARI) of performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

p.rt.ari.class = ggplot(recombined_tools_class_ri_ari %>% dplyr::group_by(Prediction_tool), aes(x=Prediction_tool, y=Adjusted_Rand_index, fill=Prediction_tool)) +
  geom_violin() +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index") +
  scale_y_continuous(breaks = c(-0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1))
p.rt.ari.class
```


## Classification of CAZy families

The following section evaluates the performance of combining CAZyme classifiers on predict CAZy family classifications, comparing the performance between the user-defined combination of classifiers and the individual classifiers.

### General trends in performance across all CAZy families

Table \@ref(tab:cazyFamStasTableRt) summarising the overall CAZy family classifications for each test set across all CAZy families.
recombined_tools_fam_df

```{r cazyFamStasTableRt, echo=FALSE}
# Calculate statistics

fam_subset_spec.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Statistical_parameter == "Specificity"), ]
fam_subset_spec.rt  <- fam_subset_spec.rt[complete.cases(fam_subset_spec.rt), ]
fam_specificity.rt <- fam_subset_spec.rt %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Spec Mean"=mean(Statistic_value),
  "Spec Standard Deviation"=sd(Statistic_value),
    "Spec Lower CI"=CI(Statistic_value)[3],
    "Spec Upper CI"=CI(Statistic_value)[1]
)

fam_subset_sens.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Statistical_parameter == "Sensitivity"), ]
fam_subset_sens.rt  <- fam_subset_sens.rt[complete.cases(fam_subset_sens.rt), ]
fam_sensitivity.rt <- fam_subset_sens.rt %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Sens Mean"=mean(Statistic_value),
  "Sens Standard Deviation"=sd(Statistic_value),
    "Sens Lower CI"=CI(Statistic_value)[3],
    "Sens Upper CI"=CI(Statistic_value)[1]
)

fam_subset_prec.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Statistical_parameter == "Precision"), ]
fam_subset_prec.rt  <- fam_subset_prec.rt[complete.cases(fam_subset_prec.rt), ]
fam_precision.rt <- fam_subset_prec.rt %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Prec Mean"=mean(Statistic_value),
  "Prec Standard Deviation"=sd(Statistic_value),
    "Prec Lower CI"=CI(Statistic_value)[3],
    "Prec Upper CI"=CI(Statistic_value)[1]
)

fam_subset_f1.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Statistical_parameter == "Fbeta_score"), ]
fam_subset_f1.rt  <- fam_subset_f1.rt[complete.cases(fam_subset_f1.rt), ]
fam_f1_score.rt <- fam_subset_f1.rt %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "F1-score Mean"=mean(Statistic_value),
  "F1-score Standard Deviation"=sd(Statistic_value),
    "F1-score Lower CI"=CI(Statistic_value)[3],
    "F1-score Upper CI"=CI(Statistic_value)[1]
)

fam_subset_acc.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Statistical_parameter == "Accuracy"), ]
fam_subset_acc.rt  <- fam_subset_acc.rt[complete.cases(fam_subset_acc.rt), ]
fam_accuracy.rt <- fam_subset_acc.rt %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise(
  "Acc Mean"=mean(Statistic_value),
  "Acc Standard Deviation"=sd(Statistic_value),
    "Acc Lower CI"=CI(Statistic_value)[3],
    "Acc Upper CI"=CI(Statistic_value)[1]
)

# combine data and build a single dataframe
fam_summary_df.rt <- merge(fam_specificity.rt, fam_sensitivity.rt)
fam_summary_df.rt <- merge(fam_summary_df.rt, fam_precision.rt)
fam_summary_df.rt <- merge(fam_summary_df.rt, fam_f1_score.rt)
fam_summary_df.rt <- merge(fam_summary_df.rt, fam_accuracy.rt)

# define factors; set order data is presented
fam_summary_df.rt$Prediction_tool <- factor(fam_summary_df.rt$Prediction_tool, levels = recombined_classifiers)

names(fam_summary_df.rt)[names(fam_summary_df.rt) == "Prediction_tool"] <- "Classifier"
# reorder the rows

row.names(fam_summary_df.rt) = NULL  # hides row names which are added by reordering the rows

kable(
  fam_summary_df.rt,
  caption="Overall performance of CAZyme classifiers CAZy family classification performance across all CAZy families",
  align='c',
  digits = 4) %>% kable_styling(full_width = F) %>% column_spec(c(1,5,9,13,17,21), border_right=TRUE)
```

The evaluate the overall performance of each classifier, for each CAZy family, the F1-score was calculated for every family. Families were grouped by their parent CAZy class and the distribution of the F1-scores is shown in figure \@ref(fig:fbetaclass).

```{r fbetafamilyRT, echo=FALSE, fig.cap="Proportaional area plot of F1-score per CAZy distribution per CAZy famiy"}
# F-beta scores already subset previously, and are stored in the var fam_subset_f1.rt

# classify the Fbeta-scores into bins
val.fam <- vector()

# Use the abbreviated names of the recombined tools
fam_subset_f1.rt$Prediction_tool <- str_replace(fam_subset_f1.rt$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
fam_subset_f1.rt$Prediction_tool <- str_replace(fam_subset_f1.rt$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")

for(i in 1:nrow(fam_subset_f1.rt)){
  if(fam_subset_f1.rt[i, 5] == 1){val.fam <- append(val.fam, '[1.00]')} 
  else if (fam_subset_f1.rt[i, 5] == 0){val.fam <- append(val.fam, '[0.00]')}
  else if (fam_subset_f1.rt[i, 5] < 1 && fam_subset_f1.rt[i, 5] >= 0.95){val.fam <- append(val.fam, '(0.95, 1.00]')}
  else if (fam_subset_f1.rt[i, 5] < 0.95 && fam_subset_f1.rt[i, 5] >= 0.9){val.fam <- append(val.fam, '(0.90, 0.95]')}
  else if (fam_subset_f1.rt[i, 5] < 0.90 && fam_subset_f1.rt[i, 5] >= 0.85){val.fam <- append(val.fam, '(0.85, 0.90]')}
  else if (fam_subset_f1.rt[i, 5] < 0.85 && fam_subset_f1.rt[i, 5] >= 0.80){val.fam <- append(val.fam, '(0.80, 0.85]')}
  else if (fam_subset_f1.rt[i, 5] < 0.80 && fam_subset_f1.rt[i, 5] >= 0.75){val.fam <- append(val.fam, '(0.75, 0.80]')}
  else if (fam_subset_f1.rt[i, 5] < 0.75 && fam_subset_f1.rt[i, 5] >= 0.70){val.fam <- append(val.fam, '(0.70, 0.75]')}
  else if (fam_subset_f1.rt[i, 5] < 0.70 && fam_subset_f1.rt[i, 5] >= 0.65){val.fam <- append(val.fam, '(0.65, 0.70]')}
  else if (fam_subset_f1.rt[i, 5] < 0.65 && fam_subset_f1.rt[i, 5] >= 0.60){val.fam <- append(val.fam, '(0.60, 0.65]')}
  else if (fam_subset_f1.rt[i, 5] < 0.60 && fam_subset_f1.rt[i, 5] >= 0.55){val.fam <- append(val.fam, '(0.55, 0.60]')}
  else if (fam_subset_f1.rt[i, 5] < 0.55 && fam_subset_f1.rt[i, 5] >= 0.50){val.fam <- append(val.fam, '(0.50, 0.55]')}
  else if (fam_subset_f1.rt[i, 5] < 0.50 && fam_subset_f1.rt[i, 5] >= 0.45){val.fam <- append(val.fam, '(0.45, 0.50]')}
  else if (fam_subset_f1.rt[i, 5] < 0.45 && fam_subset_f1.rt[i, 5] >= 0.40){val.fam <- append(val.fam, '(0.40, 0.45]')}
  else if (fam_subset_f1.rt[i, 5] < 0.40 && fam_subset_f1.rt[i, 5] >= 0.35){val.fam <- append(val.fam, '(0.35, 0.40]')}
  else if (fam_subset_f1.rt[i, 5] < 0.35 && fam_subset_f1.rt[i, 5] >= 0.30){val.fam <- append(val.fam, '(0.30, 0.35]')}
  else if (fam_subset_f1.rt[i, 5] < 0.30 && fam_subset_f1.rt[i, 5] >= 0.25){val.fam <- append(val.fam, '(0.25, 0.30]')}
  else if (fam_subset_f1.rt[i, 5] < 0.25 && fam_subset_f1.rt[i, 5] >= 0.20){val.fam <- append(val.fam, '(0.20, 0.25]')}
  else if (fam_subset_f1.rt[i, 5] < 0.20 && fam_subset_f1.rt[i, 5] >= 0.15){val.fam <- append(val.fam, '(0.15, 0.20]')}
  else if (fam_subset_f1.rt[i, 5] < 0.15 && fam_subset_f1.rt[i, 5] >= 0.10){val.fam <- append(val.fam, '(0.10, 0.15]')}
  else if (fam_subset_f1.rt[i, 5] < 0.10 && fam_subset_f1.rt[i, 5] >= 0.05){val.fam <- append(val.fam, '(0.05, 0.10]')}
  else if (fam_subset_f1.rt[i, 5] < 0.05 && fam_subset_f1.rt[i, 5] >= 0){val.fam <- append(val.fam, '(0.00, 0.05]')}
  else {val.fam <- append(val.fam, '< 0')}
}
# add the bins to the df
fam_subset_f1.rt$val.fam <- val.fam

# add the parent CAZy class to the df for facet wrapping

# retrieve all the family names
dbcan.fam.subset.rt <- recombined_tools_fam_df[which(recombined_tools_fam_df$Prediction_tool == "dbCAN"), ]
tn.dbcan.fam.subset.rt <- dbcan.fam.subset.rt[which(dbcan.fam.subset.rt$Statistical_parameter == "TNs"), ]

# separate names into one vector per CAZy class
gh.names = tn.dbcan.fam.subset.rt[1:172,]$CAZy_family
gt.names = tn.dbcan.fam.subset.rt[173:287,]$CAZy_family
pl.names = tn.dbcan.fam.subset.rt[288:329,]$CAZy_family
ce.names = tn.dbcan.fam.subset.rt[330:348,]$CAZy_family
aa.names = tn.dbcan.fam.subset.rt[349:365,]$CAZy_family
cbm.names = tn.dbcan.fam.subset.rt[366:454,]$CAZy_family

# create subsets by CAZy class and associated each data set with the name of its CAZy class
gh.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% gh.names), ]
gh.class <- rep('GH', nrow(gh.subset))
gh.subset$CAZy_class <- gh.class

gt.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% gt.names), ]
gt.class <- rep('GT', nrow(gt.subset))
gt.subset$CAZy_class <- gt.class

pl.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% pl.names), ]
pl.class <- rep('PL', nrow(pl.subset))
pl.subset$CAZy_class <- pl.class

ce.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% ce.names), ]
ce.class <- rep('CE', nrow(ce.subset))
ce.subset$CAZy_class <- ce.class

aa.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% aa.names), ]
aa.class <- rep('AA', nrow(aa.subset))
aa.subset$CAZy_class <- aa.class

cbm.subset <- fam_subset_f1.rt[which(fam_subset_f1.rt$CAZy_family %in% cbm.names), ]
cbm.class <- rep('CBM', nrow(cbm.subset))
cbm.subset$CAZy_class <- cbm.class

# combine the class dfs in to one
fam_fbeta_df.rt <- rbind(gh.subset, gt.subset)
fam_fbeta_df.rt <- rbind(fam_fbeta_df.rt, pl.subset)
fam_fbeta_df.rt <- rbind(fam_fbeta_df.rt, ce.subset)
fam_fbeta_df.rt <- rbind(fam_fbeta_df.rt, aa.subset)
fam_fbeta_df.rt <- rbind(fam_fbeta_df.rt, cbm.subset)

# use abbreviation of combined tools


# set order data is presented
fam_fbeta_df.rt$Prediction_tool <- factor(fam_fbeta_df.rt$Prediction_tool, levels = recombined_classifiers_abbrev)

fam_fbeta_df.rt$CAZy_class <- factor(fam_fbeta_df.rt$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))
fam_fbeta_df.rt$val.fam <- factor(fam_fbeta_df.rt$val.fam, levels = c('< 0', '[0.00]', '(0.00, 0.05]', '(0.05, 0.10]', '(0.10, 0.15]', '(0.15, 0.20]', '(0.20, 0.25]', '(0.25, 0.30]', '(0.30, 0.35]', '(0.35, 0.40]', '(0.40, 0.45]', '(0.45, 0.50]', '(0.50, 0.55]', '(0.55, 0.60]', '(0.60, 0.65]', '(0.65, 0.70]', '(0.75, 0.80]', '(0.85, 0.90]', '(0.90, 0.95]', '(0.95, 1.00]', '[1.00]'))

p.famF1 = ggally_count(fam_fbeta_df.rt, mapping=ggplot2::aes(x=Prediction_tool, y=CAZy_class, fill=val.fam)) +
  scale_fill_manual(values = colour_grad) +
  xlab("Classifier") + 
  ylab("CAZy class") +
  theme(legend.background = element_rect(fill = figbg, color = figbg),
        plot.background = element_rect(fill = figbg, color = figbg),
        legend.text=element_text(size=10),
        legend.title=element_text(size=12),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  labs(fill = expression(paste("F", beta, "-score"))) +
  guides(fill = guide_legend(reverse = TRUE))
p.famF1
```

```{r saveFambetaRT, include=FALSE}
svg(file = "mlcFamF1rt.svg",  width = 8.25, height = 11)
p.famF1
dev.off()
```

\@ref(fig:fbetaclass)
Below is a table displaying the number of test sets in which each CAZy class was present, and were used to draw the proportional areas for each class in figure\@ref(fig:fbetaclass).

```{r calcFamFbetaSampleSizeRT, echo=FALSE}
fam_dbcan_row = calc_class_sample_size(fam_fbeta_df, 'dbCAN')
fam_hmmer_row = calc_class_sample_size(fam_fbeta_df, 'HMMER')
fam_diamond_row = calc_class_sample_size(fam_fbeta_df, 'DIAMOND')
fam_hotpep_row = calc_class_sample_size(fam_fbeta_df, 'Hotpep')
fam_cupp_row = calc_class_sample_size(fam_fbeta_df, 'CUPP')
fam_ecami_row = calc_class_sample_size(fam_fbeta_df, 'eCAMI')
fam_hdc_row = calc_class_sample_size(fam_fbeta_df, 'H_D_C')
fam_hde_row = calc_class_sample_size(fam_fbeta_df, 'H_D_E')

cazy_class_fam_sample_size_df <- rbind(fam_dbcan_row, fam_hmmer_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_diamond_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_hotpep_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_cupp_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_ecami_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_hdc_row)
cazy_class_fam_sample_size_df <- rbind(cazy_class_fam_sample_size_df, fam_hde_row)
cazy_class_fam_sample_size_df <- as.data.frame(cazy_class_fam_sample_size_df)
names(cazy_class_fam_sample_size_df) <- c('Prediction_tool', 'GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
cazy_class_fam_sample_size_df
```


## Performance per CAZy family

To evaluate the performance of predicting each CAZy family independent of all other CAZy families, the sensitivity and precision for each CAZy family, for each CAZyme classifier was calculated and plotted against each other (Fig.\@ref(fig:famrllvspc)). Whereas sensitivity was plotted against sensitivity for CAZy classes, owing to the extremely small variation in specificity scores, sensitivity was plotted as a percentage against log10 of the specificity percentage.

```{r prepPerformancePerFamRT, include=FALSE}
# add parent CAZy classes to the sensitivity and precision data
fam_subset_sens.rt$Prediction_tool <- str_replace(fam_subset_sens.rt$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
fam_subset_sens.rt$Prediction_tool <- str_replace(fam_subset_sens.rt$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")
fam_subset_spec.rt$Prediction_tool <- str_replace(fam_subset_spec.rt$Prediction_tool, "HMMER_DIAMOND_CUPP", "H_D_C")
fam_subset_spec.rt$Prediction_tool <- str_replace(fam_subset_spec.rt$Prediction_tool, "HMMER_DIAMOND_eCAMI", "H_D_E")

fam_subset_sens.rt.gh <- add_parent_class(fam_subset_sens.rt, gh.names, 'GH')
fam_subset_sens.rt.gt <- add_parent_class(fam_subset_sens.rt, gt.names, 'GT')
fam_subset_sens.rt.pl <- add_parent_class(fam_subset_sens.rt, pl.names, 'PL')
fam_subset_sens.rt.ce <- add_parent_class(fam_subset_sens.rt, ce.names, 'CE')
fam_subset_sens.rt.aa <- add_parent_class(fam_subset_sens.rt, aa.names, 'AA')
fam_subset_sens.rt.cbm <- add_parent_class(fam_subset_sens.rt, cbm.names, 'CBM')

fam_subset_spec.rt.gh <- add_parent_class(fam_subset_spec.rt, gh.names, 'GH')
fam_subset_spec.rt.gt <- add_parent_class(fam_subset_spec.rt, gt.names, 'GT')
fam_subset_spec.rt.pl <- add_parent_class(fam_subset_spec.rt, pl.names, 'PL')
fam_subset_spec.rt.ce <- add_parent_class(fam_subset_spec.rt, ce.names, 'CE')
fam_subset_spec.rt.aa <- add_parent_class(fam_subset_spec.rt, aa.names, 'AA')
fam_subset_spec.rt.cbm <- add_parent_class(fam_subset_spec.rt, cbm.names, 'CBM')

# recombine the dataframes
fam_subset_sens.rt_all <- rbind(fam_subset_sens.rt.gh, fam_subset_sens.rt.gt)
fam_subset_sens.rt_all <- rbind(fam_subset_sens.rt_all, fam_subset_sens.rt.pl)
fam_subset_sens.rt_all <- rbind(fam_subset_sens.rt_all, fam_subset_sens.rt.ce)
fam_subset_sens.rt_all <- rbind(fam_subset_sens.rt_all, fam_subset_sens.rt.aa)
fam_subset_sens.rt_all <- rbind(fam_subset_sens.rt_all, fam_subset_sens.rt.cbm)

fam_subset_spec.rt_all <- rbind(fam_subset_spec.rt.gh, fam_subset_spec.rt.gt)
fam_subset_spec.rt_all <- rbind(fam_subset_spec.rt_all, fam_subset_spec.rt.pl)
fam_subset_spec.rt_all <- rbind(fam_subset_spec.rt_all, fam_subset_spec.rt.ce)
fam_subset_spec.rt_all <- rbind(fam_subset_spec.rt_all, fam_subset_spec.rt.aa)
fam_subset_spec.rt_all <- rbind(fam_subset_spec.rt_all, fam_subset_spec.rt.cbm)

# rename columns to faciltate merging
names(fam_subset_sens.rt_all)[names(fam_subset_sens.rt_all) == "Statistic_value"] <- "Sensitivity"
names(fam_subset_spec.rt_all)[names(fam_subset_spec.rt_all) == "Statistic_value"] <- "Specificity"

# drop the column called X - from the index number created by pandas in Python
fam_subset_sens.rt_all <- fam_subset_sens.rt_all[c("CAZy_family", "Prediction_tool", "Sensitivity", "CAZy_class")]
fam_subset_spec.rt_all <- fam_subset_spec.rt_all[c("CAZy_family", "Prediction_tool", "Specificity", "CAZy_class")]

# merge the dataframes
cazy_fam_spec_sense_df.rt <- merge(
  fam_subset_sens.rt_all,
  fam_subset_spec.rt_all,
  by=c("Prediction_tool", "CAZy_class", "CAZy_family")
)

# set order data is presented
cazy_fam_spec_sense_df.rt$Prediction_tool <- factor(cazy_fam_spec_sense_df.rt$Prediction_tool, levels = recombined_classifiers_abbrev) 
cazy_fam_spec_sense_df.rt$CAZy_class <- factor(cazy_fam_spec_sense_df.rt$CAZy_class, levels = c('CBM', 'AA', 'CE', 'PL', 'GT', 'GH'))

# calculate sensitivity and specificity as a percentage
cazy_fam_spec_sense_df.rt$Sens_percent <- cazy_fam_spec_sense_df.rt$Sensitivity * 100
cazy_fam_spec_sense_df.rt$Spec_percent <- cazy_fam_spec_sense_df.rt$Specificity * 100
```

Later on in this report the sensitivity for each CAZy family is plotted against specificity, as was done with CAZy class. However, owing to extremely small different in specificity, with no tool producing a specificity less than 0.995 it is extremely difficult to separate performance by specificity, so a boxplot and scatter plot for each is plotted. Each point represents one test set, and test sets are grouped by CAZyme classifier and facet wrapped by the parent CAZy class.

```{r famsF1CieRT, echo=FALSE, fig.cap="95% confidence interval around the mean of CAZy family classification."}
# set order data is presented
cazy_fam_spec_sense_df.rt$CAZy_class <- factor(
  cazy_fam_spec_sense_df.rt$CAZy_class, levels = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
)

p.fam.sens.rt <- ggplot(
  cazy_fam_spec_sense_df.rt %>% dplyr::group_by(Prediction_tool),
  aes(x=Prediction_tool,
      y=Sensitivity,
      fill=Prediction_tool)) +
  geom_boxplot(outlier.shape=NA) +
  geom_jitter(width=0.1, height=0) +
  scale_fill_manual(values = rt.colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Sensitivity") +
  facet_wrap(~ CAZy_class)
p.fam.sens.rt
```

```{r famsF1CiRT, echo=FALSE, fig.cap="95% confidence interval around the mean CAZy family classifier per CAZy class"}

cal_tool_ci.fam <- function(stat_subset){
  c.classes <- c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
  fam.ci.df <- data.frame()
  
  for (cc in c.classes){
    subset_class <- stat_subset[which(stat_subset$CAZy_class == cc), ]
    
    # calculate lower, upper and mean 95% CI
    UpperCI <- subset_class %>%
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("UpperCI"=CI(Statistic_value)[1])
    MeanCI <- subset_class %>% 
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("MeanCI"=CI(Statistic_value)[2])
    LowerCI <- subset_class %>%
      dplyr::group_by(Prediction_tool) %>% dplyr::summarise("LowerCI"=CI(Statistic_value)[3])
  
    class.ci_df <- merge(MeanCI, UpperCI)
    class.ci_df <- merge(class.ci_df, LowerCI)
    class.ci_df$CAZy_class <- rep(cc, nrow(class.ci_df))
    
    fam.ci.df <- rbind(fam.ci.df, class.ci_df)
  }
  
  return(fam.ci.df)
}
fam_fbeta_df.rt.ci <- cal_tool_ci.fam(fam_fbeta_df.rt)

# set order data is presented
fam_fbeta_df.rt.ci$CAZy_class <- factor(
  fam_fbeta_df.rt.ci$CAZy_class, levels = c('GH', 'GT', 'PL', 'CE', 'AA', 'CBM')
)
fam_fbeta_df.rt.ci$Prediction_tool <- factor(
  fam_fbeta_df.rt.ci$Prediction_tool, levels = recombined_classifiers_abbrev
)

p.fam.f1.ci.rt <- ggplot(
  fam_fbeta_df.rt.ci %>% dplyr::group_by(Prediction_tool),
  aes(x=Prediction_tool,
      y=MeanCI,
      color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=LowerCI, ymax=UpperCI)) +
  scale_color_manual(values = rt.colour_set) +
  theme(legend.position = "none",
	plot.background = element_rect(fill = figbg, color = figbg),
	axis.text=element_text(size=10),
	axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("F1-score") +
  facet_wrap(~ CAZy_class)
p.fam.f1.ci.rt
```

```{r saveFamSensBoxplotRT, include=FALSE}
pdf(file = "famSensBoxplotsRT.pdf", width = 11.25, height = 7)
p.fam.sens.rt
dev.off()

svg(file = "famSensBoxplotsRT.svg", width = 11.25, height = 7)
p.fam.sens
dev.off()
```

For better resolution we can group the CAZy families by their parent CAzy classes, and compare the performances of the tools CAZy class, by CAZy class. Owing to the minimal variation in specificity scores, specificity was plotted as the percentage specificity log10.

### Glycoside Hydrolases

Figure \@ref(fig:ghfamrllvspcRT) shows the plotting of sensitivity against specificity for each Glycoside Hydrolase CAZy family.

```{r ghfamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycoside Hydrolases. Each GH CAZy family is represented as a single point on the plot."}
p.gh.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'GH', 0.8, 0.1, 0.8, 99.7, 100, rt.colour_set)
p.gh.sens.spec.fam.rt
```

```{r saveGhSensSpecRT, include=FALSE}
pdf(file = "famSpecSensGhRT.pdf", width = 11.25, height = 7)
p.gh.sens.spec.fam.rt
dev.off()
```

### Glycosyltransferases

Figure \@ref(fig:gtfamrllvspcRT) shows the plotting of sensitivity against specificity for each Glycosyltransferases CAZy family.

```{r gtfamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Glycosyltransferases. Each GT CAZy family is represented as a single point on the plot."}
p.gt.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'GT', 0.8, 0.1, 0.8, 99.8, 100, rt.colour_set)
p.gt.sens.spec.fam.rt
```

```{r saveGtSensSpecRT, include=FALSE}
pdf(file = "famSpecSensGtRT.pdf", width = 11.25, height = 7)
p.gt.sens.spec.fam.rt
dev.off()
```

### Polysaccharide Lyases

Figure \@ref(fig:plfamrllvspcRT) shows the plotting of sensitivity against specificity for each Polysaccharide Lyases CAZy family.

```{r plfamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Polysaccharide Lyases. Each PL CAZy family is represented as a single point on the plot."}
p.pl.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'PL', 0.9, 0.1, 0.4, 99.9, 100, rt.colour_set)
p.pl.sens.spec.fam.rt
```

```{r savePlSensSpecRT, include=FALSE}
pdf(file = "famSpecSensPlRT.pdf", width = 11.25, height = 7)
p.pl.sens.spec.fam.rt
dev.off()
```

### Carbohydrate Esterases

Figure \@ref(fig:cefamrllvspcR) shows the plotting of sensitivity against specificity for each Carbohydrate Esterases CAZy family.

```{r cefamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Esterases. Each CE CAZy family is represented as a single point on the plot."}
p.ce.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'CE', 0.8, 0.1, 0.5, 99.7, 100, rt.colour_set)
p.ce.sens.spec.fam.rt
```

```{r saveCeSensSpecRT, include=FALSE}
pdf(file = "famSpecSensCeRT.pdf", width = 11.25, height = 7)
p.ce.sens.spec.fam.rt
dev.off()
```

### Auxillary Activities

Figure \@ref(fig:afamrllvspcRT) shows the plotting of sensitivity against specificity for each Auxillary Activities CAZy family.

```{r aafamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Auxillary Activities. Each AA CAZy family is represented as a single point on the plot."}
p.aa.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'AA', 0.8, 0.1, 0.7, 99.7, 100, rt.colour_set)
p.aa.sens.spec.fam.rt
```

```{r saveAaSensSpecRT, include=FALSE}
pdf(file = "famSpecSensAaR.pdf", width = 11.25, height = 7)
p.aa.sens.spec.fam.rt
dev.off()
```


### Carbohydate Binding Modules

Figure \@ref(fig:cbmfamrllvspcRT) shows the plotting of sensitivity against specificity for each Carbohydrate Binding Module CAZy family.

```{r cbmfamrllvspcRT, echo=FALSE, fig.cap="Scatter plot of recall (sensitivity) against specificity for predicting each CAZy family for each CAZyme classifier in the CAZy class Carbohydrate Binding Modules. Each CBM CAZy family is represented as a single point on the plot."}
p.cbm.sens.spec.fam.rt <- plot_fam_sens_vs_spec(cazy_fam_spec_sense_df.rt, 'CBM', 0.8, 0.1, 1, 99.4, 100, rt.colour_set)
p.cbm.sens.spec.fam.rt
```

```{r saveCbmSensSpecRT, include=FALSE}
pdf(file = "famSpecSensCbmR.pdf", width = 11.25, height = 7)
p.cbm.sens.spec.fam.rt
dev.off()
```


## Rand Index and Adjusted Rand Index of CAZy Family Classifications dingding

```{r famRiRT, echo=FALSE}
fam_classification_df$Prediction_tool <- factor(fam_classification_df$Prediction_tool, levels = classifiers) # set order data is presented

fam_ri_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("Mean"=mean(Rand_index), "Standard Deviation"=sd(Rand_index))

kable(fam_ri_stats_df, caption="Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```
```{r rtfamRIci, echo=FALSE, fig.cap="95% confidence interval around the mean of Rand Index (RI) of the performance of the CAZyme classifiers to predict the multilabel classification of CAZy classes."}

rt.class_ri_stats_df$Prediction_tool <- factor(rt.class_ri_stats_df$Prediction_tool, levels = recombined_classifiers) # set order data is presented

# plot RI
p.rt.ri.class = ggplot(rt.class_ri_stats_df,
                aes(x=Prediction_tool, y=Mean, color=Prediction_tool)) +
  geom_point() +
  geom_errorbar(aes(ymin=`Lower CI`, ymax=`Upper CI`)) +
  scale_colour_manual(values = rt.colour_set) +
  theme(legend.position = "none",
        plot.background = element_rect(fill = figbg, color = figbg),
        axis.text=element_text(size=10),
        axis.title=element_text(size=12,face="bold")) +
  xlab("Classifier") + 
  ylab("Adjusted Rand Index")
p.rt.ri.class
```


```{r famAriCalcRT, echo=FALSE}
fam_ari_stats_df <- fam_classification_df %>% dplyr::group_by(Prediction_tool) %>% dplyr::summarise("Mean"=mean(Adjusted_Rand_index), "Standard Deviation"=sd(Adjusted_Rand_index))

kable(fam_ari_stats_df, caption="Adjusted Rand Index of CAZyme classifier classification of CAZy family annotations", align='c', digits = 4) %>% kable_styling(full_width = F)
```

# Conclusions

Overall, all CAZyme classifiers showed strong performances at all three levels of CAZyme classification (CAZyme/non-CAZyme. CAZy class and CAZy family).

Performance was extremely strong for CAZyme classifiers for across all levels of CAZyme classification, performance in CAZyme classifiers varied most greatly for sensitivity.

In general, the CAZyme/non-CAZyme, CAZy class and CAZy family classifications were accurate for all CAZyme classifiers (i.e. when a classification is predicted it was frequently correct). however, the CAZyme classifiers do not predict a comprehensive CAZome. CAZyme classifiers performance differed most greatly by sensitivity, which indicated an non-comprehensive annotation of the CAZome, CAZy class members and CAZy family members.

Classifying Bacterial or Eukaryote had neglebialbe impact on the performance of the CAZyme classification at at every level of classification (CAZyme/non-CAZyme. CAZy class and CAZy family).


